[["index.html", "Statistical Learning of Infectious Disease Data Analytics Chapter 1 Preface 1.1 Data Sets Used in Labs and Exercises 1.2 Acknowledgements 1.3 About the Author", " Statistical Learning of Infectious Disease Data Analytics Lily Wang 2021-04-03 Chapter 1 Preface The book is written for three audiences: (1) data scientists finding themselves doing modeling and forecasting in epidemilogy; (2) graduate students studying epidemiology; (3) graduate students doing a modeling and forecasting elective. For most sections, we only assume that readers are familiar with introductory statistics, and with high-school algebra. There are a couple of sections that also require knowledge of matrices, but these are flagged. At the end of each chapter we provide a list of “further reading.” In general, these lists comprise suggested textbooks that provide a more advanced or detailed treatment of the subject. Where there is no suitable textbook, we suggest journal articles that provide more information. We use R throughout the book and we intend students to learn how to forecast with R. R is free and available on almost every operating system. It is a wonderful tool for all statistical analysis, not just for forecasting. See the Using R appendix for instructions on installing and using R. Short Description: This book provides a quick start guide to infectious disease data analysis and visualization in R. You’ll learn, how to create static and interactive graphs. Long Description: This book will introduce readers to modern statistical models and state-of-the-art learning methods to analyze infectious disease data. Many of the key approaches, examples and case studies will be presented. The primary emphasis will be interpretation, inference and hands-on data analyses. This course also provides practical concepts and R computing skills to perform infectious disease data analysis and visualization. This book provides a quick start guide to infectious disease data analysis and visualization in R. You’ll learn, how to: Apply appropriate descriptive and inferential statistical techniques to infectious disease data and interpret results of statistical analyses in the context of public health research and evaluation; Develop skills in model building, investigating model assumptions, and interpreting results from statistical models with particular applications to epidemiologic and especially the infectious disease data; Develop the ability to use R to understand basic data structures, basic data processing skills, and basic data visualization skills; 1.1 Data Sets Used in Labs and Exercises In this textbook, we illustrate statistical learning methods for infectious disease data using applications from COVID-19 data. The slid package available on the book website contains a number of data sets that are required in order to perform the labs and exercises associated with this book. 1.2 Acknowledgements I am very appreciative of all the contributors to this immense open source project of R and R-Studio. I couldn’t have made this book without your contributions and the packages that you have developed. A special thanks go to Dr. Guannan Wang, Dr. Xinyi Li, Dr. Shan Yu, Miss Yueying Wang, Miss Zhiling Gu, Mr. Myungjin Kim for their help with the book. I would also like to thank Dr. Yihui Xie (Xie 2020) for his R Markdown package, which simplifies the writing of this book by having all content written in R Markdown. 1.3 About the Author Dr. Lily Wang is a tenured Professor of Statistics at Iowa State University. She received her Ph.D. in Statistics from Michigan State University in 2007. Prior to joining Iowa State in 2014, she was a tenure-track Assistant/tenured Associate Professor in the Department of Statistics at the University of Georgia 2007-2013/2013-2014. Her primary areas of research include developing cutting-edge statistical non/semi-parametric methods, statistical learning of large datasets with complex features, methodologies for functional data, imaging data, and spatiotemporal data, survey sampling, and the application of statistics to problems in economics, engineering, neuroimaging, epidemiology, environmental studies, and biomedical science. "],["intro.html", "Chapter 2 Introduction 2.1 Aims and Scope of This Book 2.2 The Structure of This Book", " Chapter 2 Introduction 2.1 Aims and Scope of This Book Epidemiologic data are paramount to targeting and implementing evidence-based control measures to protect the public’s health and safety. Nowhere are data more important than during a field epidemiologic investigation to identify the cause of an urgent public health problem that requires immediate intervention. Many of the steps to conducting a field investigation rely on identifying relevant existing data or collecting new data that address the key investigation objectives. In today’s information age, the challenge is not the lack of data but rather how to identify the most relevant data for meaningful results and how to combine data from various sources that might not be standardized or interoperable to enable analysis. Accessing or collecting clean, valid, reliable, and timely data challenges most field epidemiologic investigations. Infectious disease learning requires lots of iteration between data manipulation, visualization, and modeling. The purpose of this book is to provide an overview of modern data science tools and methods that have been developed specifically to analyze infectious disease data. The readers are assumed to have a background in high school mathematics and introductory-level statistics, but no specialist knowledge of infectious diseases is assumed. Since the topic of this book is an enormous one, we do not claim to provide comprehensive coverage of all existing methods. However, we will describe many of the critical approaches, and throughout, there will be many examples and case studies. This book serves the complementary purpose of introducing graduate students and others to the field of infectious disease data analysis, acting as a reference for researchers in this field, and helping practicing data scientists and infectious disease epidemiologists to develop the ability to use R to understand basic data structures, basic data processing skills, and basic data visualization skills. 2.2 The Structure of This Book 2.2.1 Infectious disease data The material in this book is concerned with the statistical analysis of quantitative data obtained by observing the spread of infectious diseases. According to Porta (2014): Infectious disease (or communicable disease) is defined as an illness caused by a specific infectious agent or its toxic product that results from transmission of that agent or its products from an infected person, animal, or reservoir to a susceptible host, either directly or indirectly through an intermediate plant or animal host, vector or inanimate environment. Surveillance systems generate data that help public health officials understand existing and emerging infectious diseases. Without a proper understanding of the health problem (etiology, distribution, and mechanism of infection), it will be difficult to ameliorate the health issue. Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. The current COVID-19 pandemic raises important questions about opening, sharing and using data, and highlights the challenges associated with data use. It is well known that data is critical to understanding the impact of infectious disease, but also to inform the appropriate response, planning and allocation of resources. 2.2.2 Basic characteristics of the infection process Reservoir of infection, also called primary source of infection, is a location (person, animal, arthropod, plant, soil, or substance) in which the infectious agent finds conditions that permit it to survive and multiply and from where it can be transmitted to another susceptible host. See Barreto, Teixeira, and Carmo (2006). For the infection of a new host to occur, there must be an opportunity for a susceptible host to be exposed to the infectious agent—that is, there must be contact between the agent and the host. The infectious disease is transmitted to a susceptible host, when the individual takes in a sufficient quantity of causative organism. When a susceptible takes in an amount of infectious material, sufficient to induce infection, we say that the individual has made an infectious contact. Following the time of the infection, the newly infected individual generally passes through a latent period during which the infection develops purely internally, without the emission of any kind of infectious material. The latent period is the time from infection to onset of the ability to infect. It ends when the infected individual becomes infectious, and for the duration of the infectious period, we refer to the infected individual as an infective who can transmit infection to susceptibles. The infectious period ends when the infected individual ceases to be infectious, and he either becomes susceptible again or becomes a removal for some time. A removal is an individual who are immune or dead as a consequence of infection. A removal plays no part in the spread of the disease. The states of isolation and immunity may be temporary or permanent. Each infected individual is also referred to as a case. Figure 2.1: Chain of infection. Source: Centers for Disease Control and Prevention (Dicker and Gathany 1992). 2.2.3 Data visualization Data visualization or information visualization always played a crucial role in scientific analysis. In many infectious disease studies, data visualization could be a good starting point for the users to understand how far the disease will spread and to illustrate our findings and statistical insights. Besides, the ability to visualize, track, and predict the spread of the disease can help raise awareness, understand the impact of the disease, and ultimately assist in prevention efforts. Data visualization is having a big moment during the COVID-19 pandemic. Social media feeds are overwhelmed with infection heat maps and charts depicting transmission patterns. We have all seen models projecting the spread of the novel coronavirus. The COVID-19 pandemic also poses new challenges to data scientists, too, for its vast and rapid spread and significant economic impact. A lot of work has been done on visualizing COVID-19 data since the outbreak of the pandemic. The daily counts of cases and deaths of COVID-19 are crucial for understanding how this pandemic is spreading. Thanks to the contribution of the data science communities across the world, multiple sources provide the COVID-19 data with different precision and focus. To clean the data, we first fetch data from various sources and compile them into the same format for further comparison and integration. Appendix B describes the data used in the examples, case studies, and lab exercises in the book. R offers the opportunity to scale and automate tasks, document and track them, and reliably reproduce their output. The first few chapters investigate existing R visualization techniques used to manipulate and represent infectious disease data. Chapter 3 provides an introduction to data wrangling and how to use R packages dplyr and tidyr to manipulate your data in a useful form for visualization and modeling. This chapter is for someone who is already somewhat familiar with R, but would like to know more about using it for fundamental data analysis and manipulation. Figure 2.2: The stages of a data science workflow. Original source: Wickham and Grolemund (2016) Graphs can be presented using a variety of media: print, projection, dashboard, etc. The visualizations can be primarily classified into two groups: visualization with zero or less interactivity represents the first group, and complex interactive visualization techniques and tools represent the second. See Figure 2.3 for different types of visualization. Before constructing any display of epidemiologic data, it is important to first determine the point to be conveyed and which media you want to use for communications Figure 2.3: Types of visualization plots. Chapter 4 introduces static visualization, which uses basic graphs such as bar and line graphs for representing attributes of the COVID-19 dataset. We use a collection of graphs for comparing cumulative or daily new cases and deaths between states/counties in the US. Chapter 5 provide insight and practical skills for creating interactive and dynamic web graphics for data analysis from R. This kind of visualizations allow user interaction like hovering the mouse over bars and points in the charts. It makes heavy use of plotly for rendering graphics, but you’ll also learn about other R packages that augment a data science workflow, such as the tidyverse and shiny. Along the way, you’ll gain insight into best practices for visualization of infectious disease data, statistical graphics, and graphical perception. Chapter 6 focuses on linking plotly graphs with shiny, an open-source R package that provides an elegant and powerful web framework for building web applications. Chapter 7 is an in-depth look at visualizing data in a spatial setting and presenting findings through some geospatial visualization. 2.2.4 Modeling and Forecasting The concepts and techniques discussed in Chapters 3–6 have dealt with describing, visualizing, and exploring the data. The use of scientific models for understanding the dynamics of infectious diseases has a very rich history in epidemiology. Starting in December 2019 in China, the outbreak of COVID-19 has spread globally within weeks. To efficiently combat COVID-19, it is crucial to have a better understanding of how far the virus will spread, and how many lives it will claim. Scientific modeling is an essential tool to answer these questions and ultimately assist in disease prevention, policymaking, and resource allocation. Chapter 8 presents a few classic epidemic modeling approaches, and takes the reader through steps required for fundamental infectious data analysis and presentation of data typically encountered in epidemiology using COVID-19 data set. Chapter 9 introduces the compartment models. Chapter 11 provides the analytical techniques of regression and discrimination as a means of quantifying the effect of a set of explanatory variables on the spatial distribution of a particular outcome. Chapter 10 takes the reader through time series modeling and forecasting. Chapter 12 introduces some neural network models for forecasting. Chapter 13 describes the ensemble methods using multiple forecasting algorithms to improve the predictive performance. "],["dplyr.html", "Chapter 3 Data Wrangling with dplyr and tidyr 3.1 Learning dplyr 3.2 Selecting Columns and Filtering Rows 3.3 Make New Variables: Mutate 3.4 Summarize Data 3.5 Combine Data Sets 3.6 Reshaping Data 3.7 Exercises", " Chapter 3 Data Wrangling with dplyr and tidyr The package dplyr is an R package created to make tabular data wrangling less difficult by using a limited set of functions that are able to be used together to extract and summarize insights from your data. It works well with the package, tidyr, which allows you to quickly convert between different data formats (long vs. wide) for the purpose of plotting and analysis. It tackles the common problem of reshaping your data for plotting and use by different R functions. Sometimes we may want data sets where we have one row per measurement, or a data frame where each measurement type has its own column and rows are, instead, more aggregated groups, or maybe you want to select important variables, filter out key observations, create new variables and obtain summary statistics. As illustrated in Figure 3.1, it is necessary that you work back and forth between those formats. tidyr and dplyr give you the right tools for this as well as for more advanced and intricate data wrangling. Figure 3.1: A typical data science process. The packages dplyr and tidyr are built to work directly with data frames. In this chapter, you will learn how to use these packages to perform data manipulation and transform your data into the appropriate form. 3.1 Learning dplyr # load the packages library(tidyverse) ## ── Attaching packages ─────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.4 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(dplyr) 3.1.1 Tibbles In this book, we will work with “tibbles,” rather than R’s “data.frame.” Tibbles are data frames but modernized so that they keep what time has proven to be effective and throwing out what is not. Here, we will use the tibble package, which provides opinionated data frames that make working in tidyverse a little easier. # The easiest way to get tibble is to install the whole tidyverse: install.packages(&quot;tidyverse&quot;) # Alternatively, install just tibble: install.packages(&quot;tibble&quot;) For the most part, we will use the terms “tibble” and “data frame” interchangeably. When we choose to point out R’s data frame, we will call it a “data.frame.” See Wickham and Grolemund (2016) for more details about how to create and use “tibbles.” 3.1.2 Import data Recall R offers many ways to import data: read_csv() reads comma delimited files, read_csv2() reads semicolon separated files. read_tsv() reads tab delimited files, and read_delim() reads in files with any delimiter. read_fwf() reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions(). read_table() reads a common variation of fixed width files where columns are separated by white space. load() loads an .RData file and import all of the objects contained in the .RData file into your current workspace. Below we will work on a COVID-19, county level infected count data (I.county), and we can obtain the data from the Github R package slid. It is a “data frame” which includes “ID” (county-level Federal Information Processing System code), “County” (name of county), “State” (name of state), “XYYYY.MM.DD” (the number of cumulative infected cases in a county related to the date of YYYY.MM.DD) for 3,104 counties in the US. For example, the variable X2020.01.22 is the number of cumulative infected cases in a county on 01/22/2020. See Appendix B for more detailed description of the data and its source. # install the slid package from github # library(devtools) # devtools::install_github(&#39;covid19-dashboard-us/slid&#39;) # load objects in I.county into my workspace library(slid) data(I.county) # make I.county a tibble with as_tibble() I.county &lt;- as_tibble(I.county) # preview the data # View(I.county) 3.1.3 Common dplyr functions Next, we will learn some of the most common dplyr functions: select(): subset columns; filter(): subset rows on conditions; mutate(): create new columns by using information from other columns; group_by() and summarize(): create summary statistics on grouped data; arrange(): sort results; join() family: combine datasets. A typical code structure of dplyr is: data.new &lt;- data.original %&gt;% select rows or columns to manipulate %&gt;% arrange or group the data %&gt;% summarize the data The first argument is a data frame, and the subsequent arguments separated by %&gt;% describe the data manipulation and/or summary, and the result is a new data frame. We will explain more details in the following sections. 3.2 Selecting Columns and Filtering Rows 3.2.1 Subset Variables (Columns) To select columns of a dataframe, use select(). The first argument to this function is the data frame (I.county), and the subsequent arguments are the columns to keep, separated by commas. Alternatively, if you are selecting columns adjacent to each other, you can use a : to select a range of columns, read as “select columns from __ to __”. # load the tidyverse dplyr::select(I.county, ID, County, State) # select a series of connected columns dplyr::select(I.county, ID, County, State, X2020.12.11:X2020.12.01) 3.2.2 Subset Observations (Rows) To choose rows based on specific criteria, we can use the filter() function. The arguments after the dataframe are the condition(s) we want for our final dataframe to adhere to (e.g. State name is “Iowa”). We can chain a series of conditions together using commas between each condition. # all Iowa counties dplyr::filter(I.county, State == &quot;Iowa&quot;) Here is an example of filter() function with multiple conditions: # all Iowa counties with cumulative infection count &gt; 10000 dplyr::filter(I.county, State == &quot;Iowa&quot;, X2020.12.11 &gt; 10000) To use filtering effectively, it is better to know some of the comparison and logical operators. Figure 3.2 shows some commonly used R logic comparisons: Figure 3.2: Some commonly used logic comparisons. 3.2.3 Pipes What if you wanted to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes. With intermediate steps, you create a temporary data frame and use that as input to the next function, like this: # all Iowa counties from 2020.12.01 to 2020.12.11 # method 1 Iowa.I.county &lt;- dplyr::filter(I.county, State == &quot;Iowa&quot;) Iowa.I.county.DEC &lt;- dplyr::select(Iowa.I.county, X2020.12.11:X2020.12.01) This is readable, but can easily fill up your workspace with lots of objects that need to be named individually. With multiple steps, that can be difficult to keep track of. You can also nest functions (i.e. one function inside of another), like this: # all Iowa counties from 2020.12.01 to 2020.12.11 # method 2 Iowa.I.county.DEC &lt;- dplyr::select(dplyr::filter(I.county, State == &quot;Iowa&quot;), ID, County, State, X2020.12.11:X2020.12.01) This is useful but can be tricky to read if too many functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting). The last option, pipes, were recently added to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do multiple things to the same data set. Pipes in R look like %&gt;% and are made available via the magrittr package, installed automatically with dplyr. # all Iowa counties from 2020.12.01 to 2020.12.11 # method 3 I.county %&gt;% dplyr::filter(State == &quot;Iowa&quot;) %&gt;% dplyr::select(ID, County, X2020.12.11:X2020.12.01) In the above code, we use the pipe to send the interviews data set first through filter() to keep rows for the state of Iowa, then through select() to keep only the count in December. Since %&gt;% takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include the dataframe as an argument to the filter() and select() functions anymore. Some may find it helpful to read the pipe like the word “then.” For instance, in the above example, we take the data frame I.county, then we filter for rows with State == \"Iowa\", then we select columns from X2020.12.11 to X2020.12.01. The dplyr functions are somewhat simple, but by combining them into linear workflows with the pipe, we can accomplish more complex data wrangling operations. If we want to create a new object with this smaller version of the data, we can assign it a new name: # assign a name to all Iowa counties # from 2020.12.01 to 2020.12.11 Iowa.I.county.DEC &lt;- I.county %&gt;% dplyr::filter(State == &quot;Iowa&quot;) %&gt;% dplyr::select(ID, County, X2020.12.11:X2020.12.01) head(Iowa.I.county.DEC) ## # A tibble: 6 x 13 ## ID County X2020.12.11 X2020.12.10 X2020.12.09 ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 19001 Adair 506 503 499 ## 2 19003 Adams 208 206 200 ## 3 19005 Allam… 995 990 971 ## 4 19007 Appan… 868 862 858 ## 5 19009 Audub… 326 323 321 ## 6 19011 Benton 1852 1847 1826 ## # … with 8 more variables: X2020.12.08 &lt;int&gt;, ## # X2020.12.07 &lt;int&gt;, X2020.12.06 &lt;int&gt;, ## # X2020.12.05 &lt;int&gt;, X2020.12.04 &lt;int&gt;, ## # X2020.12.03 &lt;int&gt;, X2020.12.02 &lt;int&gt;, X2020.12.01 &lt;int&gt; 3.2.4 Select and order top n entries (by group if grouped data). The function top_n can be used to select top (or bottom) n rows (by value). This is a convenient wrapper that uses filter()and min_rank() to select the top or bottom entries in each group, ordered by wt. Usage top_n(x, n, wt) Arguments x: a tbl() to filter n: number of rows to return. If x is grouped, this is the number of rows per group. Will include more than n rows if there are ties. If n is positive, selects the top n rows. If negative, selects the bottom n rows. wt (Optional). The variable to use for ordering. If not specified, defaults to the last variable in the tbl. This argument is automatically quoted and later evaluated in the context of the data frame. It supports unquoting. Let us find the top ten counties with the largest cumulative infected count on December 11, 2020. # top ten counties with the cum. infected count I.county.top10 &lt;- I.county %&gt;% top_n(10, wt = X2020.12.11) I.county.top10$County ## [1] Maricopa LosAngeles SanBernardino Broward ## [5] Miami-Dade Cook Clark Dallas ## [9] Harris Tarrant ## 1839 Levels: Abbeville AcadiaParish Accomack Ada ... obrien Let us find the bottom ten counties with the smallest cumulative infected count on December 11, 2020. # bottom ten counties with the cum. infected count I.county.bottom10 &lt;- I.county %&gt;% top_n(-10, wt = X2020.12.11) I.county.bottom10$County ## [1] Dukes Nantucket OglalaLakota Beaver ## [5] BoxElder Cache Carbon Daggett ## [9] Duchesne Emery Garfield Grand ## [13] Iron Juab Kane Millard ## [17] Morgan Piute Rich Sanpete ## [21] Sevier Uintah Washington Wayne ## [25] Weber ## 1839 Levels: Abbeville AcadiaParish Accomack Ada ... obrien Let us find the county with the largest cumulative infected count on December 11, 2020 for each state. # county with the largest cum. infected count for each state I.county.top1 &lt;- I.county %&gt;% group_by(State) %&gt;% top_n(1, wt = X2020.12.11) %&gt;% dplyr::select(State, County) 3.3 Make New Variables: Mutate Frequently you will want to create new columns based on the values in existing columns, for example, to obtain the number of daily new cases based on the cumulative count. For this, we can use the mutate() function. # create a new variable Y2020.12.11 (new count 2020.12.11) I.county.new &lt;- I.county %&gt;% dplyr::filter(State == &quot;Iowa&quot;) %&gt;% dplyr::select(ID, County, X2020.12.11:X2020.12.10) %&gt;% mutate(Y2020.12.11 = X2020.12.11 - X2020.12.10) head(I.county.new) ## # A tibble: 6 x 5 ## ID County X2020.12.11 X2020.12.10 Y2020.12.11 ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 19001 Adair 506 503 3 ## 2 19003 Adams 208 206 2 ## 3 19005 Allamakee 995 990 5 ## 4 19007 Appanoose 868 862 6 ## 5 19009 Audubon 326 323 3 ## 6 19011 Benton 1852 1847 5 If we want to obtain the number of daily new cases based on the cumulative count for the dates in December only, we can try the following: # create variables Y2020.12.01 : Y2020.12.11 # with daily new count I.county.Iowa &lt;- I.county %&gt;% dplyr::filter(State == &quot;Iowa&quot;) I.county.tmp &lt;- I.county.Iowa[, -(1:3)] I.county.Iowa.new &lt;- I.county.Iowa I.county.Iowa.new[, -(1:3)] &lt;- I.county.tmp - cbind(I.county.tmp[, -1], 0) I.county.Iowa.DEC &lt;- I.county.Iowa.new %&gt;% dplyr::select(ID, County, X2020.12.11:X2020.12.01) name.tmp &lt;- substring(names(I.county.Iowa.DEC)[-(1:2)], 2) names(I.county.Iowa.DEC)[-(1:2)] &lt;- paste0(&quot;Y&quot;, name.tmp) head(I.county.Iowa.DEC) ## # A tibble: 6 x 13 ## ID County Y2020.12.11 Y2020.12.10 Y2020.12.09 ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 19001 Adair 3 4 10 ## 2 19003 Adams 2 6 4 ## 3 19005 Allam… 5 19 17 ## 4 19007 Appan… 6 4 5 ## 5 19009 Audub… 3 2 6 ## 6 19011 Benton 5 21 7 ## # … with 8 more variables: Y2020.12.08 &lt;int&gt;, ## # Y2020.12.07 &lt;int&gt;, Y2020.12.06 &lt;int&gt;, ## # Y2020.12.05 &lt;int&gt;, Y2020.12.04 &lt;int&gt;, ## # Y2020.12.03 &lt;int&gt;, Y2020.12.02 &lt;int&gt;, Y2020.12.01 &lt;int&gt; 3.4 Summarize Data Many data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy via the group_by() function. The summarize() function uses summary functions, functions that take a vector of values and return a single value, such as: dplyr::first: first value of a vector. dplyr::last: last value of a vector. dplyr::nth: nth value of a vector. dplyr::n: number of values in a vector. dplyr::n_distinct: number of distinct values in a vector. IQR: IQR of a vector. min: minimum value in a vector. max: maximum value in a vector. mean: mean value of a vector. median: median value of a vector. var: variance of a vector. sd: standard deviation of a vector. The group_by() function is often used together with summarize(), which collapses each group into a single-row summary of that group. The group_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. Once the data are grouped, you can also summarize multiple variables simultaneously (and not necessarily on the same variable). So to compute the state level cumulative infected count by State: # state level cumulative infected count # method 1: summarize() I.state &lt;- I.county %&gt;% group_by(State) %&gt;% summarize(across(X2020.12.11:X2020.01.22, ~ sum(.x, na.rm = TRUE))) head(I.state, 2) ## # A tibble: 2 x 326 ## State X2020.12.11 X2020.12.10 X2020.12.09 X2020.12.08 ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Alab… 288775 284922 280187 276665 ## 2 Ariz… 394512 387529 382601 378157 ## # … with 321 more variables: X2020.12.07 &lt;int&gt;, ## # X2020.12.06 &lt;int&gt;, X2020.12.05 &lt;int&gt;, ## # X2020.12.04 &lt;int&gt;, X2020.12.03 &lt;int&gt;, ## # X2020.12.02 &lt;int&gt;, X2020.12.01 &lt;int&gt;, ## # X2020.11.30 &lt;int&gt;, X2020.11.29 &lt;int&gt;, ## # X2020.11.28 &lt;int&gt;, X2020.11.27 &lt;int&gt;, ## # X2020.11.26 &lt;int&gt;, X2020.11.25 &lt;int&gt;, ## # X2020.11.24 &lt;int&gt;, X2020.11.23 &lt;int&gt;, ## # X2020.11.22 &lt;int&gt;, X2020.11.21 &lt;int&gt;, ## # X2020.11.20 &lt;int&gt;, X2020.11.19 &lt;int&gt;, ## # X2020.11.18 &lt;int&gt;, X2020.11.17 &lt;int&gt;, ## # X2020.11.16 &lt;int&gt;, X2020.11.15 &lt;int&gt;, ## # X2020.11.14 &lt;int&gt;, X2020.11.13 &lt;int&gt;, ## # X2020.11.12 &lt;int&gt;, X2020.11.11 &lt;int&gt;, ## # X2020.11.10 &lt;int&gt;, X2020.11.09 &lt;int&gt;, ## # X2020.11.08 &lt;int&gt;, X2020.11.07 &lt;int&gt;, ## # X2020.11.06 &lt;int&gt;, X2020.11.05 &lt;int&gt;, ## # X2020.11.04 &lt;int&gt;, X2020.11.03 &lt;int&gt;, ## # X2020.11.02 &lt;int&gt;, X2020.11.01 &lt;int&gt;, ## # X2020.10.31 &lt;int&gt;, X2020.10.30 &lt;int&gt;, ## # X2020.10.29 &lt;int&gt;, X2020.10.28 &lt;int&gt;, ## # X2020.10.27 &lt;int&gt;, X2020.10.26 &lt;int&gt;, ## # X2020.10.25 &lt;int&gt;, X2020.10.24 &lt;int&gt;, ## # X2020.10.23 &lt;int&gt;, X2020.10.22 &lt;int&gt;, ## # X2020.10.21 &lt;int&gt;, X2020.10.20 &lt;int&gt;, ## # X2020.10.19 &lt;int&gt;, X2020.10.18 &lt;int&gt;, ## # X2020.10.17 &lt;int&gt;, X2020.10.16 &lt;int&gt;, ## # X2020.10.15 &lt;int&gt;, X2020.10.14 &lt;int&gt;, ## # X2020.10.13 &lt;int&gt;, X2020.10.12 &lt;int&gt;, ## # X2020.10.11 &lt;int&gt;, X2020.10.10 &lt;int&gt;, ## # X2020.10.09 &lt;int&gt;, X2020.10.08 &lt;int&gt;, ## # X2020.10.07 &lt;int&gt;, X2020.10.06 &lt;int&gt;, ## # X2020.10.05 &lt;int&gt;, X2020.10.04 &lt;int&gt;, ## # X2020.10.03 &lt;int&gt;, X2020.10.02 &lt;int&gt;, ## # X2020.10.01 &lt;int&gt;, X2020.09.30 &lt;int&gt;, ## # X2020.09.29 &lt;int&gt;, X2020.09.28 &lt;int&gt;, ## # X2020.09.27 &lt;int&gt;, X2020.09.26 &lt;int&gt;, ## # X2020.09.25 &lt;int&gt;, X2020.09.24 &lt;int&gt;, ## # X2020.09.23 &lt;int&gt;, X2020.09.22 &lt;int&gt;, ## # X2020.09.21 &lt;int&gt;, X2020.09.20 &lt;int&gt;, ## # X2020.09.19 &lt;int&gt;, X2020.09.18 &lt;int&gt;, ## # X2020.09.17 &lt;int&gt;, X2020.09.16 &lt;int&gt;, ## # X2020.09.15 &lt;int&gt;, X2020.09.14 &lt;int&gt;, ## # X2020.09.13 &lt;int&gt;, X2020.09.12 &lt;int&gt;, ## # X2020.09.11 &lt;int&gt;, X2020.09.10 &lt;int&gt;, ## # X2020.09.09 &lt;int&gt;, X2020.09.08 &lt;int&gt;, ## # X2020.09.07 &lt;int&gt;, X2020.09.06 &lt;int&gt;, ## # X2020.09.05 &lt;int&gt;, X2020.09.04 &lt;int&gt;, ## # X2020.09.03 &lt;int&gt;, X2020.09.02 &lt;int&gt;, ## # X2020.09.01 &lt;int&gt;, X2020.08.31 &lt;int&gt;, ## # X2020.08.30 &lt;int&gt;, … or we can use summarize_at(), which affects variables selected with a character vector or vars(): # state level cumulative infected count # method 2: summarize_at() I.state &lt;- I.county %&gt;% group_by(State) %&gt;% summarize_at(vars(X2020.12.11:X2020.01.22), ~ sum(.x, na.rm = TRUE)) head(I.state, 2) ## # A tibble: 2 x 326 ## State X2020.12.11 X2020.12.10 X2020.12.09 X2020.12.08 ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Alab… 288775 284922 280187 276665 ## 2 Ariz… 394512 387529 382601 378157 ## # … with 321 more variables: X2020.12.07 &lt;int&gt;, ## # X2020.12.06 &lt;int&gt;, X2020.12.05 &lt;int&gt;, ## # X2020.12.04 &lt;int&gt;, X2020.12.03 &lt;int&gt;, ## # X2020.12.02 &lt;int&gt;, X2020.12.01 &lt;int&gt;, ## # X2020.11.30 &lt;int&gt;, X2020.11.29 &lt;int&gt;, ## # X2020.11.28 &lt;int&gt;, X2020.11.27 &lt;int&gt;, ## # X2020.11.26 &lt;int&gt;, X2020.11.25 &lt;int&gt;, ## # X2020.11.24 &lt;int&gt;, X2020.11.23 &lt;int&gt;, ## # X2020.11.22 &lt;int&gt;, X2020.11.21 &lt;int&gt;, ## # X2020.11.20 &lt;int&gt;, X2020.11.19 &lt;int&gt;, ## # X2020.11.18 &lt;int&gt;, X2020.11.17 &lt;int&gt;, ## # X2020.11.16 &lt;int&gt;, X2020.11.15 &lt;int&gt;, ## # X2020.11.14 &lt;int&gt;, X2020.11.13 &lt;int&gt;, ## # X2020.11.12 &lt;int&gt;, X2020.11.11 &lt;int&gt;, ## # X2020.11.10 &lt;int&gt;, X2020.11.09 &lt;int&gt;, ## # X2020.11.08 &lt;int&gt;, X2020.11.07 &lt;int&gt;, ## # X2020.11.06 &lt;int&gt;, X2020.11.05 &lt;int&gt;, ## # X2020.11.04 &lt;int&gt;, X2020.11.03 &lt;int&gt;, ## # X2020.11.02 &lt;int&gt;, X2020.11.01 &lt;int&gt;, ## # X2020.10.31 &lt;int&gt;, X2020.10.30 &lt;int&gt;, ## # X2020.10.29 &lt;int&gt;, X2020.10.28 &lt;int&gt;, ## # X2020.10.27 &lt;int&gt;, X2020.10.26 &lt;int&gt;, ## # X2020.10.25 &lt;int&gt;, X2020.10.24 &lt;int&gt;, ## # X2020.10.23 &lt;int&gt;, X2020.10.22 &lt;int&gt;, ## # X2020.10.21 &lt;int&gt;, X2020.10.20 &lt;int&gt;, ## # X2020.10.19 &lt;int&gt;, X2020.10.18 &lt;int&gt;, ## # X2020.10.17 &lt;int&gt;, X2020.10.16 &lt;int&gt;, ## # X2020.10.15 &lt;int&gt;, X2020.10.14 &lt;int&gt;, ## # X2020.10.13 &lt;int&gt;, X2020.10.12 &lt;int&gt;, ## # X2020.10.11 &lt;int&gt;, X2020.10.10 &lt;int&gt;, ## # X2020.10.09 &lt;int&gt;, X2020.10.08 &lt;int&gt;, ## # X2020.10.07 &lt;int&gt;, X2020.10.06 &lt;int&gt;, ## # X2020.10.05 &lt;int&gt;, X2020.10.04 &lt;int&gt;, ## # X2020.10.03 &lt;int&gt;, X2020.10.02 &lt;int&gt;, ## # X2020.10.01 &lt;int&gt;, X2020.09.30 &lt;int&gt;, ## # X2020.09.29 &lt;int&gt;, X2020.09.28 &lt;int&gt;, ## # X2020.09.27 &lt;int&gt;, X2020.09.26 &lt;int&gt;, ## # X2020.09.25 &lt;int&gt;, X2020.09.24 &lt;int&gt;, ## # X2020.09.23 &lt;int&gt;, X2020.09.22 &lt;int&gt;, ## # X2020.09.21 &lt;int&gt;, X2020.09.20 &lt;int&gt;, ## # X2020.09.19 &lt;int&gt;, X2020.09.18 &lt;int&gt;, ## # X2020.09.17 &lt;int&gt;, X2020.09.16 &lt;int&gt;, ## # X2020.09.15 &lt;int&gt;, X2020.09.14 &lt;int&gt;, ## # X2020.09.13 &lt;int&gt;, X2020.09.12 &lt;int&gt;, ## # X2020.09.11 &lt;int&gt;, X2020.09.10 &lt;int&gt;, ## # X2020.09.09 &lt;int&gt;, X2020.09.08 &lt;int&gt;, ## # X2020.09.07 &lt;int&gt;, X2020.09.06 &lt;int&gt;, ## # X2020.09.05 &lt;int&gt;, X2020.09.04 &lt;int&gt;, ## # X2020.09.03 &lt;int&gt;, X2020.09.02 &lt;int&gt;, ## # X2020.09.01 &lt;int&gt;, X2020.08.31 &lt;int&gt;, ## # X2020.08.30 &lt;int&gt;, … or we can use summarize_if(), which affects variables selected with a predicate function: # state level cumulative infected count # method 3: summarize_if() I.state &lt;- I.county %&gt;% group_by(State) %&gt;% summarize_if(is.numeric, ~ sum(.x, na.rm = TRUE)) head(I.state, 2) ## # A tibble: 2 x 327 ## State ID X2020.12.11 X2020.12.10 X2020.12.09 ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Alab… 71489 288775 284922 280187 ## 2 Ariz… 60208 394512 387529 382601 ## # … with 322 more variables: X2020.12.08 &lt;int&gt;, ## # X2020.12.07 &lt;int&gt;, X2020.12.06 &lt;int&gt;, ## # X2020.12.05 &lt;int&gt;, X2020.12.04 &lt;int&gt;, ## # X2020.12.03 &lt;int&gt;, X2020.12.02 &lt;int&gt;, ## # X2020.12.01 &lt;int&gt;, X2020.11.30 &lt;int&gt;, ## # X2020.11.29 &lt;int&gt;, X2020.11.28 &lt;int&gt;, ## # X2020.11.27 &lt;int&gt;, X2020.11.26 &lt;int&gt;, ## # X2020.11.25 &lt;int&gt;, X2020.11.24 &lt;int&gt;, ## # X2020.11.23 &lt;int&gt;, X2020.11.22 &lt;int&gt;, ## # X2020.11.21 &lt;int&gt;, X2020.11.20 &lt;int&gt;, ## # X2020.11.19 &lt;int&gt;, X2020.11.18 &lt;int&gt;, ## # X2020.11.17 &lt;int&gt;, X2020.11.16 &lt;int&gt;, ## # X2020.11.15 &lt;int&gt;, X2020.11.14 &lt;int&gt;, ## # X2020.11.13 &lt;int&gt;, X2020.11.12 &lt;int&gt;, ## # X2020.11.11 &lt;int&gt;, X2020.11.10 &lt;int&gt;, ## # X2020.11.09 &lt;int&gt;, X2020.11.08 &lt;int&gt;, ## # X2020.11.07 &lt;int&gt;, X2020.11.06 &lt;int&gt;, ## # X2020.11.05 &lt;int&gt;, X2020.11.04 &lt;int&gt;, ## # X2020.11.03 &lt;int&gt;, X2020.11.02 &lt;int&gt;, ## # X2020.11.01 &lt;int&gt;, X2020.10.31 &lt;int&gt;, ## # X2020.10.30 &lt;int&gt;, X2020.10.29 &lt;int&gt;, ## # X2020.10.28 &lt;int&gt;, X2020.10.27 &lt;int&gt;, ## # X2020.10.26 &lt;int&gt;, X2020.10.25 &lt;int&gt;, ## # X2020.10.24 &lt;int&gt;, X2020.10.23 &lt;int&gt;, ## # X2020.10.22 &lt;int&gt;, X2020.10.21 &lt;int&gt;, ## # X2020.10.20 &lt;int&gt;, X2020.10.19 &lt;int&gt;, ## # X2020.10.18 &lt;int&gt;, X2020.10.17 &lt;int&gt;, ## # X2020.10.16 &lt;int&gt;, X2020.10.15 &lt;int&gt;, ## # X2020.10.14 &lt;int&gt;, X2020.10.13 &lt;int&gt;, ## # X2020.10.12 &lt;int&gt;, X2020.10.11 &lt;int&gt;, ## # X2020.10.10 &lt;int&gt;, X2020.10.09 &lt;int&gt;, ## # X2020.10.08 &lt;int&gt;, X2020.10.07 &lt;int&gt;, ## # X2020.10.06 &lt;int&gt;, X2020.10.05 &lt;int&gt;, ## # X2020.10.04 &lt;int&gt;, X2020.10.03 &lt;int&gt;, ## # X2020.10.02 &lt;int&gt;, X2020.10.01 &lt;int&gt;, ## # X2020.09.30 &lt;int&gt;, X2020.09.29 &lt;int&gt;, ## # X2020.09.28 &lt;int&gt;, X2020.09.27 &lt;int&gt;, ## # X2020.09.26 &lt;int&gt;, X2020.09.25 &lt;int&gt;, ## # X2020.09.24 &lt;int&gt;, X2020.09.23 &lt;int&gt;, ## # X2020.09.22 &lt;int&gt;, X2020.09.21 &lt;int&gt;, ## # X2020.09.20 &lt;int&gt;, X2020.09.19 &lt;int&gt;, ## # X2020.09.18 &lt;int&gt;, X2020.09.17 &lt;int&gt;, ## # X2020.09.16 &lt;int&gt;, X2020.09.15 &lt;int&gt;, ## # X2020.09.14 &lt;int&gt;, X2020.09.13 &lt;int&gt;, ## # X2020.09.12 &lt;int&gt;, X2020.09.11 &lt;int&gt;, ## # X2020.09.10 &lt;int&gt;, X2020.09.09 &lt;int&gt;, ## # X2020.09.08 &lt;int&gt;, X2020.09.07 &lt;int&gt;, ## # X2020.09.06 &lt;int&gt;, X2020.09.05 &lt;int&gt;, ## # X2020.09.04 &lt;int&gt;, X2020.09.03 &lt;int&gt;, ## # X2020.09.02 &lt;int&gt;, X2020.09.01 &lt;int&gt;, ## # X2020.08.31 &lt;int&gt;, … It is sometimes useful to rearrange the result of a query to inspect the values. For instance, we can sort on X2020.12.11 to put the group with the largest cumulative infected count first using the arrange() function: # state level cumulative infected count # method 4: sort by the cum. infected count I.state &lt;- I.county %&gt;% group_by(State) %&gt;% summarize_if(is.numeric, ~ sum(.x, na.rm = TRUE)) %&gt;% arrange(desc(X2020.12.11)) head(I.state, 2) ## # A tibble: 2 x 327 ## State ID X2020.12.11 X2020.12.10 X2020.12.09 ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Cali… 3.51e5 1516215 1482551 1448987 ## 2 Texas 1.23e7 1388909 1374143 1359740 ## # … with 322 more variables: X2020.12.08 &lt;int&gt;, ## # X2020.12.07 &lt;int&gt;, X2020.12.06 &lt;int&gt;, ## # X2020.12.05 &lt;int&gt;, X2020.12.04 &lt;int&gt;, ## # X2020.12.03 &lt;int&gt;, X2020.12.02 &lt;int&gt;, ## # X2020.12.01 &lt;int&gt;, X2020.11.30 &lt;int&gt;, ## # X2020.11.29 &lt;int&gt;, X2020.11.28 &lt;int&gt;, ## # X2020.11.27 &lt;int&gt;, X2020.11.26 &lt;int&gt;, ## # X2020.11.25 &lt;int&gt;, X2020.11.24 &lt;int&gt;, ## # X2020.11.23 &lt;int&gt;, X2020.11.22 &lt;int&gt;, ## # X2020.11.21 &lt;int&gt;, X2020.11.20 &lt;int&gt;, ## # X2020.11.19 &lt;int&gt;, X2020.11.18 &lt;int&gt;, ## # X2020.11.17 &lt;int&gt;, X2020.11.16 &lt;int&gt;, ## # X2020.11.15 &lt;int&gt;, X2020.11.14 &lt;int&gt;, ## # X2020.11.13 &lt;int&gt;, X2020.11.12 &lt;int&gt;, ## # X2020.11.11 &lt;int&gt;, X2020.11.10 &lt;int&gt;, ## # X2020.11.09 &lt;int&gt;, X2020.11.08 &lt;int&gt;, ## # X2020.11.07 &lt;int&gt;, X2020.11.06 &lt;int&gt;, ## # X2020.11.05 &lt;int&gt;, X2020.11.04 &lt;int&gt;, ## # X2020.11.03 &lt;int&gt;, X2020.11.02 &lt;int&gt;, ## # X2020.11.01 &lt;int&gt;, X2020.10.31 &lt;int&gt;, ## # X2020.10.30 &lt;int&gt;, X2020.10.29 &lt;int&gt;, ## # X2020.10.28 &lt;int&gt;, X2020.10.27 &lt;int&gt;, ## # X2020.10.26 &lt;int&gt;, X2020.10.25 &lt;int&gt;, ## # X2020.10.24 &lt;int&gt;, X2020.10.23 &lt;int&gt;, ## # X2020.10.22 &lt;int&gt;, X2020.10.21 &lt;int&gt;, ## # X2020.10.20 &lt;int&gt;, X2020.10.19 &lt;int&gt;, ## # X2020.10.18 &lt;int&gt;, X2020.10.17 &lt;int&gt;, ## # X2020.10.16 &lt;int&gt;, X2020.10.15 &lt;int&gt;, ## # X2020.10.14 &lt;int&gt;, X2020.10.13 &lt;int&gt;, ## # X2020.10.12 &lt;int&gt;, X2020.10.11 &lt;int&gt;, ## # X2020.10.10 &lt;int&gt;, X2020.10.09 &lt;int&gt;, ## # X2020.10.08 &lt;int&gt;, X2020.10.07 &lt;int&gt;, ## # X2020.10.06 &lt;int&gt;, X2020.10.05 &lt;int&gt;, ## # X2020.10.04 &lt;int&gt;, X2020.10.03 &lt;int&gt;, ## # X2020.10.02 &lt;int&gt;, X2020.10.01 &lt;int&gt;, ## # X2020.09.30 &lt;int&gt;, X2020.09.29 &lt;int&gt;, ## # X2020.09.28 &lt;int&gt;, X2020.09.27 &lt;int&gt;, ## # X2020.09.26 &lt;int&gt;, X2020.09.25 &lt;int&gt;, ## # X2020.09.24 &lt;int&gt;, X2020.09.23 &lt;int&gt;, ## # X2020.09.22 &lt;int&gt;, X2020.09.21 &lt;int&gt;, ## # X2020.09.20 &lt;int&gt;, X2020.09.19 &lt;int&gt;, ## # X2020.09.18 &lt;int&gt;, X2020.09.17 &lt;int&gt;, ## # X2020.09.16 &lt;int&gt;, X2020.09.15 &lt;int&gt;, ## # X2020.09.14 &lt;int&gt;, X2020.09.13 &lt;int&gt;, ## # X2020.09.12 &lt;int&gt;, X2020.09.11 &lt;int&gt;, ## # X2020.09.10 &lt;int&gt;, X2020.09.09 &lt;int&gt;, ## # X2020.09.08 &lt;int&gt;, X2020.09.07 &lt;int&gt;, ## # X2020.09.06 &lt;int&gt;, X2020.09.05 &lt;int&gt;, ## # X2020.09.04 &lt;int&gt;, X2020.09.03 &lt;int&gt;, ## # X2020.09.02 &lt;int&gt;, X2020.09.01 &lt;int&gt;, ## # X2020.08.31 &lt;int&gt;, … In the above, desc() is used to re-oorder by a column in descending order. 3.5 Combine Data Sets R has a number of quick, elegant ways to join data frames by a common column. There are at least three ways: Base R’s merge() function, Join family of functions from dplyr, and Bracket syntax based on data.table. 3.5.1 The join family The dplyr uses SQL database syntax for its join functions. For example, a left join means: Include everything on the left and all rows that match from the right data frame. If the join columns have the same name, all you need is left_join(x, y). If they don’t have the same name, you need a by argument, such as left_join(x, y, by = c(\"df1ColName\" = \"df2ColName\")). See an illustration in Figure 3.3. Figure 3.3: An illustration of left join and right join. Different join functions control what happens to rows that exist in one table but not the other. left_join keeps all the entries that are present in the left (first) table and excludes any that are only in the right table. right_join keeps all the entries that are present in the right table and excludes any that are only in the left table. inner_join keeps only the entries that are present in both tables. inner_join is the only function that guarantees you won’t generate any missing entries. full_join keeps all of the entries in both tables, regardless of whether or not they appear in the other table. Figure 3.4: An illustration of inner join and full join. The join functions are nicely illustrated in RStudio’s Data wrangling cheatsheet. Figure 3.5: An illustration of the join functions in RStudio’s Data wrangling cheatsheet. 3.5.2 Toy examples with joins a &lt;- tibble(x1 = LETTERS[c(1:3)], x2 = 1:3) b &lt;- tibble(x1 = LETTERS[c(1, 2, 4)], x3 = c(T, F, T)) a ## # A tibble: 3 x 2 ## x1 x2 ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 C 3 b ## # A tibble: 3 x 2 ## x1 x3 ## &lt;chr&gt; &lt;lgl&gt; ## 1 A TRUE ## 2 B FALSE ## 3 D TRUE # include all rows in a and b inner_join(a, b, by = &quot;x1&quot;) ## # A tibble: 2 x 3 ## x1 x2 x3 ## &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 A 1 TRUE ## 2 B 2 FALSE # return all rows from a left_join(a, b, by = &quot;x1&quot;) ## # A tibble: 3 x 3 ## x1 x2 x3 ## &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 A 1 TRUE ## 2 B 2 FALSE ## 3 C 3 NA # return all rows from b right_join(a, b, by = &quot;x1&quot;) ## # A tibble: 3 x 3 ## x1 x2 x3 ## &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 A 1 TRUE ## 2 B 2 FALSE ## 3 D NA TRUE # includes all rows in a or b full_join(a, b, by = &quot;x1&quot;) ## # A tibble: 4 x 3 ## x1 x2 x3 ## &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 A 1 TRUE ## 2 B 2 FALSE ## 3 C 3 NA ## 4 D NA TRUE # include the rows in a that are not in b anti_join(a, b, by = &quot;x1&quot;) ## # A tibble: 1 x 2 ## x1 x2 ## &lt;chr&gt; &lt;int&gt; ## 1 C 3 # want everything that doesn&#39;t match? full_join(anti_join(a, b, by = &quot;x1&quot;), anti_join(b, a, by = &quot;x1&quot;), by = &quot;x1&quot;) ## # A tibble: 2 x 3 ## x1 x2 x3 ## &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 C 3 NA ## 2 D NA TRUE 3.5.3 Practice with joins for real data Example 1 We first get the data named pop.county from the github R package slid. Note that there are four variables in this data: “ID” (county-level Federal Information Processing System code), “County” (name of county matched with “ID”), “State” (name of state matched with “ID”), population (population of county matched with “ID”). data(I.county) data(pop.county) # make I.county a tibble with as_tibble() I.county &lt;- as_tibble(I.county) dim(I.county) ## [1] 3104 328 # make pop.county a tibble with as_tibble() pop.county &lt;- as_tibble(pop.county) dim(pop.county) ## [1] 3142 4 Now, we would like to join the two tables: I.county and pop.county using the left_join as follows: pop.county.tmp &lt;- pop.county %&gt;% dplyr::select(-c(County, State)) I.county.w.pop &lt;- left_join(I.county, pop.county.tmp, by = &quot;ID&quot;) or we can: I.county.w.pop &lt;- left_join(I.county, dplyr::select(pop.county, c(ID, population)), by = &quot;ID&quot;) Example 2 In this example, we would like to create a map to show the risk of COVID-19 infection in each state of the US. So, first, we need to have a new dataset that contains infection risk and the geographic information of each state. We will get infected count and state population in the state.long dataset in the slid package. state.long is a tibble with 15,925 rows and 7 columns. Next, we obtain the boundary information of each state downloaded from PublicaMundi. Then, we merge the two datasets to create a new dataset. We need the R package geojsonio to read the data from PublicaMundi. library(geojsonio) ## Registered S3 method overwritten by &#39;geojsonsf&#39;: ## method from ## print.geojson geojson ## ## Attaching package: &#39;geojsonio&#39; ## The following object is masked from &#39;package:base&#39;: ## ## pretty library(slid) data(state.long) # get the geospatial information from PublicaMundi urlRemote &lt;- &quot;https://raw.githubusercontent.com/&quot; pathGithub &lt;- &quot;PublicaMundi/MappingAPI/master/data/geojson/&quot; fileName &lt;- &quot;us-states.json&quot; states0 &lt;- geojson_read(x = paste0(urlRemote, pathGithub, fileName), what = &quot;sp&quot;) head(states0@data) ## id name density ## 1 01 Alabama 94.650 ## 2 02 Alaska 1.264 ## 3 04 Arizona 57.050 ## 4 05 Arkansas 56.430 ## 5 06 California 241.700 ## 6 08 Colorado 49.330 states1 &lt;- states0 states1@data &lt;- states0@data %&gt;% # remove the space in the name of state if there is one mutate(name_ns = sapply(name, gsub, pattern = &quot; &quot;, replacement = &quot;&quot;)) # the following merge step can be done both using sp::merge # or the join functions in dplyr # states1 &lt;- sp::merge(states1, state.long %&gt;% # filter(DATE == as.Date(&#39;2020-12-11&#39;)), # by.x = &quot;name_ns&quot;, by.y = &quot;State&quot;) # merge the two datasets states1@data &lt;- left_join(states1@data, state.long %&gt;% filter(DATE == as.Date(&#39;2020-12-11&#39;)), by = c(&#39;name_ns&#39; = &#39;State&#39;)) # calculate the risk of infection states1@data &lt;- states1@data %&gt;% mutate(Infect_risk = Infected / pop) 3.6 Reshaping Data Sometimes, we want to convert data from a wide format to a long format. Many functions in R expect data to be in a long format rather than a wide format. Programs like SPSS, however, often use wide-formatted data. Take the dataset I.state for example, the column names “XYYYY.MM.DD” are not names of variables, but values of a variable, which contains the values of cumulative infected count. We need to pivot the column names into new variables. There are two sets of methods that are explained below: gather() and spread() from the tidyr package. This is a newer interface to the reshape2 package. pivot_longer and pivot_wider from the tidyr package. Many other methods aren’t covered here since they are not as easy to use. 3.6.1 From wide to long Below we would like to change the data I.state from wide format to long format. library(slid) data(I.state) names(I.state) Use gather(data, key, value, ...) data = the dataframe you want to morph from wide to long key = the name of the new column that is levels of what is represented in the wide format as many columns value = the name of the column that will contain the values ... = columns to gather, or leave (use -column to gather all except that one) The gather functions are nicely illustrated in RStudio’s [Data wrangling cheatsheet][https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf] as shown in Figure 3.6. Figure 3.6: An illustration of gather function. # method 1 I.state.wide &lt;- I.state dim(I.state.wide) ## [1] 49 326 I.state.long &lt;- gather(I.state.wide, DATE, Infected, X2020.12.11:X2020.01.22, factor_key = TRUE) %&gt;% arrange(State) dim(I.state.long) ## [1] 15925 3 Use pivot_longer() The function pivot_longer() is an updated approach to gather(), designed to be both simpler to use and to handle more use cases. It is recommended to use pivot_longer() for new code; gather() isn’t going away but is no longer under active development. # method 2 I.state.wide &lt;- I.state dim(I.state.wide) ## [1] 49 326 I.state.long &lt;- I.state.wide %&gt;% pivot_longer(X2020.12.11:X2020.01.22, names_to = &quot;DATE&quot;, values_to = &quot;Infected&quot;) See more complicated examples from the introduction of ‘tidyr’ package. 3.6.2 From long to wide Now let’s change the data back to the wide format, and we can use spread. Use Use spread(data, key, value) data = the dataframe you want to morph from long to wide key = the name of the column that contains the key value = the name of the column contains the values The spread functions are nicely illustrated in RStudio’s [Data wrangling cheatsheet][https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf] as shown in Figure 3.7. Figure 3.7: An illustration of spread function. # method 1 I.state.wide &lt;- spread(I.state.long, DATE, Infected) dim(I.state.wide) ## [1] 49 326 Use pivot_wider() We can also use the function pivot_wider(), which “widens” data, increasing the number of columns and decreasing the number of rows. The inverse transformation is pivot_longer(). # method 2 I.state.wide &lt;- I.state.long %&gt;% pivot_wider(names_from = DATE, values_from = Infected) dim(I.state.wide) ## [1] 49 326 See more complicated examples from the introduction of ‘tidyr’ package. 3.7 Exercises We are going to explore the basic data manipulation verbs of dplyr using I.county. Install the github R package slid. library(slid) data(I.county) Obtain a subset of the I.county by selecting ID, County, State. Obtain a subset of the I.county by including all counties in California. Obtain a subset of the I.county by including all counties that in the midwest states. Midwest = c( &quot;Illinois&quot;, &quot;Michigan&quot;, &quot;Indiana&quot; ,&quot;Ohio&quot;, &quot;Wisconsin&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Minnesota&quot;, &quot;Missouri&quot;, &quot;Nebraska&quot; , &quot;SouthDakota&quot; , &quot;NorthDakota&quot;) Obtain a subset of the I.county by including the top ten counties from each midwest state based on the cumulative infected count on December 11, 2020. Obtain a subset of the I.county by including all the counties California with the culumative infected counts up till Judy 31, 2020. Create new columns of I.county by taking the logarithm of each of the count column. Sort the cumulative infected count on December 11, 2020 to find the state with the largest cumulative infected count. Downlad the data pop.county from the github R package slid. Join the tables of I.county and pop.county using inner_join, left_join , right_join, full_join. Do you get same or different tables? Based on the inner_join create a table and name it as I.pop.county, then create new columns of I.pop.county by dividing each of the count column by the popolation in the corresponding county, for example, risk.2020.12.11 = X.2020.12.11 / pop. For each state, list the top ten county with the highest risk based on risk.2020.12.11 "],["ggplot2.html", "Chapter 4 Data Visualization with ggplot2 4.1 Introduction 4.2 Types of Variables and Preparation 4.3 Position Scales and Axes 4.4 Color Scales and Size of geom_points() 4.5 Individual geoms 4.6 Collective geoms 4.7 Time Series 4.8 Maps 4.9 Arranging Plots 4.10 Output 4.11 Exercises", " Chapter 4 Data Visualization with ggplot2 4.1 Introduction The first thing to do is to plot the data in the epidemiologic data analysis task. Data visualization enables many features of the data to be displayed or summarized in a graphical format, including patterns, changes over time, unusual observations, relationships among variables, and spatial variations. The features that are seen in graphs of the data must then be incorporated as much as possible into the modeling or forecasting methods. There are many types of graphs available, each with its own strengths and use cases. One of the challenges in the statistical learning process is choosing the appropriate visualization method to represent the data. Before constructing any display of epidemiologic data, it is important to understand the type of task that we want to perform and determine the information to convey. Some common roles for data visualization include: highlighting a change from past patterns in the data; displaying a part-to-whole composition; showing how data is distributed; showing a difference or similarity between groups; displaying the spatial variation in geographical data; illustrating relationships among variables. When the data is more complex, visualize broader patterns with graphs. Graphs can also visualize trends as well as identify variations from those trends. Variations in data may represent important new findings or could just be errors in typing or coding that need to be corrected. Thus, graphs can be helpful tools to aid in verifying and analyzing the data. Once an analysis is complete, graphs further serve as useful visual aids for describing the data to others. This chapter will introduce the ggplot2, and we will gain insight and practical skills for visualization of infectious disease data. Recommended Reading: https://ggplot2.tidyverse.org https://opr.princeton.edu/workshops/Downloads/ https://ggplot2-book.org/introduction.html ggplot2 builds on Leland Wilkinson’s The Grammar of Graphics and focuses on the primacy of layers and adapts it for R. In brief, the grammar tells us that a graphic maps the data to the aesthetic attributes (color, shape, size) of geometric objects (points, lines, bars). The plot may also include statistical transformations of the data and information about the plot’s coordinate system. Facetting can be used to plot for different subsets of the data. The combination of these independent components is what makes up a graphic. In this chapter, we will introduce the basics of ggplot2 grammar and some of the key features, including the use of geom, stat, scale, coord, and facet. 4.2 Types of Variables and Preparation Keep in mind that the primary purpose of preparing graphs is to communicate information. The types of variables we are analyzing and the media for the visualization can also affect your graphics practice. 4.2.1 Types of Variables When examining data, you must know which data type you are working with in order to choose an appropriate display format. The data are most likely going to be in one of the following categories: Categorical (Qualitative) variables A nominal variable is one whose values are categories without any numerical ranking. Good examples are occupation, place of birth, county of residence and diagnosis. Nominal variable is called dichotomous when it is characterised by only two classes. In epidemiology, it is common to see dichotomous variables: sex (male/female), exposure history (yes/no), alive or dead, ill or well, vaccinated or un-vaccinated. An ordinale variable has values that can be ranked but are not necessarily evenly spaced. For example, severity of illness may be categorized and ordered as “mild,” “moderate” or “severe.” Numerical (Quantitative) variables There are two types of quantitative variables: Discrete variables have values that are distinct and separate. Discrete data can’t be measured but can be counted. For example, the number of new cases of a certain disease in a given year. Continuous variables represents measurements and can have any value in a range. Examples of continuous data would be the amount of the time period between when you catch a virus and when your symptoms start. Discrete data can’t be counted but can be measured. An interval-scale variable is measured on a scale of equally spaced units, but without a true zero point. An example of interval data is date of birth. A ratio-scale variable is the same as interval values, with the difference that they do have an absolute zero. Good examples are be height in centimeters or duration of illness. 4.2.2 Rules for Graph Designing When designing graphs, we need to follow some rules to achieve the best practices, and Dicker and Gathany (1992) suggests the following: Check to ensure that a graphic can stand alone by clearly labeling the title, sources, axes, scales, and legends; Identify variables portrayed (legends or keys), which includes units of measure; Minimize the number of lines on a graph; Generally, we portray frequency on the vertical scale, starting at zero while we portray classification variable on the horizontal scale; Check to ensure that the scales for each axis are appropriate for data presented; Define any abbreviations or symbols Specify any data excluded. 4.2.3 Installing packages and loading data Before we begin, please get ready by installing the ggplot2 package by any of the following method. # The easiest way to get ggplot2 is to install the whole tidyverse: install.packages(&quot;tidyverse&quot;) # Alternatively, install just ggplot2: install.packages(&quot;ggplot2&quot;) # Or the development version from GitHub: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;tidyverse/ggplot2&quot;) Note: devtools downloads and installs the package from GitHub. By default, install.packages() is only able to install packages that are available in Comprehensive R Archive Network (CRAN). In that case, the developer’s tool, devtools::install_github() enables users to install packages that have not been submitted to CRAN, but is available in GitHub. In addition, we need to library the required packages as following. In the rest of this section, we demonstrate how to create a basic scatter plot and output the figure in “png” and “rds” format. To create a ggplot2 plot, we need to know three key components: A dataframe with each column being an attribute/variable, each row being an individual; A set of aesthetic mappings between variables in the data and visual properties. At least one layer which describes how to render each observation. Layers are usually created with a geom function. “ggplot” generally prefers data in the “long” format: i.e., a column for every dimension, and a row for every observation. For illustration, we are going to use the state.long data set in the R package slid. To prepare the data, install the R package slid from Github using the following command, which includes the data sets that we use for this book. Open the state.long dataset, which includes the variables, cumulative infected cases (Infected), cumulative death counts (Death), Region, Division, State, population (pop), and DATE, starting from Jan 22, 2020. Take a look at the first few lines using head(). df &lt;- slid::state.long head(df) ## # A tibble: 6 x 7 ## State Region Division pop DATE Infected Death ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 Alaba… South East South… 4.89e6 2020-12-11 288775 4086 ## 2 Alaba… South East South… 4.89e6 2020-12-10 284922 4034 ## 3 Alaba… South East South… 4.89e6 2020-12-09 280187 3985 ## 4 Alaba… South East South… 4.89e6 2020-12-08 276665 3940 ## 5 Alaba… South East South… 4.89e6 2020-12-07 272228 3891 ## 6 Alaba… South East South… 4.89e6 2020-12-06 269877 3888 4.2.4 Your first scatterplot Here we introduce how to draw a simple scatter plot using the data of ‘2020-12-11.’ Treat log(Infected) as the x-axis, and log(Death) as the y-axis. We create it by telling “ggplot” the data df, the aesthetic mapping aes(log(Infected), log(Death)), and the layer geom_point(). The structure ggplot() + geom_point() is the typical way to create a plot, in which “ggplot” is told the data and mapping, and geom_point is a layer of a picture using the information embedded in “ggplot.” Later in this chapter, you will see how we can use + to assign additional adjustments and add multiple layers to the existing figure. # Select the date df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) # Create scatter plot # Data: df # Aesthetic: first mapped to x, second mapped to y # Layer: render the plot as a scatterplot p &lt;- ggplot(df, aes(log(Infected + 1), log(Death + 1))) p + geom_point() Figure 4.1: Scatterplot of log cumulative death against log cumulative infected Remarks: In the above example, the dataframe df is the first parameter in the above ggplot(), and aesthetics are defined within an aes() function. We need to place + at the end of the previous line instead of the beginning of new line. Aesthetics are properties of the plot that can show certain elements of the data. The following is a list of some common plot aesthetics you might want to specify in your geom_point(): x: position on x-axis y: position on y-axis alpha: transparency (1: opaque; 0: transparent) color: color of border of elements fill: color of inside of elements shape: shape group: group size: size stroke: border size of points We will explain more details in the following sections. 4.3 Position Scales and Axes 4.3.1 Change the labels of the axis using xlab() and ylab() See Figure 4.2 for the customized labels and title. # Change the transparency using alpha p &lt;- p + geom_point(alpha = 0.7) + # Change the label of horizontal axis xlab(&#39;log Infected&#39;) + # Change the label of vertical axis ylab(&#39;log Death&#39;) + # Change the title labs(title = &#39;Log death against infected cases in US&#39;) p Figure 4.2: Scatterplot with customized lables and title #p + scale_x_reverse() #p + scale_y_reverse() 4.3.2 Change the range of the axis using xlim() and ylim() For continuous variables, we can provide the lower and upper limits. For categorical variables, we can provide the names of categories desired. To suppress the warning “Removed XXX rows containing missing values,” use na.rm. This needs to be carefully used because the data outside the range are converted to NA, which will affect later manipulations, such as calculating the mean or sum. # For continuous variable, provide the lower and upper limits p &lt;- p + geom_point() + ylim(4, 12) p Figure 4.3: Scatterplot with customized axis range for continuous features # For discrete variable, provide the names of categories desired # To suppress the warning &#39;Removed XXX rows containing...&#39;, use `na.rm`. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) ggplot(df, aes(Region, log(Death + 1))) + geom_point(na.rm = TRUE) + xlim(&#39;West&#39;, &#39;Midwest&#39;, &#39;South&#39;) Figure 4.4: Scatterplot with customized axis range for discrete features 4.4 Color Scales and Size of geom_points() There are two ways of coloring. One approach is to color all points with the same color. The other method is to color the points according to a particular feature of the observation. 4.4.1 Change the color of all points p + geom_point(color = &quot;blue&quot;) Figure 4.5: Scatterplot with all points colored blue 4.4.2 Color the observations by the value of a feature # Use Region as the feature for coloring p + geom_point(aes(color = Region)) # p + aes(color = Region) # Use population for coloring p + geom_point(aes(color = pop)) Figure 4.6: Scatterplot with points colored by Region or Population Remarks: the color feature is located at different layers in three figures. In the first figure, it is under geom_point(), while in the latter two figures it is under geom_point(aes()). Because we only have one layer in this example, we can equivalently use aes(), i.e., the aesthetic mapping for the whole scatterplot. 4.4.3 Change the color palette In addition, we can personalize the color palette using scale_fill_brewer() for a discrete color scale, and scale_fill_distiller() for a continuous color scale. # Change the palette # For discrete scale p + geom_point(aes(color = Region)) + scale_fill_brewer(palette = &quot;Set1&quot;, aesthetics = &quot;color&quot;) # For continuous scale p + geom_point(aes(color = pop)) + scale_fill_distiller(palette = 2, aesthetics = &quot;color&quot;) Figure 4.7: Scatterplot with customized color palette 4.4.4 Change the size by the value of a feature p &lt;- ggplot(df, aes(log(Infected + 1), log(Death + 1))) + xlab(&#39;log Infected&#39;) + ylab(&#39;log Death&#39;) + labs(title = &#39;Log death against infected cases in US&#39;) # Change the point size p + geom_point(aes (size = pop)) # Change the point color and size p + geom_point(aes (size = pop , color = pop)) Figure 4.8: Scatterplot with customized point size or color # Combine the color and size in legend # Method 1: keep the size and color the same limits and breaks p + geom_point(aes (size = pop, color = pop)) + scale_color_continuous(limits = c(0e7, 4e7), breaks = seq(0, 4e7, by = 1e7)) + scale_size_area(limits = c(0e7, 4e7), breaks = seq(0, 4e7, by = 1e7), max_size = 12) + guides(color = guide_legend(), size = guide_legend()) # Method 2: use scale_color_gradient and scale_size p &lt;- p + geom_point(aes(size = pop, color = pop), alpha = 0.7) + scale_color_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) + scale_size_area(max_size = 12) + guides(color = guide_legend(), size = guide_legend()) p Figure 4.9: Scatterplot with customized point color and point size Remarks: For Method 1, the key to combining two aesthetic settings of the layer to one legend, in this case, color and size, is to set the limits and breaks to be the same in guides. For both methods, guides(color = guide_legend(), size = guide_legend()) is needed. 4.5 Individual geoms Apart from the scatter plot, there are many individual geoms, for example: geom_line(): line graphs geom_boxplot():boxplots geom_bar(): bar chart geom_histogram(): histogram plots geom_smooth(): regression lines or curves We introduce a few of them in detail as follows. 4.5.1 Histogram The histogram is an important tool to summarize the range and frequency of observations. Here we plot the histogram of log daily new infected cases counts using using geom_histogram. We can adjust the option binwidth to control the widths of the bins. # Prepare the daily new Infected for each state # in the period 2020-11-12 to 2020-12-11 df &lt;- slid::state.long %&gt;% dplyr::filter(DATE &lt;= &#39;2020-12-11&#39; &amp; DATE &gt; &#39;2020-11-11&#39;) %&gt;% group_by(State) %&gt;% # Group by State mutate(Y.Infected = c(Infected[-length(Infected)] - Infected[-1], 0)) df ## # A tibble: 1,470 x 8 ## # Groups: State [49] ## State Region Division pop DATE Infected Death ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 2 Alab… South East So… 4.89e6 2020-12-10 284922 4034 ## 3 Alab… South East So… 4.89e6 2020-12-09 280187 3985 ## 4 Alab… South East So… 4.89e6 2020-12-08 276665 3940 ## 5 Alab… South East So… 4.89e6 2020-12-07 272228 3891 ## 6 Alab… South East So… 4.89e6 2020-12-06 269877 3888 ## 7 Alab… South East So… 4.89e6 2020-12-05 267589 3876 ## 8 Alab… South East So… 4.89e6 2020-12-04 264199 3831 ## 9 Alab… South East So… 4.89e6 2020-12-03 260359 3776 ## 10 Alab… South East So… 4.89e6 2020-12-02 256828 3711 ## # … with 1,460 more rows, and 1 more variable: ## # Y.Infected &lt;dbl&gt; p &lt;- ggplot(df, aes(log(Y.Infected + 1))) p + geom_histogram(binwidth = 1) p + geom_histogram(binwidth = 1) + aes(fill = Region) Figure 4.10: Histogram examples 4.5.2 Bar chart The discrete analogue of histogram is the bar chart. 4.5.3 Default bar chart The default geom_bar(), or equivalently geom_bar(stat='count'), counts the number of observations in each category shown as following. This plot essentially tells us how many states there are in each region. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) p &lt;- ggplot(df, aes(Region)) p + geom_bar() Figure 4.11: Bar plot example 4.5.4 Bar chart with assigned value In addition to the previous example, we can assign the height of the bars by ourselves by using the option geom_bar(stat = 'identity'). In that case, we tell geom_bar to use y value in the data frame as the height of the bars. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) p &lt;- ggplot(df, aes(Region, Infected)) p + geom_bar(stat = &#39;identity&#39;) p + geom_bar(stat = &#39;identity&#39;, aes(fill = Division)) Figure 4.12: Bar plot with assigned values 4.5.5 Legend Legend position We can adjust the position of the legends using theme(legend.position = 'left/right/bottom/none'). p &lt;- ggplot(df, aes(Region, fill = Region)) + ylab(&#39;Number of states&#39;) + geom_bar() p + theme(legend.position = &#39;bottom&#39;) Figure 4.13: Scatterplot with legend at bottom Legend guide guide_legend() We can also assign individual keys to the legend using options of guide_legend(). Here we introduce the most useful options. nrow and ncol: specify the dimensions of the table. byrow: fills the rows, set to FALSE by default. p + guides(fill = guide_legend(ncol = 2, byrow = TRUE)) Figure 4.14: Bar plots of number of states in each region reverse: reverse the order of the keys p + guides(fill = guide_legend(reverse = TRUE)) Figure 4.15: Bar plots of number of states in each region 4.5.6 Boxplots, jittering and violin plots Conditioning on a categorical feature, or conditioning on groups, we may want to conduct a side-by-side comparison for a certain variable. We can use the following tools. Jittering, geom_jitter(), adds a small amount of random noise to the data, which can help avoid over-plotting. Boxplots, geom_boxplot(), summarizes the shape of the distribution with summary statistics. Violin plots, geom_violin(), shows a compact representation of the “density” of the distribution and highlights the areas where more points are found. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) %&gt;% mutate(Risk = Infected / pop) p &lt;- ggplot(df, aes(Region, Risk, color = Region)) p + geom_point() p + geom_jitter() p + geom_boxplot() p + geom_violin() Figure 4.16: Points, jittering, boxplot, and violin plot examples 4.6 Collective geoms An individual “geom” can draw a distinct graphical object for each observation (row). For example, the “point geom” draws one point per row. Several “geoms” may be added to the same ggplot object, which will let you build layers to design complex graphs as well as displays multiple observations with one geometric object. For example, we have previously created a scatter plot, and then we can add regressed lines on the top of the scatter plot layer. You can add more information from a statistical summary, or add a text geom to annotate your plot. 4.6.1 Smoother On top of the scatter plot, we can add regressed lines and prediction band to it using geom_smooth(). The “loess” method By default, the model for small data is “loess,” we can call the layer either by geom_smooth() or geom_smooth(method ='loess'). In addition, we can adjust option span to control the wiggliness of the line. The higher span is, the less wiggle the line will be. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) p &lt;- ggplot(df, aes(log(Infected + 1), log(Death + 1))) + geom_point() # Use `span` to control the wiggliness of the line # The higher `span` is, the less wiggle the line is p + geom_smooth(method = &#39;loess&#39;, span = 0.5) p + geom_smooth(method = &#39;loess&#39;, span = 1) Figure 4.17: loess smoother examples Linear regression method We can also use method = 'lm' to fit a simple linear model: # Fit a linear model p + geom_smooth(method = &#39;lm&#39;) Figure 4.18: Linear regression estimator. After introducing the idea of individual and collective geoms, we would like to spend the rest of the chapter discussing two important collective plots, time series plots and maps. We will build them step by step from scratch. 4.7 Time Series 4.7.1 Basic line plots In traditional time series plots, we use time as the x-axis variable, and plot the time series using geom_line(). We consider the time series of the daily new infected count for Iowa. df &lt;- slid::state.long %&gt;% dplyr::filter(State == &quot;Iowa&quot;) %&gt;% arrange(DATE) %&gt;% mutate(Y.Infected = Infected - lag(Infected)) %&gt;% dplyr::filter(!is.na(Y.Infected)) To visualize the data, we draw a time series plot first. p &lt;- ggplot(df, aes(DATE, Y.Infected)) + geom_line() + labs(x = &quot;Days&quot;, y = &quot;Count&quot;, title = &#39;Daily new infected cases in Iowa&#39;) p Figure 4.19: Basic time series 4.7.2 Add a second line Next, we display the prediction results on the time series plot. The prediction and prediction intervals for the next 14 days are saved in the dataset: slid::fore. # Data Preparation if(!require(&#39;lubridate&#39;)) install.packages(&#39;lubridate&#39;) library(lubridate) df.pred &lt;- as.data.frame(slid::fore[c(&#39;mean&#39;, &#39;lower&#39;, &#39;upper&#39;)]) names(df.pred) &lt;- c(&#39;mean&#39;, &#39;lower&#39;, &#39;upper&#39;) df.pred$DATE &lt;- tail(df$DATE,1) + c(1:length(slid::fore$mean)) # Add a line for predicted value p + geom_line(mapping = aes(x = DATE, y = mean, color = &#39;Predicted Value&#39;), linetype = &quot;dashed&quot;, # Set the line type in legend key_glyph = &quot;timeseries&quot;, data = df.pred) + scale_color_manual(&quot;&quot;, values = &quot;red&quot;) Figure 4.20: Time series with added predictions 4.7.3 Add ribbons Next, we show the prediction intervals. On top of the line plots, we can add another layer to the existing line, and create a line with two parts. # Add prediction intervals p &lt;- p + geom_ribbon(mapping = aes(x = DATE, y = mean, ymin = lower, ymax = upper, fill = &#39;95% Prediction Intervals&#39;), data = df.pred, alpha = 0.2) + # Add line for predicted value geom_line(mapping = aes(x = DATE, y = mean, color = &#39;Predicted Value&#39;), linetype = &quot;dashed&quot;, data = df.pred, # Set the line type in legend key_glyph = &quot;timeseries&quot;) + scale_color_manual(&quot;&quot;, values = &quot;red&quot;)+ scale_fill_manual(&quot;&quot;, values = &quot;pink&quot;) p Figure 4.21: Time series with ribbons and second line Remarks: the layer added later is put on the top, therefore it is important to keep track of the order you add the layers. 4.7.4 Adjust the scale of time axis There are multiple ways to define the ticks on the axis of dates and times. There are the labeled major breaks and further the minor breaks, which are not labeled but marked by grid lines. These can be customized with the arguments date_breaks and date_minor_breaks, respectively. # Adjust the scale of time axis p + scale_x_date( limits = as.Date(c(&quot;2020-10-01&quot;, &quot;2021-01-01&quot;)), date_breaks = &quot;1 month&quot;, date_minor_breaks = &quot;1 week&quot;, date_labels = &quot;%B %Y&quot; ) Figure 4.22: Time series plot with adjusted time range and format In the above syntax, date_labels set to a string of formatting codes, defining order, format and elements to be displayed: %d: day of the month (01-31) %m: month, numeric (01-12) %b: month, abbreviated (Jan-Dec) %B: month, full (January-December) %y: year, without century (00-99) %Y: year, with century (0000-9999) 4.7.5 Add annotations It is often necessary to make annotations to the data displayed when constructing a data visualization. An annotation provides additional information around what data is being displayed. For example, adding text to a plot is one of the most common forms of annotation. The primary tool for labeling plots is geom_text(), which adds label text at the specified x and y positions. We can also add reference lines to the plot using geom_vline or geom_hline. Figure ?? shows an annotated time series plot with shades and reference lines for each quarter. # Prepare the data df &lt;- df %&gt;% mutate(start = floor_date(DATE, &quot;quarter&quot;)) %&gt;% mutate(end = ceiling_date(DATE, &quot;quarter&quot;)) %&gt;% mutate(quarters = as.factor(quarter(DATE))) df.quarters &lt;- df %&gt;% dplyr::select(start, end, quarters) %&gt;% unique() # Draw the base ggplot ggplot(df, aes(DATE, Y.Infected)) + labs(x = &quot;Days&quot;, y = &quot;Count&quot;, title = &#39;Daily new infected cases in Iowa&#39;) + # Add rectangle for each quarter geom_rect( aes(xmin = (start), xmax = (end), fill = quarters), inherit.aes = F, ymin = -Inf, ymax = Inf, alpha = .5, data = df.quarters) + scale_fill_brewer(palette = &quot;Blues&quot;, aesthetics = &quot;fill&quot;) + # Add vertical line geom_vline(aes(xintercept = as.numeric(start)), data = df, color = &quot;gray&quot;, linetype = &#39;dashed&#39;, size = 0.5) + # Add text geom_text( aes(x = start, y = 0 , label = paste0(&#39;Quarter:&#39;, quarters)), data = df.quarters, inherit.aes = F, size = 3, vjust = 0, hjust = 0, nudge_x = 20) + # Add time series lines geom_line() + geom_line(mapping = aes(x = DATE, y = mean, color = &#39;Predicted Value&#39;), linetype = &quot;dashed&quot;, data = df.pred , key_glyph = &quot;timeseries&quot;) Figure 4.23: Time series plot with shades and reference lines. 4.8 Maps In epidemilogy, data often includes geographical information such as latitude and longitude or regions like country, state or county. To plot these types of data, we can extend an existing visualization onto a map background. We will learn how to make choropleth maps, sometimes called heat maps, using the ggplot2 package. A choropleth map is a map that shows a geographic landscape with units such as countries, states, or watersheds where each unit is colored according to a particular value. # Read map and data library(ggplot2) library(maps) ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map library(dplyr) # Load United States state map data MainStates &lt;- map_data(&quot;state&quot;) head(MainStates, 3) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; MainStates &lt;- MainStates %&gt;% mutate(&#39;state&#39; = gsub(&#39; &#39;, &#39;&#39;, MainStates$region)) %&gt;% select(-c(&#39;region&#39;,&#39;subregion&#39;)) head(MainStates, 3) ## long lat group order state ## 1 -87.46201 30.38968 1 1 alabama ## 2 -87.48493 30.37249 1 2 alabama ## 3 -87.52503 30.37249 1 3 alabama state.long.shape &lt;- slid::state.long %&gt;% mutate(&#39;state&#39; = tolower(slid::state.long$State)) %&gt;% right_join(MainStates, by = &#39;state&#39;) %&gt;% select(-state) state.long.shape ## # A tibble: 5,049,525 x 11 ## State Region Division pop DATE Infected Death ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 2 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 3 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 4 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 5 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 6 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 7 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 8 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 9 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## 10 Alab… South East So… 4.89e6 2020-12-11 288775 4086 ## # … with 5,049,515 more rows, and 4 more variables: ## # long &lt;dbl&gt;, lat &lt;dbl&gt;, group &lt;dbl&gt;, order &lt;int&gt; df &lt;- state.long.shape %&gt;% dplyr::filter(DATE == &#39;2020-12-11&#39;) %&gt;% mutate (Risk = Infected / pop * 1000) 4.8.1 Making a base map Using qplot(), we can obtain our first map like this: qplot(long, lat, geom = &quot;point&quot;, data = df) Figure 4.24: Basic US map with dotted state boundaries We can use the geom_polygon() function to create a map with black borders and add light blue to fill in the map. # Plot all states with ggplot2, black borders and light blue fill ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group), color = &quot;black&quot;, fill = &quot;lightblue&quot;) Figure 4.25: US map with colored state areas 4.8.2 Customizing choropleth map Now that we have created a base map of the mainland states, we will color each state according to its the risk. Make the use of slid::ggplot_map_state dataset. # Create a Choropleth map of the United States p &lt;- ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = Risk), color = &quot;white&quot;, size = 0.2) p Figure 4.26: US map with colored state areas according to infected per thousand population Remarks Each state is colored by “Infected per 1000 people” to make the legend easier to read. Border color (white) and line thickness (size = 0.2) are specifically defined within this geom_polygon(). Once a map is created, it is often helpful to modify color schemes, determine how to address missing values (na.values), and formalize labels. Notice that we assigned the graph a name, p. This is particularly useful as we add new components to the map. p + scale_fill_continuous(name = &quot;Infected per 1000 pop&quot;, low = &quot;yellow&quot;, high = &quot;darkred&quot;, limits = c(0, 125), breaks = c(5, 25, 50, 75, 100, 125), na.value = &quot;grey50&quot;) + labs(title = &quot;Infected per 1000 population on 2020-12-11&quot;) Figure 4.27: US map with colored state areas with limits on the values 4.8.3 Overlay polygon maps It is also possible to overlay two polygon maps. The code below creates county borders with a small line size and then adds a thicker line to represent state borders. The alpha = .3 causes the fill in the state map to be transparent, allowing us to see the county map behind the state map. ggplot() + geom_polygon(data = map_data(&quot;county&quot;), aes(x = long, y = lat, group = group), color = &quot;darkblue&quot;, fill = &quot;lightblue&quot;, size = .1) + geom_polygon(data = map_data(&#39;state&#39;), aes(x = long, y = lat, group = group), color = &quot;black&quot;, fill = &quot;lightblue&quot;, size = .5, alpha = .3) Figure 4.28: US map with colored state areas and county boundaries 4.9 Arranging Plots 4.9.1 Facet Sometimes, we wish to look that the scatterplot within each factor of categorical variables. For example, we may want to look at the situation within each Region in our case. We can split a single plot into many related plots using the function facet_wrap() or facet_grid(): facet_wrap(~variable) will return a symmetrical matrix of plots for the number of levels of variable; facet_grid(. ~variable) will return facets equal to the levels of variable distributed horizontally. facet_grid(variable~.) will return facets equal to the levels of variable distributed vertically. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) p &lt;- ggplot(df, aes(log(Infected + 1), log(Death + 1))) + geom_point(na.rm = TRUE) + aes(color = Region) p Figure 4.29: Facetting examples p + facet_grid(.~Region) Figure 4.30: Facetting examples p + facet_grid(Region~.) Figure 4.31: Facetting examples p + facet_wrap(~Region) Figure 4.32: Facetting examples 4.9.2 Combining plots using patchwork package Before plots can be laid out, they have to be assembled. The goal of patchwork is to make it simple to combine separate ggplots into the same graphic. We can install patchwork from CRAN using if (!require(&#39;patchwork&#39;)) install.packages(&#39;patchwork&#39;) library(patchwork) Let us consider some simple examples. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == as.Date(&#39;2020-12-11&#39;)) p1 &lt;- ggplot(df, aes(log(Infected+1), log(Death + 1))) + geom_point(na.rm = TRUE) p2 &lt;- ggplot(df, aes(log(Death + 1))) + geom_histogram(binwidth = 1) + aes(fill = Region) p3 &lt;- ggplot(df, aes(log(Infected + 1))) + geom_histogram(binwidth = 1) + aes(fill = Region) # Horizontal arrangement p1 + p2 Figure 4.33: Patchwork examples # Vertical arrangement p1 / p2 Figure 4.34: Patchwork examples # Grouped arrangements p1 | (p2 / p3) Figure 4.35: Patchwork examples # Combine three plots p1 + p2 + p3 Figure 4.36: Patchwork examples # Set the number of plots per row p1 + p2 + p3 + plot_layout(ncol = 2) Figure 4.37: Patchwork examples # Combine the duplicate legends p1 + p2 + p3 + plot_layout(ncol = 2, guides = &quot;collect&quot;) Figure 4.38: Patchwork examples # Add title and subtitles p123 &lt;- (p1 | (p2 / p3))+ plot_annotation( title = &quot;Add title here&quot;, caption = &quot;Add caption here&quot; ) p123 Figure 4.39: Patchwork examples 4.10 Output After polishing the figure, we need to save the figure and output it as a readable file for later use. We can either output it in a standard figure format, such as png, tiff, jpeg; or we can save it as an R readable data file, usually referred to as XXX.rds, XXX.rda or XXX.RData, and read by readRDS('XXX.rds'). 4.10.1 Save in figure format # Take a look at the figure before saving # print(p) ggsave(&#39;example_ggplot2.png&#39;, p) # Save the figure in png format ## Saving 7 x 5 in image 4.10.2 Save in RDS format # Save the figure in .rda format saveRDS(p, &#39;example_ggplot2.rds&#39;) # Read the figure in .rda format q &lt;- readRDS(&#39;example_ggplot2.rds&#39;) # print(q) 4.11 Exercises Scatter plot using slid::state.long on 2020-11-01. Create a scatter plot. Treat Infected/1000 as x-axis, and Death/1000 as y-axis. Color the points according to Division. Hint: use aes(color = ). Change the size of the points to be proportional to population. Hint: use aes(size = ). Change the label of x-axis to ‘Infected (in thousands),’ the label of y-axis to ‘Death(in thousands).’ Change the title of the figure as ‘Infected against death on 2020-11-01.’ Save the plot to file ‘q1.png.’ Time series plot using slid::state.ts for Florida. Obtain the daily new death count for Florida. Create a line plot, time as x-axis, daily new death as y-axis. Add the title “Daily new death count for Florida” to the plot. Using the data up till 2020-11-27, a model obtained the following prediction and 80% prediction intervals for the period from 2020-11-28 to 2020-12-11. DATE Y.Death PI 1 2020-11-28 72 [33, 111] 2 2020-11-29 56 [17, 96] 3 2020-11-30 74 [34, 114] 4 2020-12-01 88 [48, 128] 5 2020-12-02 91 [50, 131] 6 2020-12-03 59 [18, 101] 7 2020-12-04 104 [62, 146] 8 2020-12-05 79 [31, 128] 9 2020-12-06 64 [14, 113] 10 2020-12-07 81 [31, 132] 11 2020-12-08 95 [43, 148] 12 2020-12-09 98 [44, 152] 13 2020-12-10 67 [12, 122] 14 2020-12-11 111 [54, 168] Add another line on your time series plot indicating the predicted daily new death data. Change the title to “Two weeks ahead forecast of the daily new death count for Florida” your plot. Add ribbons on your time series plot in part c to illustrate the prediction intervals in part c. Change the title to “Two weeks ahead forecast of the daily new death count for Florida with 80% prediction intervals.” Save the plot to file ‘q2.png.’ For the data slid::state.long and focus on 2020-11-01, do the following: Create a map using Death per 1000 population as the coloring feature. Save the plot to file ‘q3.png.’ Combine the three figures and save the plot to file ‘q4.png.’ Hint: In R, save each plot with different names (e.g. p1, p2, p3), and then use the patchwork package. "],["plotly.html", "Chapter 5 Interactive Visualization 5.1 An Introduction 5.2 Creating Plotly Objects 5.3 Scatterplots and Line Plots 5.4 Pie Charts 5.5 Animation 5.6 Saving HTML 5.7 Exercises", " Chapter 5 Interactive Visualization 5.1 An Introduction As the volume and complexity of infectious disease data increases, public health professionals must synthesize highly disparate data to facilitate communication with the public and inform decisions regarding measures to protect the public’s health. Interactive data visualization allows users the freedom to explore data fully. Here are some key advantages of using interactive data visualization software: Hovering over any data point to see the data behind it; Identifying causes and trends more quickly; Adding multiple highlights and change view subsets of the data by editing options below each graph; Auto-refreshing your visuals to show the most recent data. So far, your primary tool for creating these data visualizations has been “ggplots.” In the past few years, interactive tools for visualization of disease outbreaks has been improving markedly. In this chapter, we will introduce the R plotly package, which allows you to make more professional and interactive graphics, share them on websites, and customize them as you wish. Figure 5.1: A typical data science process. Plotly is an R package for creating interactive, publication-quality graphs. Some of the charts you can do are Basic charts, Statistical charts, Scientific charts, Financial charts, Maps, 3D charts, Subplots, Transforms, Animations. Plotly is built on top of visualization library D3.js, HTML, and CSS. Here are some benefits of using plotly. Plotly is compatible with several languages/ tools: R, Python, MATLAB, Perl, Julia, Arduino. Using plotly, we can easily share interactive plots online with multiple people. Plotly can also be used by people with no technical background for creating interactive plots by uploading the data and using plotly GUI. Plotly is compatible with ggplots in R and Python. Plotly allows embedding interactive plots in websites using iframes or HTML. The syntax for creating interactive plots using plotly is straightforward as well. Suggested references: https://plotly-r.com/overview.html https://plot.ly/r https://plot.ly/r/reference/ Read the book Sievert (2020): Interactive web-based data visualization with R, plotly, and shiny. Read the Cheatsheet from https://images.plot.ly/. Before we begin, please get ready by installing the plotly R package by any of the following methods. Install Plotly You can download the package by using written code below: install.packages(&quot;plotly&quot;) Install from Github Alternatively, you can install the latest development version of plotly from GitHub via the devtools R package: devtools::install_github(&quot;ropensci/plotly&quot;) 5.2 Creating Plotly Objects To create a plotly object, you start with a call to plotly() and pass the data. Next, you decide which graphical representation you want to use: points, lines, bar charts, etc. Then, you customize labels, colors, titles, fonts, etc. Here is a typical code structure: plot_ly(data) %&gt;% add_* (x, y, type, mode, color, size) %&gt;% layout(title, xaxis = list(title, titlefont), yaxis = list(title, titlefont)) In the above code, layout() is used to add/modify part(s) of the graph’s layout. There are a family of add_*() functions, such as add_histogram(), add_trace(), add_lines(), add_pie(), that you can define how to render data into geometric objects. These functions add a graphical layer to a plot. A layer can be considered as a group of graphical elements that can be sufficiently described using only five components: data, aesthetic mappings (e.g., assigning clarity to color), a geometric representation (e.g., rectangles, circles, etc.), statistical transformations (e.g., sum, mean, etc.), and positional adjustments (e.g., dodge, stack, etc.). Here are some arguments that are typically used in the add_*() function: x: values for x-axis; y: values for y-axis; type: to specify the plot that you want to create like “histogram,” “bar,” “scatter,” etc. mode: format in which you want data to be represented in the plot, and possible values are “markers,” “lines, “points”; color: values of same length as x, y and z that represents the color of data points or lines in plot. size: values for same length as x, y and z that represents the size of data points or lines in plot. 5.2.1 Using plot_ly() to create a plotly object Before you try this example, please make sure to install plotly, dplyr and lubridate packages. The lubridate is an R package of choice for working with variables that store dates’ values. library(lubridate) library(dplyr) library(plotly) The county-level dataset is used to create the bar chart below. You can download the county.top10 dataset from the slid R package. This data contains the top 10 counties with the largest number of infected cases on 2020/12/11. library(devtools) install_github(&#39;covid19-dashboard-us/slid&#39;) library(slid) data(county.top10) county.top10 ## ID County State Infection Death ## 176 6037 LosAngeles California 501635 8199 ## 577 17031 Cook Illinois 346004 7282 ## 334 12086 Miami-Dade Florida 253403 3959 ## 75 4013 Maricopa Arizona 245671 4299 ## 2586 48201 Harris Texas 204850 3128 ## 2542 48113 Dallas Texas 156225 1751 ## 1715 32003 Clark Nevada 137100 1962 ## 193 6071 SanBernardino California 120186 1209 ## 297 12011 Broward Florida 118512 1728 ## 2705 48439 Tarrant Texas 116931 1158 Now let’s use the plot_ly() to initialize a plotly object. plot_ly(data = county.top10) %&gt;% add_trace(y = ~Infection, x = ~County, type = &#39;bar&#39;, name = &#39;Infection&#39;) Figure 5.2: Bar chart of the infected count. After running the code, you will see a modebar showing in the top right-hand side of your plotly graph on mouse hover. There are sevel buttons appearing in the modebar. Here are a few things that you can try in the interactive plots: Hovering your mouse over the plot to view associated attributes; Selecting a particular region on the plot using your mouse to zoom; Resetting the axis; Zooming in and zooming out. Next, you can use layout() to modify the layout of a plotly visualization and specify more complex plot arrangements. plot_ly(data = county.top10) %&gt;% add_trace(y = ~Infection, x = ~County, type = &#39;bar&#39;, name = &#39;Infection&#39;) %&gt;% layout(xaxis = list(title = &quot;County&quot;), yaxis = list(title =&quot;Infected Count&quot;), title = &quot;Total Infected Cases on 2020-12-11&quot;) Figure 5.3: Modified bargraph of the infected count. You can also add text labels and annotations to a plotly project in R using add_text(). plot_ly(data = county.top10) %&gt;% add_bars(y = ~Infection, x = ~County, name = &#39;Infection&#39;) %&gt;% add_text( text = ~scales::comma(Infection), y = ~Infection, x = ~County, textposition = &quot;top middle&quot;, showlegend = FALSE, cliponaxis = FALSE ) %&gt;% add_bars(y = ~Death, x = ~County, name = &#39;Death&#39;, color = I(&quot;red&quot;)) %&gt;% add_text( text = ~Death, y = ~Death, x = ~County, textposition = &quot;top middle&quot;, showlegend = FALSE, cliponaxis = FALSE ) %&gt;% layout(xaxis = list(title = &quot;County&quot;), yaxis = list(title = &quot;Number of Cases&quot;), title = &quot;Total Infected/Death Cases on 2020-12-11&quot;) Figure 5.4: Bargraph of the infected count and death count. 5.2.2 Use dplyr verbs to modify data To visualize the states that the counties with the most infected cases locate in, we can use the dplyr verbs to modify data and calculate counts and use add_bars to add a new bar chart. county.top10 %&gt;% group_by(State) %&gt;% summarise(n = n()) %&gt;% plot_ly() %&gt;% add_bars(x = ~State, y = ~n) Figure 5.5: Bargraph of the infected count by adding bars. Next, suppose we are interested in the distribution of the logarithm of the daily new infected cases from 2020-11-12 to 2020-12-11 from all the states in the US. We can use the state.long data in the slid R package, and plot the histogram of log(daily new infected cases) using add_histogram. # Prepare the daily new Infected for each state in the period # from 2020-11-12 to 2020-12-11 slid::state.long %&gt;% dplyr::filter(DATE &lt;= &#39;2020-12-11&#39; &amp; DATE &gt; &#39;2020-11-11&#39;) %&gt;% group_by(State) %&gt;% # Group by State # Create daily new from cum. Infected count mutate(Y.Infected = c(Infected[-length(Infected)] - Infected[-1], 0)) %&gt;% plot_ly() %&gt;% add_histogram(x = ~log(Y.Infected+1)) Figure 5.6: Histogram of the log(daily new infected cases). 5.2.3 Using ggplotly() to create a plotly object The ggplotly() function from the plotly package has the ability to translate ggplot2 to plotly. This functionality can be really helpful for quickly adding interactivity to your existing ggplot2 workflow. We consider the state.long dataset, which includes the variables, cumulative infected cases (Infected). Chapter 4 shows how to draw a simple scatterplot using the reported data on December 11, 2020. Figure 5.7 shows a translated scatterplot from ggplot2 to plotly. df &lt;- slid::state.long %&gt;% dplyr::filter(DATE == &#39;2020-12-11&#39;) p &lt;- ggplot(df, aes(log(Infected), log(Death))) + geom_point() + geom_point(aes(color = Region)) # Translate ggplot2 to plotly ggplotly(p) Figure 5.7: A translated scatterplot from ggplot2 to to plotly. 5.3 Scatterplots and Line Plots The plot_ly() function initiates an object where one or multiple traces can be added to it via functions add_trace() or add_*(). In add_trace(), the layer’s type can be specified using the type argument. For example, some most commonly used types include 'scatter', 'bar', 'box', 'histogram', 'heatmap', etc. Some add_*() functions are specific cases of a trace type. If the type is not specified when adding a layer, a sensible default will be set. We focus on type = 'scatter', which works well in displaying lines and points, such as the time series of infected cases or the number of people vaccinated during the pandemic. 5.3.1 Make a scatterplot We use the state.long data to draw a basic scatterplot with log(Death) vs log(Infected). library(slid) data(state.long) plot_ly(data = state.long %&gt;% filter(DATE == as.Date(&#39;2020-12-11&#39;))) %&gt;% add_trace(x = ~log(Infected), y = ~log(Death), text = ~State, type = &#39;scatter&#39;, mode = &#39;markers&#39;) 5.3.2 Markers We now describe how to change the point colors, and shapes of markers generated using plotly. color: values mapped to relevant fill-color’ attribute(s); I(): avoid mapping a data value to colors and specify the color manually (e.g., color = I(\"red\")). variable: numeric: generate one trace with a filled color determined by the variable value and a color bar as a guide; factor: generate multiple traces with different colors, one for each factor level; symbol: can be specified similarly as color by factor value; I() to set a fixed color. size: for scatterplots, unless otherwise specified via the sizemode, the size argument controls the area of markers and must be a numeric variable. The size argument controls the minimum and maximum size of circles in pixels. Below, we customize the scatterplot and change the size and color of the markers. data(state.long) plot_ly(data = state.long %&gt;% filter(DATE == as.Date(&#39;2020-12-11&#39;))) %&gt;% add_trace(x = ~log(Infected), y = ~log(Death), text = ~State, type = &#39;scatter&#39;, mode = &#39;markers&#39;, # change the size and color of the markers size = ~pop, color = ~Region, marker = list(opacity = 0.5, symbol = &#39;circle&#39;, sizemode = &#39;diameter&#39;)) 5.3.3 A single time series plot We draw a time series of the cumulative infected count for Cook county, IL. # Load data library(slid) data(county.top10.long) # Start plotly from here plot_ly() %&gt;% # add Cook County’s time series using mode: lines+markers add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39; &amp; County == &#39;Cook&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, showlegend = TRUE, name = &#39;mode:lines+markers&#39;, text = &#39;Cook, Illinois&#39;) Figure 5.8: Time series plot of the cumulative infected count for Cook County, IL. 5.3.4 Hover text and template You can add summary statistics or additional information to your plot in the form of tooltips that appear when viewers hover their mouse over areas of your project. There are two main approaches to controlling the tooltip: hoverinfo and hovertemplate. The default value of hoverinfo is x+y+text+name, meaning that plotly.js will use the relevant values of x, y, text, and name to populate the tooltip text. # Start plotly from here plot_ly() %&gt;% # add Cook County’s time series using mode: lines+markers add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39; &amp; County == &#39;Cook&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, showlegend = TRUE, name = &#39;mode:lines+markers&#39;, text = &#39;Cook, Illinois&#39;, hoverinfo = &quot;x+y+text&quot;) To customize the tooltip on your plot, you can use hovertemplate, a template string used to render the information that appears on the hover box. See Chapter 25 of Sievert (2020) for more details on how to design and control the tooltips. # Prepare hover text and formatting label.template &lt;- paste(&#39;County, State: %{text}&lt;br&gt;&#39;, &#39;Date: %{x}&lt;br&gt;&#39;, &#39;Infected Cases: %{y}&#39;) # Start plotly from here plot_ly() %&gt;% # add Cook County’s time series using mode: lines+markers add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39; &amp; County == &#39;Cook&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, showlegend = TRUE, name = &#39;mode:lines+markers&#39;, text = &#39;Cook, Illinois&#39;, hovertemplate = label.template) 5.3.5 Multiple time series plots Using different options in the mode argument Figure 5.9 shows different types of time series plots for the cumulative infected count for three counties by changing the mode argument. # Start plotly from here plot_ly() %&gt;% # add Cook County’s time series using mode: lines+markers add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39; &amp; County == &#39;Cook&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, showlegend = TRUE, name = &#39;mode:lines+markers&#39;, text = &#39;Cook, Illinois&#39;, hovertemplate = label.template) %&gt;% # add LosAngeles county’s time series using mode: lines add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39; &amp; County == &#39;LosAngeles&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines&#39;, showlegend = TRUE, name = &#39;mode:lines&#39;, text = &#39;Los Angeles, California&#39;, hovertemplate = label.template) %&gt;% # add Miami-Dada county’s time series using mode: markers add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39; &amp; County == &#39;Miami-Dade&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;markers&#39;, showlegend = TRUE, name = &#39;mode:markers&#39;, text = &#39;Miami-Dade, Florida&#39;, hovertemplate = label.template) Figure 5.9: Time series plot of the cumulative infected count for three counties. Mapping the value of a variable to color plot_ly() %&gt;% add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, color = ~County, showlegend = TRUE) Controlling the color scale We can use the colors argument to control the color scale: “colorbrewer2.org” palette name (e.g., “YlOrRd” or “Blues”); a vector of colors to interpolate in hexadecimal “#RRGGBB” format; a color interpolation function like colorRamp(). For example, you can define your own color palette: mycol &lt;- c(&quot;#5B1A18&quot;, &quot;#F21A00&quot;, &quot;#D67236&quot;, &quot;#F1BB7B&quot;, &quot;#D8B70A&quot;, &quot;#A2A475&quot;, &quot;#81A88D&quot;, &quot;#78B7C5&quot;, &quot;#3B9AB2&quot;, &quot;#7294D4&quot;, &quot;#C6CDF7&quot;, &quot;#E6A0C4&quot;) plot_ly() %&gt;% add_trace(data = county.top10.long %&gt;% filter(wday(Date) == 1 &amp; type == &#39;Observed&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, color = ~factor(County), colors = mycol, showlegend = TRUE) 5.3.6 More features about the lines We can also alter the thickness of the lines in your time series plot, and make them dashed or dotted using default types or self-defined method. In the following code, we change the line type by the value of variable type by linetype = ~type. plot_ly() %&gt;% add_trace(data = county.top10.long %&gt;% filter(County == &#39;Cook&#39;), x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines&#39;, linetype = ~type, showlegend = TRUE, text = &#39;Cook, Illinois&#39;, hovertemplate = label.template) 5.3.7 Add ribbons You can use the add_ribbons() function to draw a filled area plot, for example, the confidence band or prediction intervals. Its main arguments are: * data: the data * x: x values * ymin: the lower bound of the ribbon * ymax: the upper bound of the ribbon The following code adds the 80% prediction intervals for the cumulative infected cases for Cook County, Illinois. plot_ly(data = county.top10.long %&gt;% filter(County == &#39;Cook&#39;)) %&gt;% add_trace(x = ~Date, y = ~Count, type = &#39;scatter&#39;, mode = &#39;lines&#39;, linetype = ~type, showlegend = TRUE, text = &#39;Cook, Illinois&#39;, hovertemplate = label.template) %&gt;% add_ribbons(x = ~Date, ymin = ~Count_lb, ymax = ~Count_ub, color = I(&quot;#74A089&quot;), opacity = 0.75, name = &quot;80% prediction intervals&quot;) 5.4 Pie Charts We then demonstrate how to make static and interactive pie charts in R. To draw the pie chart, we download the features.state from the slid R package, and the dataset contains four variables: State, Region, Division and pop. We are interested in the composition of the population in each region. 5.4.1 Draw static pie charts using ggplot2 In the following, we will try ggplot2 to draw the pie chart. Before you start to draw the plot, you will need to prepare the data first. # Prepare the data features.region &lt;- features.state %&gt;% group_by(Region) %&gt;% summarize(tpop = sum(pop)) df &lt;- features.region %&gt;% arrange(desc(Region)) %&gt;% mutate(prop = round(tpop / sum(features.region$tpop), 4) *100) %&gt;% mutate(lab.pos = cumsum(prop)- 0.5*prop ) Next, we will apply the geom_bar and coord_polar functions together with ggplot to display the pie chart; see Figure 5.10. Figure 5.10: A simple ggplot pie chart for population in different regions. 5.4.2 Draw interactive pie charts Now, we will try to use the plotly to make the pie chart, and you will see that the function add_pie() can be implemented easily without data preparition. Figure 5.11: An interactive pie chart for population in different regions. Next, we are interested in finding the composition of the cumulative infected/death cases in each region using add_pie. We can create pie chart subplots by using the domain attribute. It is important to note that the x array sets the horizontal position while the y array sets the vertical. For example, x=[0,0.5], y=[0, 0.5] mean the bottom left position of the plot. Figure 5.12: Pie charts with subplots: left plot is for infected count, and right plot is for the death count. 5.5 Animation Animated plots are a great way to display the dynamics of the underlying data. Both plot_ly() and ggplotly() support keyframe animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id. This section provides a walk-through for creating an animated plots using the plotly R package. 5.5.1 An animation of the evolution of infected vs. death count Figure 5.13 creates an animation of the evolution in the relationship between the state-level logarithm of cumulative infected count and the logarithm of cumulative death count evolved over time. For simple illustration purpose, we only show the evoluation in December of 2020. The data state.long from slid package provides a daily time series for 48 mainland states and the District of Columbia in the US. Below, we first prepare the data: #install_github(&#39;covid19-dashboard-us/slid&#39;) library(slid) data(state.long) state.long.DEC &lt;- state.long %&gt;% dplyr::filter(DATE &gt; as.Date(&quot;2020-11-30&quot;)) %&gt;% mutate(log.Infected = log(Infected + 1)) %&gt;% mutate(log.Death = log(Death + 1)) Next, we load the required packages. Animations can be created by either using the frame argument in plot_ly() or the frame ggplot2 aesthetic in ggplotly(). Animated plots can be generated with the frame = and ids = arguments in the geom_point() function when using ggplot and ggplotly(). In this case, the data state.long is recorded on a daily basis, so we will assign the DATE variable to frame; each point in the scatterplot represents a state, so we will assign the State variable to ids, which ensures a smooth transition from date to date for the lower 48 states and the District of Columbia in the US: gg &lt;- ggplot(state.long.DEC, aes(log.Infected, log.Death, color = Region)) + geom_point(aes(size = pop, frame = as.numeric(DATE), ids = State)) anim1 &lt;- ggplotly(gg) anim1 Figure 5.13: Your first animated plot between logarithms of the death count and infected count. As long as frame = is provided, an animation is produced with play/pause button(s) and a slider component for controlling the animation. By default, animations populate a play button and slider component for controlling the state of the animation. You can pause an animation by clicking on a relevant location on the slider bar. These components can be removed or customized via the animation_button() and animation_slider() functions. You can control the play button and slider component transition between frames according to rules specified by animation_opts(). Moreover, various animation options, like the amount of time between frames, the smooth transition duration, and the type of transition easing may be altered via the animation_opts() function, too. Here are some animation configuration options in the function animation_opts(): p: a plotly object; frame: the amount of time between frames (this amount should include the transition); transition frame: the duration of the smooth transition between frames; easing: the type of transition easing; redraw = TRUE: trigger a redraw of the plot at the completion of the transition or not; mode: describe how a new animate call interacts with currently-running animations. The detailed options in the above arguments can be found from here. Figure 5.14 illustrates a similar plot as in Figure 5.13 with a slightly different aesthetic style, and it also doubles the amount of time between frames, uses elastic transition easing, places the animation buttons closer to the slider. base &lt;- state.long.DEC %&gt;% plot_ly(x = ~log.Infected, y = ~log.Death, size = ~pop, text = ~State, hoverinfo = &quot;x+y+text&quot;) %&gt;% layout(xaxis = list(type = &quot;log&quot;)) anim2 &lt;- base %&gt;% add_markers(color = ~Region, frame = ~DATE, alpha = 0.8, span = I(2), ids = ~State, colors = &quot;Set1&quot;) %&gt;% animation_opts(frame = 1000, easing = &quot;elastic&quot;, redraw = FALSE) %&gt;% animation_button( x = 1, xanchor = &quot;right&quot;, y = 0, yanchor = &quot;bottom&quot; ) %&gt;% animation_slider( currentvalue = list(type = &quot;date&quot;, font = list(color=&quot;red&quot;)) ) anim2 Figure 5.14: Modified animation with frame = 1000 and elastic easing. The speed at which the animation progresses is controlled by the frame argument, with the default value being 500 milliseconds. In this example, we increase it to 1000 milliseconds, resulting in slower transitions between frames. We can change the way of transition from frame to frame via the easing argument, here easing = \"elastic\" causes the points to bounce when a new frame occurs. See the options of easing from here. The redraw = FALSE argument can improve laggy animations’ performance by not entirely redrawing the plot at each transition. However, in this example, it doesn’t make much difference. 5.5.2 An animation of the state-level time series plot of infected count We now would like to show the state-level time series plot of the infected count. We then show the animation by Region. Since there is no meaningful relationship between objects in different frames of Figure 5.15, the smooth transition duration is set to 0. This helps avoid any confusion that there is a meaningful connection between the smooth transitions. Note that these options control both animations triggered by the play button or via the slider. mycol &lt;- c(&quot;#5B1A18&quot;, &quot;#F21A00&quot;, &quot;#D67236&quot;, &quot;#F1BB7B&quot;, &quot;#D8B70A&quot;, &quot;#A2A475&quot;, &quot;#81A88D&quot;, &quot;#78B7C5&quot;, &quot;#3B9AB2&quot;, &quot;#7294D4&quot;, &quot;#C6CDF7&quot;, &quot;#E6A0C4&quot;) base &lt;- state.long %&gt;% mutate(log.Infected = log(Infected + 1)) %&gt;% plot_ly(x = ~DATE, y = ~log.Infected, frame = ~Region, text = ~State, hoverinfo = &quot;text&quot;) %&gt;% add_lines(color = ~factor(State), colors = mycol, showlegend = FALSE) anim3 &lt;- base %&gt;% layout(xaxis = list(type = &quot;date&quot;, range=c(&#39;2020-01-22&#39;, &#39;2020-12-11&#39;))) %&gt;% animation_opts(1000, easing = &quot;elastic&quot;, redraw = FALSE, transition = 0) anim3 Figure 5.15: Animation of time series plot of infected count by region. 5.6 Saving HTML After polishing the figure, we need to save the figure and animation for later use. We can save any widget made from any htmlwidgets package (e.g., plotly, leaflet, etc) as a standalone HTML file via the saveWidget() function. By default, it produces a completely self-contained HTML file, and all the necessary JavaScript and CSS dependency files are bundled inside the HTML file. 5.6.1 Save as a standalone HTML file # Save plotly object into a standalone html file library(htmlwidgets) saveWidget(fig1, &quot;pie1.html&quot;, selfcontained = T) saveWidget(anim1, &quot;anim1.html&quot;, selfcontained = T) 5.6.2 Save as non-selfcontained HTML file Sometimes, you may want to embed numerous widgets in a larger HTML document and save all the dependency files externally into a single directory. You can do this by setting selfcontained = FALSE and specifying a fixed libdir in saveWidget(). # Save plotly object into a non-selfcontained html file library(htmlwidgets) saveWidget(fig2, &quot;pie2.html&quot;, selfcontained = F, libdir = &quot;lib&quot;) saveWidget(anim2, &quot;anim2.html&quot;, selfcontained = F, libdir = &quot;lib&quot;) 5.7 Exercises We will explore the basic functions of plot_ly using state.long. Install the Github R package slid. library(slid) data(state.long) Create a bar graph for the top ten states with the largest new number of infected cases on December 11, 2020. Create a time series plot for the logarithm of the cumulative infected cases for Iowa. Create a time series plot for the logarithm of the cumulative infected cases for the top ten states with the largest new number of infected cases on December 11, 2020. Create a pie chart for the daily new infected cases on December 11, 2020, for different regions. Save the above plots as an HTML file, and save all the dependency files externally into a single directory using htmlwidgets::saveWidget() with selfcontained = FALSE. Redraw the above plots in (a), (b) and (c) for the death count using the ggplotly() function. Save the above plots each as an HTML file, and save all the dependency files externally into a single directory using htmlwidgets::saveWidget() with selfcontained = FALSE. During the COVID-19 pandemic, we are interested in how many tests are coming back positive. The state.long data set comprises state-level cumulative tests. We want to create an animation to demonstrate the weekly test positivity rate for each state based on a 7-day moving average. It is calculated by dividing the state’s new positive counts in the past seven days by the state’s new tests in the past seven days. The PosTest.state.rda and Test.state.rda are the daily reported positive test and daily total test data collected from COVIDTracking Project, and it can be downloaded from the Github slid R package. Load the the datasets to your working directory: library(slid) data(Test.state) data(PosTest.state) change them from the wide form to the long form, and combine them into one dataset. Add a new column of the weekly test positivity rate. Create an animated time series plot for Iowa’s weekly test positivity rate in the past month (30 days) starting from December 1 to December 11, 2020. For example, if I pause on December 7, it should show a time series of Iowa’s weekly test positivity rate from November 8 to December 7. Save your animation in part c as a standalone HTML file and a non-selfcontained HTML file. "],["shiny.html", "Chapter 6 R Shiny 6.1 An Introduction to Shiny 6.2 Useful Input Widgets 6.3 Displaying Reactive Output 6.4 Rendering Plotly Inside Shiny", " Chapter 6 R Shiny In infectious disease data learning, users can understand and use complex data thanks to interactive visualization. Shiny allows you to create a graphical user interface (GUI) that can be used locally or remotely. It has a lot of potential for making interactive, web-based visualizations much more accessible to users. For example, it provides multiple views or panels, allowing users to examine their data from various angles. Shiny is also useful for displaying and communicating updated findings to a large audience. In this chapter, we will look at how to connect Plotly graphs to shiny, an open-source R package that provides an elegant and powerful web framework for creating R-based web applications. Shiny allows you to turn your data into interactive web applications without having to know HTML, CSS, or JavaScript. Installation Shiny is available on CRAN, so you can install it in the usual way from your R console: install.packages(&quot;shiny&quot;) 6.1 An Introduction to Shiny A shiny app has two main parts: The user interface, or ui, determines how input and output widgets appear on a page. The UI can be customized, and packages like “shinydashboard” make it simple to use more advanced layout frameworks. The server function, server, establishes a link between input and output widgets. More specifically, the shiny server is a R function that connects client input values to webserver outputs. Figure 6.1: An illustration of Shiny Structure. ui.R library(shiny) # Define UI for miles per gallon application shinyUI(pageWithSidebar( # Application title headerPanel(&quot;Hello Shiny!&quot;), sidebarPanel(), mainPanel() )) server.R library(shiny) # Define server logic required to plot variables against mpg shinyServer(function(input, output) { }) Finally, to execute the shiny app, there are two ways. You can run runApp('appname') in an R file, usually named as apps.R, that shares the same directory with a folder, which is under the name 'appname', and contains UI.R and server.R mentioned above. You can define the server function as server and UI function as ui in the same R file, and run shinyApp(ui, server) as follows. library(shiny) # Define UI ui &lt;- shinyUI(pageWithSidebar( # ... Program here )) # Define server logic server &lt;- shinyServer(function(input, output) { # ... Program here }) shinyApp(ui = ui , server = server) In practice, we prefer the first method over the second to manage multiple shiny apps. 6.1.1 Your first shiny app A Shiny application is simply a directory that contains a user-interface definition, a server script, and any additional data, scripts, or other resources that are needed to support the application. To begin developing the app, create a new empty directory wherever you want, then create empty ui.R and server.R files within it. The user interface is defined in the ui.R source file: library(shiny) # Define UI for application that plots random distributions shinyUI(pageWithSidebar( # Application title headerPanel(&quot;Hello Shiny!&quot;), # Sidebar with a slider input for number of observations sidebarPanel( sliderInput(&quot;obs&quot;, &quot;Number of observations:&quot;, min = 1, max = 1000, value = 500) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) )) The application’s server-side is depicted below. It’s very straightforward: a random distribution with the desired number of observations is generated, and the histogram is plotted. You may also notice that the function that returns the plot is wrapped in a renderPlot call. See the source file named server.R below. library(shiny) # Define server logic required to generate and plot a random # distribution shinyServer(function(input, output) { # Expression that generates a plot of the distribution. # The expression is wrapped in a call to renderPlot # to indicate that: # # 1) It is &quot;reactive&quot; and therefore should be automatically # re-executed when inputs change # 2) Its output type is a plot # output$distPlot &lt;- renderPlot({ # generate an rnorm distribution and plot it dist &lt;- rnorm(input$obs) hist(dist) }) }) If everything is working properly, the application should look something like this in your browser: The Hello Shiny example. 6.1.2 Create a new shiny app in RStudio In RStudio, you can create a new directory and an app.R file containing a basic app in one step by clicking File -&gt; New File -&gt; Shiny Web App, then providing “Application Name” and select “Application Type.” The RStudio New Shiny Web App Window. 6.1.3 Share your app RStudio offers three ways to host your Shiny app as a web page: Create a free or professional account at http://shinyapps.io, a cloud-based service from RStudio, to host your shiny apps. Click the Publish icon in the RStudio IDE (&gt;=0.99) or run: rsconnect::deployApp(&quot;&lt;path to directory&gt;&quot;) Shiny Server is a companion program to Shiny that builds a web server designed to host Shiny apps. It’s free, open-source, and available from GitHub. 6.2 Useful Input Widgets Shiny also includes a number of other useful input widgets, or web elements, with which your users can interact. Your users can use widgets to send messages to the Shiny app. Shiny widgets ask your user for a value. When a user modifies a widget, the value modifies as well. Input widgets can be easily stylized with CSS and/or SASS, and even custom input widgets can be integrated, despite the fact that many shiny apps use them “out of the box.” From https://shiny.rstudio.com/, we can explore and select the appropriate input widgets for your interactive visualization. The Basic widgets. selectInput() or selectizeInput() for dropdown menus. numericInput() for a single number. sliderInput() for a numeric range. textInput() for a character string. dateInput() for a single date. dateRangeInput() for a range of dates. fileInput() for uploading files. checkboxInput(), or checkboxGroupInput() or radioButtons() for choosing a list of options. In the future, we will concentrate on using plotly and static R graphics as inputs to other output widgets, rather than using these input widgets to link multiple graphs in shiny through direct manipulation. 6.3 Displaying Reactive Output You can create reactive output with a two step process. Add an R object to your user interface. Tell Shiny how to build the object in the server function. The object will be reactive if the code that builds it calls a widget value. Step 1: Add an R object to the UI Shiny provides a family of functions that turn R objects into output for your user interface. Each function creates a specific type of output. Output function Creates dataTableOutput: DataTable htmlOutput: raw HTML imageOutput: image plotOutput: plot tableOutput: table textOutput: text uiOutput: raw HTML verbatimTextOutput: text You can add output to the user interface in the same way that you added HTML elements and widgets. Place the output function inside sidebarPanel or mainPanel in the ui. Step 2: Provide R code to build the object. Placing a function in ui tells Shiny where to display your object. Next, you need to tell Shiny how to build the object. We do this by providing the R code that builds the object in the server function. In the Shiny process, the server function creates a list-like object called output that contains all of the code needed to update the R objects in your app. In the list, each R object must have its own entry. You can make an entry by defining a new output element in the server function, as shown below. The name of the element should be the same as the name of the reactive element you created in the ui. You do not need to explicitly state in the server function’s last line of code that it should return output. R uses reference class semantics to update output automatically. The output of one of Shiny’s render* functions should be included in each entry to output. These functions take a R expression and perform some basic pre-processing on it. The render* function that corresponds to the type of reactive object you’re creating is used. The render function generates the following: renderDataTable: DataTable renderImage: images (saved as a link to a source file) renderPlot: plots renderPrint: any printed output renderTable: data frame, matrix, other table like structures renderText: character strings renderUI: a Shiny tag object or HTML Each render* function takes a single argument: an R expression surrounded by {} and it can be as simple as a single line of text or as complex as a function call with many lines of code. This R expression can be thought of as a set of instructions that you give Shiny to remember. When you first launch your app, Shiny will run the instructions, and then it will run them again every time your object needs to be updated. The Shiny Cheatsheet provides nice summary of the render*() and *Output() functions. The Basic widgets. 6.4 Rendering Plotly Inside Shiny The renderPlotly() function renders anything that the plotly_build() function understands, including plot_ly(), ggplotly(), and ggplot2 objects. It also renders NULL as an empty HTML div, which is handy for certain cases where it doesn’t make sense to render a graph. library(tidyr) library(wesanderson) library(shiny) library(dplyr) library(slid) #library(devtools) #install_github(&#39;https://github.com/covid19-dashboard-us/slid&#39;) Example 1. Top 10 states with the highest daily new infected count ui.R shinyUI(fluidPage( sliderInput(&quot;date.update&quot;, label = h5(&quot;Select date&quot;), min = as.Date(&quot;2020-11-12&quot;), max = as.Date(&quot;2020-12-11&quot;), value = as.Date(&quot;2020-12-11&quot;), timeFormat = &quot;%d %b&quot;, animate = animationOptions(interval = 2000, loop = FALSE) ), # Show a plot of the generated distribution mainPanel( plotlyOutput(&quot;state_daily_bc&quot;, height = &quot;100%&quot;, width = &quot;150%&quot;) ) )) server.R state.daily.bc &lt;- function(date.update){ # load daily new case data for each state dat.sd = slid::dat.sd # select the top 10 states with highest daily new df.sd &lt;- dat.sd[ind.sd[1:10],] df.sd &lt;- df.sd %&gt;% dplyr::select(State, format(date.update, &#39;X%Y.%m.%d&#39;)) %&gt;% mutate(Date &lt;- format(date.update, &#39;%m/%d&#39;)) df.sd$State &lt;- as.character(df.sd$State) names(df.sd) &lt;- c(&#39;State&#39;, &#39;DailyCases&#39;, &#39;Date&#39;) plot.title &lt;- paste0(&quot;New Cases on &quot;, as.character(date.update)) bc.sd &lt;- ggplot(df.sd, aes(State, DailyCases)) + labs(title = plot.title) + xlab(&#39;&#39;) + ylab(&#39;&#39;) + geom_bar(stat = &#39;identity&#39;, fill = &quot;#C93312&quot;) return(bc.sd) } shinyServer(function(input, output) { output$state_daily_bc &lt;- renderPlotly({ ts &lt;- state.daily.bc(input$date.update) }) }) If everything is working correctly you’ll see the application appear in your browser looking something like this: The top ten states with the highest daily new infected count. Example 2. ui.R shinyUI(fluidPage( div(class = &quot;outer&quot;, tags$head(includeCSS(&quot;styles.css&quot;)), plotlyOutput(&quot;us_case_ts&quot;, height = &quot;100%&quot;, width = &quot;100%&quot;), absolutePanel(id = &quot;control&quot;, class = &quot;panel panel-default&quot;, top = 60, left = 70, width = 255, fixed=TRUE, draggable = TRUE, height = &quot;auto&quot;, style = &quot;opacity: 0.8&quot;, selectInput(&quot;plot_type&quot;, label = h5(&quot;Select type&quot;), choices = c(&quot;Original Counts&quot; = &quot;counts&quot;, &quot;Log Counts&quot; = &quot;logcounts&quot;) )# end of selectInput1 ) ) # end of div ) # end of tab ) server.R cols &lt;- c(&quot;#045a8d&quot;, &quot;#cc4c02&quot;) us.case.ts &lt;- function(date.update, plot.type) { # Cum infected cases in Iowa: Observation and Prediction dfplot = slid::dfplot if (plot.type == &#39;counts&#39;){ ts &lt;- ggplot(dfplot, aes(Date, DailyCases, colour = Group)) + ## Plot observed geom_line(colour = &#39;darkgray&#39;) + geom_point() + scale_color_manual(values = c(&quot;Observation&quot; = cols[1], &quot;Prediction&quot; = cols[2])) + ## Change labs labs(title = &#39;Daily new infected cases and prediction&#39;) + xlab(&#39;Date&#39;) + ylab(&#39;Daily new cases&#39;) } else if (plot.type == &#39;logcounts&#39;){ ts &lt;- ggplot(dfplot, aes(Date, logDailyCases, colour = Group) ) + ## Plot observed geom_line(colour = &#39;darkgray&#39;) + geom_point() + scale_color_manual(values = c(&quot;Observation&quot; = cols[1], &quot;Prediction&quot; = cols[2])) + ## Change labs labs(title = &#39;Logarithm of daily new infected count and prediction&#39;) + xlab(&#39;Date&#39;) + ylab(&#39;Log (Daily new cases)&#39;) } return(ts) } shinyServer(function(input, output) { output$us_case_ts &lt;- renderPlotly({ ts &lt;- us.case.ts(date.update = date.update, plot.type = input$plot_type) }) }) If everything is working correctly you’ll see the application appear in your browser looking something like this: The time series plot of the daily new infected cases of Iowa. Remark: In plotly.js, there are four different mouse click+drag behavior modes (i.e., dragmode): zoom, pan, rectangular selection, and lasso selection. Example 3. ui.R shinyUI(fluidPage( div(class=&quot;outer&quot;, tags$head(includeCSS(&quot;styles.css&quot;)), plotlyOutput(&quot;county_risk_ts&quot;, height=&quot;100%&quot;, width=&quot;100%&quot;), absolutePanel(id = &quot;control&quot;, class = &quot;panel panel-default&quot;, top = 60, left = 70, width = 255, fixed=TRUE, draggable = TRUE, height = &quot;auto&quot;, style = &quot;opacity: 0.8&quot;, selectInput(&quot;plot_type&quot;, label = h5(&quot;Select type&quot;), choices = c(&quot;WLR&quot; = &quot;wlr&quot;, &quot;IR&quot; = &quot;localrisk&quot;, &quot;SIR&quot; = &quot;smr&quot;) ) # end of selectInput ) # end of absolutePanel ) # end of div )) ** server.R ** date.update &lt;- as.Date(&#39;2020-12-12&#39;) mycol &lt;- c(&quot;#5B1A18&quot;, &quot;#F21A00&quot;, &quot;#D67236&quot;, &quot;#F1BB7B&quot;, &quot;#D8B70A&quot;, &quot;#A2A475&quot;, &quot;#81A88D&quot;, &quot;#78B7C5&quot;, &quot;#3B9AB2&quot;, &quot;#7294D4&quot;, &quot;#C6CDF7&quot;, &quot;#E6A0C4&quot;) ts.plotly = function(df, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, group = group, mycol, showlegend = TRUE, visible = T, xaxis = xaxis, yaxis = yaxis, legend = legend) { ts &lt;- plot_ly(df) %&gt;% add_trace(x = ~x, y = ~y, type = type, mode = mode, color = ~group, colors = mycol, showlegend = showlegend, visible = visible) %&gt;% layout(xaxis = xaxis, yaxis = yaxis, legend = legend) return(ts) } county.risk.ts = function(date.update, type = &#39;localrisk&#39;){ date.all = date.update - (1:30) date.lag = date.all - 7 County.pop0 &lt;- slid::pop.county County.pop &lt;- County.pop0 %&gt;% filter((!(State %in% c(&quot;Alaska&quot;,&quot;Hawaii&quot;)))) County.pop &lt;- County.pop %&gt;% filter((!(ID %in% c(36005, 36047, 36081, 36085)))) County.pop$ID[County.pop$ID == 46102] = 46113 dat &lt;- slid::I.county dat &lt;- dat %&gt;% filter((!(State %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;)))) var.names &lt;- paste0(&quot;X&quot;, as.character(date.all), sep = &quot;&quot;) var.names &lt;- gsub(&quot;\\\\-&quot;, &quot;\\\\.&quot;, var.names) var.lag &lt;- paste0(&quot;X&quot;, as.character(date.lag), sep = &quot;&quot;) var.lag &lt;- gsub(&quot;\\\\-&quot;, &quot;\\\\.&quot;, var.lag) tmp &lt;- as.matrix((dat[, var.names] - dat[, var.lag])/7) dat &lt;- dat[, c(&quot;ID&quot;, &quot;County&quot;, &quot;State&quot;, var.names)] smr.c &lt;- sum(County.pop$population)/as.matrix(colSums(dat[,-(1:3)])) I0 &lt;- LogI0 &lt;- LocRisk0 &lt;- SMR0 &lt;- dat # I0[,-(1:3)] &lt;- as.matrix(dat[,-(1:3)]) LogI0[,-(1:3)] &lt;- as.matrix(log(dat[,-(1:3)]+1)) LocRisk0[,-(1:3)] &lt;- sweep(as.matrix(dat[,-(1:3)]), 1, County.pop$population[match(dat$ID, County.pop$ID)], &quot;/&quot;) * 1000 SMR0[,-(1:3)] &lt;- sweep(LocRisk0[,-(1:3)],2,smr.c/10,&quot;*&quot;) WLR0 &lt;- sweep(tmp, 1, County.pop$population[match(dat$ID, County.pop$ID)], &quot;/&quot;) * 1e5 county.dat &lt;- data.frame(Date = date.all) CountyState &lt;- paste(as.character(dat$County), as.character(dat$State), sep = &quot;,&quot;) LogI &lt;- cbind(county.dat,round(t(LogI0[,-(1:3)]),2)) names(LogI) &lt;- c(&quot;Date&quot;, CountyState) LocRisk &lt;- cbind(county.dat,round(t(LocRisk0[,-(1:3)]),2)) names(LocRisk) &lt;- c(&quot;Date&quot;, CountyState) SMR &lt;- cbind(county.dat,t(SMR0[,-(1:3)])) names(SMR) &lt;- c(&quot;Date&quot;, CountyState) WLR &lt;- cbind(county.dat, round(t(WLR0), 2)) names(WLR) &lt;- c(&quot;Date&quot;, CountyState) xaxis.fr &lt;- list(title = &quot;&quot;, showline = FALSE, showticklabels = TRUE, showgrid = TRUE, type = &#39;date&#39;, tickformat = &#39;%m/%d&#39;) legend.fr &lt;- list(orientation = &#39;h&#39;, x = 0, y = -0.05, autosize = F, width = 250, height = 200) if (type == &#39;localrisk&#39;){ ind.county = order(LocRisk0[,var.names[1]], decreasing = TRUE) df.fr &lt;- LocRisk %&gt;% select(c(1, 1 + ind.county[1:10])) %&gt;% gather(key = &quot;County.State&quot;, value = &quot;LogI&quot;, -Date) names(df.fr) &lt;- c(&quot;x&quot;,&quot;group&quot;,&quot;y&quot;) yaxis.fr &lt;- list(title = &quot;Local Risk (Cases per Thousand)&quot;) ts.fr &lt;- ts.plotly(df.fr, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, group = group, mycol, showlegend = TRUE, visible = T, xaxis = xaxis.fr, yaxis = yaxis.fr, legend = legend.fr) }else if (type == &#39;smr&#39;){ ind.county = order(SMR0[,var.names[1]], decreasing = TRUE) df.fr &lt;- SMR %&gt;% select(c(1, 1 + ind.county[1:10])) %&gt;% gather(key = &quot;County.State&quot;, value = &quot;LogI&quot;, -Date) names(df.fr) &lt;- c(&quot;x&quot;,&quot;group&quot;,&quot;y&quot;) yaxis.fr &lt;- list(title = &quot;SMR (%)&quot;) ts.fr &lt;- ts.plotly(df.fr, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, group = group, mycol, showlegend = TRUE, visible = T, xaxis = xaxis.fr, yaxis = yaxis.fr, legend = legend.fr) }else if (type == &#39;logcount&#39;){ ind.county = order(dat[,var.names[1]], decreasing = TRUE) df.fr &lt;- LogI %&gt;% select(c(1, 1 + ind.county[1:10])) %&gt;% gather(key = &quot;County.State&quot;, value = &quot;LogI&quot;, -Date) names(df.fr) = c(&quot;x&quot;, &quot;group&quot;, &quot;y&quot;) yaxis.fr &lt;- list(title = &quot;Log Counts&quot;) ts.fr &lt;- ts.plotly(df.fr, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, group = group, mycol, showlegend = TRUE, visible = T, xaxis = xaxis.fr, yaxis = yaxis.fr, legend = legend.fr) }else if (type == &#39;wlr&#39;){ ind.county &lt;- order(WLR0[, var.names[1]], decreasing = TRUE) df.fr &lt;- WLR %&gt;% select(c(1, 1 + ind.county[1:10])) %&gt;% gather(key = &quot;CountyState&quot;, value = &quot;WLR&quot;, -Date) names(df.fr) &lt;- c(&quot;x&quot;, &quot;group&quot;, &quot;y&quot;) yaxis.fr &lt;- list(title = &quot;WLR (New Cases Per 100K)&quot;) ts.fr &lt;- ts.plotly(df.fr, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;, group = group, mycol, showlegend = TRUE, visible = T, xaxis = xaxis.fr, yaxis = yaxis.fr, legend = legend.fr) } return(ts.fr) } shinyServer(function(input, output) { output$county_risk_ts &lt;- renderPlotly({ ts &lt;- county.risk.ts(date.update, type = input$plot_type) }) }) If everything is working correctly you’ll see the application appear in your browser looking something like this: The time series plot of risk or log(count) of the top ten counties (based on the corresponding measurement on 12/11/2011). "],["map.html", "Chapter 7 Interactive Geospatial Visualization 7.1 An Introduction to Leaflet 7.2 The Data Object 7.3 Choropleth map 7.4 Legend 7.5 An Example of County-level Map 7.6 Spot Maps 7.7 Integrating Leaflet with R Shiny 7.8 Exercises", " Chapter 7 Interactive Geospatial Visualization In the last two decades, many spatial and spatiotemporal methods for early outbreak detection, cluster detection, risk area and factor identification, and disease transmission pattern evaluation have been developed, boosting the study of spatial epidemiology. By definition, the focus of spatial epidemiology is the study of the geographical or spatial distribution of health outcomes. It is sometimes interchangeably known as disease mapping. Usually, it has the incidence of disease or prevalence of disease as its main focus. It is a commonplace to consider a geographic dimension included within a research design in infectious disease studies. This may involve initial visualization of the distribution and some simple summary measures. The growing development of the open-source community has aided the application of spatial epidemiology methods, with R, a programming language and free software environment for statistical computing and graphics, being the most widely used and popular. For the display of COVID-19 data, interactive geospatial visualization is used by websites ranging from the New York Times and the Washington Post to GitHub and Flickr. In this chapter, we focus on the interactive geospatial visualization of the data. 7.1 An Introduction to Leaflet One of the most popular open-source JavaScript libraries for interactive maps is “leaflet.” GIS specialists such as OpenStreetMap, Mapbox, and CartoDB use it frequently. This R package makes integrating and controlling map objects in R simple. 7.1.1 More References Recommended Reading: Leaflet for R Making maps with R Data Visualization Series IV: Create Interactive Map in R Use the Leaflet Cheat Sheet for inspiration. More Examples of Shiny with Leaflet: Using Leaflet with Shiny COVID-19 US Dashboard 7.1.2 Features and installation Unlike static visualization packages like “ggplot2” or “ggmap,” “leaflet” maps are fully interactive and can include features like interactive panning and zooming pop-up tooltips and labels, and highlighting and selecting regions. Features Use the JavaScript library “Leaflet” and the “htmlwidgets” package to create and customize interactive maps. Compose maps using arbitrary combinations of: Map tiles Markers Polygons Lines Popups GeoJSON The maps created can be used directly from the R console, “RStudio,” Shiny apps, and R Markdown documents. Render spatial objects from the sp and sf packages, as well as data frames with latitude and longitude columns, with ease. Use map bounds and mouse events to drive “Shiny” logic. Enhance map features using plugins from the “leaflet” plugins repository Installation To install this R package, run this command at your R prompt: install.packages(&quot;leaflet&quot;) # to install the development version from Github, run devtools::install_github(&quot;rstudio/leaflet&quot;) Once installed, you can use this package at the R console, within R Markdown documents, and Shiny applications. 7.1.3 Basic Usage Similar to “ggplot2,” leaflet maps are built using layers. We can create a Leaflet map with these basic steps: Step 1. Create a base map widget by calling leaflet(); Step 2. Customize the map widget by using layer functions (addTiles(), addMarkers(), etc.) to add features to the map Step 3. Print the map widget to display it and save it. Here is a base example. Suppose we would like to show the Eiffel Tower on a street map, we can run the following: library(leaflet) m &lt;- leaflet() %&gt;% addTiles() %&gt;% # Add default OpenStreetMap map tiles addMarkers(lng = 2.2945, lat = 48.8584, popup = &quot;The Eiffel Tower&quot;) m # Print the map 7.2 The Data Object The data parameter in both the leaflet() and map layer functions is designed to receive spatial data in one of several formats from: the base R lng-lat matrix data frame with lng-lat matrix the sp package: SpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects; SpatialLines, SpatialLinesDataFrame, Lines, and Line objects; the map package the data frame returned from map() The data argument is used to pass spatial data to functions that need it; for example, if data is a SpatialPolygonsDataFrame object, then calling addPolygons on that map widget will add the polygons from the SpatialPolygonsDataFrame. Specifying latitude/longitude in the base R By providing lng and lat arguments to the layer function, we can always explicitly identify latitude/longitude columns. Alternatively, we can provide a data frame that consists of the latitude/longitude information. For example, in the addCircles() below, we directly pass a data frame, df, with variables Lat and Long in the data frame: # add some circles to a map df &lt;- data.frame(Lat = rexp(10) + 40, Long = rnorm(10) - 92) leaflet(df) %&gt;% addTiles() %&gt;% addCircles(data = df, lat = ~Lat, lng = ~Long) The sp Package The first general R package to provide classes and methods for spatial data is called sp, which provides classes and methods to create points, lines, polygons, and grids and to operate on them. For example, we can generate the polygons objects using the function Polygon(), and we can also generate SpatialPolygons objects using lists of Polygon. library(sp) library(rgeos) x1 &lt;- c(3, 3, 6, 12, 3) x2 &lt;- c(6, 3, 2, 6, 6) y1 &lt;- c(6, 3, 2, 6) y2 &lt;- c(2, 3, 2, 2) Poly1 &lt;- Polygon(cbind(x1, x2)) Poly2 &lt;- Polygon(cbind(y1, y2)) Polys1 &lt;- Polygons(list(Poly1), &quot;s1&quot;) Polys2 &lt;- Polygons(list(Poly2), &quot;s2&quot;) SPolys &lt;- SpatialPolygons(list(Polys1, Polys2), 1:2) To draw this in leaflet, we can use addPolygons(): leaflet(height = &quot;300px&quot;) %&gt;% addPolygons(data = SPolys) The maps Package The R maps package contains many outlines of continents, countries, states, and counties. For example, World: world, world.cities, lakes USA: states, county, state, usa and you can check help(package='maps') for a whole list. We can specify map(fill = TRUE) for polygons, FALSE for polylines. The code below shows how to obtain and plot the geospatial object of the states in the US. library(maps) mapStates &lt;- map(&quot;state&quot;, fill = TRUE, plot = FALSE) leaflet(data = mapStates) %&gt;% addTiles() %&gt;% addPolygons(fillColor = topo.colors(10), opacity = 0.5, stroke = FALSE) 7.3 Choropleth map A choropleth map is a map that colors or patterns a set of pre-defined areas in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population, different numbers, or disease rates. Let us start by loading data from JSON into sp objects using the geojsonio package to create a choropleth map. This will make manipulating geographic features and their properties in R much easier. library(geojsonio) urlRemote &lt;- &quot;https://raw.githubusercontent.com/&quot; pathGithub &lt;- &quot;PublicaMundi/MappingAPI/master/data/geojson/&quot; fileName &lt;- &quot;us-states.json&quot; states0 &lt;- geojson_read(x = paste0(urlRemote, pathGithub, fileName), what = &quot;sp&quot;) class(states0) In Chapter 3, we have seen an example of merging the states0 data with state.long in the slid R package, and the combined data is saved as states1 in the slid package. In the following, we will work with states1 directly. Now, let’s load the required R packages and data to our working directory. library(geojsonio) library(leaflet) library(dplyr) library(slid) data(state.long) data(states1) # Remove the following regions due to lack of data states1 &lt;- states1 %&gt;% subset(!name %in% c(&#39;Alaska&#39;, &#39;Hawaii&#39;, &#39;Puerto Rico&#39;)) 7.3.1 Create a base map First, we will create a basic states map. The easiest way to add tiles is by calling addTiles() with no arguments: dmap &lt;- leaflet(states1) %&gt;% setView(-96, 37.8, 4, zoom = 4) %&gt;% addTiles() Here, the setView function sets the view of the map (center and zoom level) with the following arguments: lng: the longitude of the map center; lat: the latitude of the map center; zoom: the zoom level. Besides the setView() function, there are other methods to manipulate the map widget, for example: fitBounds(map, lng1, lat1, lng2, lat2): sets the bounds of a map setMaxBounds(map, lng1, lat1, lng2, lat2): restricts the map view to the given bounds clearBounds(map): clears the bounds of a map, which are automatically determined, if available, from the latitudes and longitudes of the map elements (otherwise the full world view is used) Next, if we use the function addPolygons with no additional arguments, then we will obtain the uniform polygons with default styling without any customization. dmap %&gt;% addPolygons() 7.3.2 Color the map Next, we design the color palette for the map. The function colorFactor() converts numeric or factor/character data values to colors using a palette that can be provided in a variety of formats. Two often-used arguments are palette: the colors or color function that values will be mapped to; domain: the possible values that can be mapped. pal.state.factor &lt;- colorFactor( palette = &quot;YlOrRd&quot;, domain = states1$Division ) Now, let us color the states according to the division that they belong to. In this case, you can map the value of the division to colors using fillColor = ~pal.state.factor(Division). We can also customize the map, change the color, line type of the state boundary, and other style properties. dmap %&gt;% addPolygons( fillColor = ~pal.state.factor(Division), weight = 1, opacity = 1, color = &quot;white&quot;, dashArray = &quot;3&quot;, fillOpacity = 0.9, layerId = ~name_ns) 7.3.3 Interactive map On the interactive choropleth map, it is possible to zoom and hover a state to get more details about it. The next step will be to highlight the polygons as the mouse passes over them. The highlight argument in the addPolygon() function makes this simple. We will generate the highlight labels using the sprintf() function, a wrapper for the C library function with the same name. sprintf() returns a character vector with a formatted mix of text and variable values. labels_cases &lt;- sprintf( &quot;&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt;Population: %g M&lt;br&gt; Cumulative Cases: %g&lt;br&gt;Death: %g&lt;br&gt; Infected Cases per Thousand: %g&quot;, states1$name, round(states1$pop / (1e6), 2), states1$Infected, states1$Death, states1$Infect_risk * 1000) %&gt;% lapply(htmltools::HTML) labels_cases[[1]] AlabamaPopulation: 4.89 M Cumulative Cases: 288775Death: 4086 Infected Cases per Thousand: 59.0799 In the above sprintf() function, each % is referred to as a slot, which is basically a placeholder for a variable that will be formatted. The letter s indicates that the formatted variable is specified as a string. %g indicates the formatted variable uses compact decimal or scientific notation. The labelOptions argument of the addPolygons function allows you to customize marker labels. To emphasize the currently moused-over polygon, we will use the highlightOptions function. The labelOptions argument can be populated using the labelOptions() function. Now let us display the state names and values to the user. dmap &lt;- dmap %&gt;% addPolygons( fillColor = ~pal.state.factor(Division), weight = 1, opacity = 1, color = &quot;white&quot;, dashArray = &quot;3&quot;, fillOpacity = 0.9, layerId = ~name_ns, # Options to highlight a shape on hover highlight = highlightOptions( weight = 5, color = &quot;#666&quot;, dashArray = NULL, fillOpacity = 0.9, bringToFront = TRUE), label = labels_cases, labelOptions = labelOptions( style = list(&quot;font-weight&quot; = &quot;normal&quot;, padding = &quot;3px 8px&quot;), textsize = &quot;15px&quot;, direction = &quot;auto&quot;)) dmap When you hover the mouse over a state, the label is displayed. The bringToFront = TRUE in the highlightOptions function is used to prevent the active polygon’s thicker, white border from being hidden behind the borders of other polygons higher in the z-order. Finally, let us add the legend using the function addLegend(). dmap &lt;- dmap %&gt;% addLegend(pal = pal.state.factor, values = ~Division, opacity = 0.7, title = NULL, position = &quot;bottomright&quot;) dmap 7.4 Legend 7.4.1 Classification schemes The map legend can be used to list the features on the map and what they represent. Symbols in the legend should be the same as they are in the body of the map. The following types of spatial attribute data exist: Nominal: attributes are nominal if they are given names or titles to differentiate one entity from another, such as the name of a place, whether urban or rural. Ordinal: attributes are ordinal if their values take on natural order; for instance, the risk level of a disease may be classified with Level 1 representing the lowest risk, level 2 second-lowest, and so on). Numeric: examples of numeric data include temperature, population density, male-to-female ratio, the number of infected cases. Numeric values may vary on a discrete (e.g., integer) or continuous scale. According to Pfeiffer et al. (2008), the continuous attribute data can be divided into six basic classification schemes: Natural groupings of data values: Classes are defined based on what appear to be natural groupings of data values. Breakpoints that are known to be relevant to a particular application, such as fractions and multiples of mobility levels or risk thresholds, may be used to define the breaks. Quantile breaks: The data is divided into a set of classes, each with an equal number of observations. Quintile (five-category) classifications, for example, are ideal for displaying data that is linearly distributed. Breaks with equal intervals: The attribute value’s range is calculated and divided into evenly spaced intervals. This method is useful for mapping data that is uniformly distributed or if the user of the map is familiar with the data ranges (e.g., herd sizes or temperature bands). Standard deviation classifications: This method uses the number of standard deviations above and below the mean to calculate the distance between the observation and the mean. It’s best for data that’s normally distributed. Arithmetic progressions: At an arithmetic (additive) rate, the widths of category intervals increase in size. If the first category is one unit wide and the width is increased by one unit, the second category becomes two units wide, the third three units wide, and so on (1, 3, 6, …). This method is particularly useful when dealing with data from skewed distributions. Geometric progressions: The widths of the category intervals are multiplicatively increased at a geometric rate. For example, if the first category’s interval width is two units, the second category will be \\(2 times 2 = 4\\) units wide, the third \\(2 times 2 times 2 = 8\\) units wide, and so on. This method can also be used to analyze data from skewed distributions. 7.4.2 Mapping variables to colors We demonstrate how to apply the above classification schemes to map values to colors. For simplicity, we wrap the above code into a function and run it with different palettes, data, labels, variables mapped to fill color, etc. map.state &lt;- function(dat, fill.var, labels, pal, ID = &#39;name_ns&#39;){ dmap &lt;- leaflet(dat) %&gt;% setView(-96, 37.8, 4, zoom = 4) %&gt;% addTiles() %&gt;% addPolygons( fillColor = ~pal(dat@data %&gt;% pull(fill.var)), weight = 1, opacity = 1, color = &quot;white&quot;, dashArray = &quot;3&quot;, fillOpacity = 0.9, layerId = ~dat@data %&gt;% pull(ID), highlight = highlightOptions( weight = 5, color = &quot;#666&quot;, dashArray = NULL, fillOpacity = 0.9, bringToFront = TRUE), label = labels, labelOptions = labelOptions( style = list(&quot;font-weight&quot; = &quot;normal&quot;, padding = &quot;3px 8px&quot;), textsize = &quot;15px&quot;, direction = &quot;auto&quot;)) %&gt;% addLegend(pal = pal, values = ~dat@data %&gt;% pull(fill.var), opacity = 0.7, title = NULL, position = &quot;bottomright&quot;) dmap } The family of color*() can be used to generate palette functions easily. There are currently three color functions for dealing with continuous input: colorNumeric, colorBin, and colorQuantile; and one for categorical input, colorFactor. Each function has two required arguments: palette: specifies the colors to map the data to; domain: specifies the range of input values. colorNumeric pal.state.numeric &lt;- colorNumeric( palette = &quot;YlOrRd&quot;, domain = states1$Infected ) map.state(dat = states1, fill.var = &#39;Infected&#39;, labels = labels_cases, pal = pal.state.numeric, ID = &#39;name_ns&#39;) colorQuantile pal.state.quantile &lt;- colorQuantile( palette = &quot;YlOrRd&quot;, domain = states1$Infected, n = 8) map.state(dat = states1, fill.var = &#39;Infected&#39;, labels = labels_cases, pal = pal.state.quantile, ID = &#39;name_ns&#39;) colorBin bins.state&lt;- c(0, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6) pal.state.bins &lt;- colorBin(&quot;YlOrRd&quot;, domain = states1$Infected, bins = bins.state) map.state(dat = states1, fill.var = &#39;Infected&#39;, labels = labels_cases, pal = pal.state.bins, ID = &#39;name_ns&#39;) colorFactor pal.state.factor &lt;- colorFactor(&quot;YlOrRd&quot;, domain = states1$Region) map.state(dat = states1, fill.var = &#39;Region&#39;, labels = labels_cases, pal = pal.state.factor, ID = &#39;name_ns&#39;) 7.5 An Example of County-level Map We are interested in the infection rate at the county level, and would like to draw a county-level choropleth map to illustrate the spatial variation from county to county. First, let us prepare the data. urlRemote &lt;- &quot;https://raw.githubusercontent.com/&quot; pathGithub &lt;- &quot;plotly/datasets/master/&quot; fileName &lt;- &quot;geojson-counties-fips.json&quot; counties0 &lt;- geojson_read(x = paste0(urlRemote, pathGithub, fileName), what = &quot;sp&quot;) Next, we combine the county-level data with the state-level data. counties1 &lt;- counties0 counties1@data &lt;- left_join(counties1@data, states1@data %&gt;% select(id, name, density, name_ns, Region, Division, pop, DATE), by = c(&#39;STATE&#39; = &#39;id&#39;)) names(counties1)[8] &lt;- &#39;state_name&#39; names(counties1)[10] &lt;- &#39;state_name_ns&#39; counties1$id &lt;- as.integer(counties1$id) counties1@data &lt;- left_join(counties1@data, pop.county %&gt;% select(ID, population), by = c(&quot;id&quot; = &quot;ID&quot;)) counties1@data &lt;- left_join(counties1@data, I.county %&gt;% select(ID, X2020.12.11), by = c(&quot;id&quot; = &quot;ID&quot;)) counties1@data &lt;- left_join(counties1@data, D.county %&gt;% select(ID, X2020.12.11), by = c(&quot;id&quot; = &quot;ID&quot;)) names(counties1@data)[16:17] &lt;- c(&#39;Infected&#39;, &#39;Death&#39;) names(counties1@data)[13] &lt;- &#39;pop_state&#39; counties1@data &lt;- counties1@data %&gt;% mutate(Infect_risk = Infected/population) counties1[counties1$id == 46113, &#39;population&#39;] &lt;- 14309 The final dataset counties1 can also be downloaded from the slid package directly. First, let us load the data, color palette and scheme, and the highlight label. # Load the data directly data(counties1) data(states1) counties1 &lt;- counties1 %&gt;% subset(!state_name %in% c(&#39;Alaska&#39;, &#39;Hawaii&#39;, &#39;Puerto Rico&#39;)) states1 &lt;- states1 %&gt;% subset(!name %in% c(&#39;Alaska&#39;, &#39;Hawaii&#39;, &#39;Puerto Rico&#39;)) # Prepare the color palette and scheme col2 &lt;- colorRampPalette(c(&quot;#053061&quot;, &quot;#2166AC&quot;, &quot;#4393C3&quot;, &quot;#92C5DE&quot;,&quot;#D1E5F0&quot;, &quot;#FFFFFF&quot;, &quot;#FDDBC7&quot;, &quot;#F4A582&quot;, &quot;#D6604D&quot;, &quot;#B2182B&quot;, &quot;#67001F&quot;)) pal.county.quantile &lt;- colorQuantile( palette = col2(200), domain = counties1$Infect_risk, n = 8) # Prepare the highlight label labels_cases.county &lt;- sprintf( &quot;&lt;strong&gt;%s&lt;/strong&gt;, &lt;strong&gt;%s&lt;/strong&gt; &lt;br/&gt;Infection Rate: %g &lt;br&gt; Population: %g K &lt;br&gt; Infected Cases on 2020-12-11: %g&lt;br&gt; Death Cases on 2020-12-11: %g&quot;, counties1$NAME, counties1$state_name, round(counties1$Infect_risk, 3), counties1$population / 1000, counties1$Infected, counties1$Death ) %&gt;% lapply(htmltools::HTML) Next, we will draw the county-level map. In Leaflet, map panes group layers together implicitly, which allows web browsers to work with multiple layers at once, saving time over working with layers individually. The z-index CSS property is used in map panes to always show some layers on top of others. The following is the default order: TileLayers and GridLayers Paths, like lines, polylines, circles, or GeoJSON layers. Marker shadows Marker icons Popups You can use the function addMapPane to customize map panes for a leaflet map to control layer order, and you can specify: name: the name of the new pane. zIndex: the zIndex of the pane. Panes with a higher index are rendered first, followed by panes with a lower index. By setting the pane argument in leafletOptions, you can use this pane to render overlays (points, lines, and polygons). This will allow you to control the layer order; for example, points will always be on top of polygons, and states will always be on top of counties. # Draw the county-level map dmap2 &lt;- leaflet() %&gt;% setView(-96, 37.8, zoom = 4) %&gt;% addTiles() %&gt;% # Add additional panes to control layer order # Display borders (zIndex: 420) above the polygons (zIndex: 410) addMapPane(&quot;polygons&quot;, zIndex = 410) %&gt;% addMapPane(&quot;borders&quot;, zIndex = 420) %&gt;% # Add state polygons (borders pane) addPolygons( data = states1, fill = FALSE, weight = 1, color = &quot;gray&quot;, fillOpacity = 0, options = pathOptions(pane = &quot;borders&quot;) ) %&gt;% # Add county polygons (polygons pane) addPolygons( data = counties1, fillColor = ~pal.county.quantile(Infect_risk), weight = 1, opacity = 1, color = &quot;white&quot;, dashArray = &quot;3&quot;, fillOpacity = 0.9, highlight = highlightOptions( weight = 5, color = &quot;#666&quot;, dashArray = NULL, fillOpacity = 0.9, bringToFront = TRUE), label = labels_cases.county, layerId = ~id, labelOptions = labelOptions( style = list(&quot;font-weight&quot; = &quot;normal&quot;, padding = &quot;3px 8px&quot;), textsize = &quot;15px&quot;, direction = &quot;auto&quot;), options = pathOptions(pane = &quot;polygons&quot;)) %&gt;% addLegend(data = counties1, pal = pal.county.quantile, values = ~Infect_risk, opacity = 0.7, title = NULL, position = &quot;bottomright&quot;) dmap2 7.6 Spot Maps A spot map is a map that depicts the geographic location of people who share a common characteristic, such as the number of infectious disease cases. Spot maps are typically used to visualize clusters or outbreaks with a small number of cases. Next, we draw a spot map and highlight the top 10 counties with the largest cumulative infected count on December 11, 2020. First, let us prepare the data required. data(features.county) names(features.county) ## [1] &quot;ID&quot; &quot;County&quot; ## [3] &quot;State&quot; &quot;FIPS_C&quot; ## [5] &quot;FIPS_S&quot; &quot;avemort&quot; ## [7] &quot;BlackRate&quot; &quot;HLRate&quot; ## [9] &quot;Gini&quot; &quot;Affluence&quot; ## [11] &quot;HighIncome&quot; &quot;EduAttain&quot; ## [13] &quot;OccupAdv&quot; &quot;MedHouVal&quot; ## [15] &quot;Disadvantage&quot; &quot;PublicAssistance&quot; ## [17] &quot;FemaleLeadRate&quot; &quot;EmployStatus&quot; ## [19] &quot;ViolentCrime&quot; &quot;PropertyCrime&quot; ## [21] &quot;ResidStability&quot; &quot;UrbanRate&quot; ## [23] &quot;HealCovRate&quot; &quot;ExpHealth&quot; ## [25] &quot;Latitude&quot; &quot;Longitude&quot; ## [27] &quot;MF&quot; &quot;dPop_ml2&quot; ## [29] &quot;LOG_pop&quot; &quot;prop_old&quot; ## [31] &quot;BED_SUM&quot; data(county.top10.long) names(county.top10.long) ## [1] &quot;ID&quot; &quot;County&quot; &quot;State&quot; &quot;Date&quot; &quot;Count&quot; ## [6] &quot;type&quot; &quot;Count_lb&quot; &quot;Count_ub&quot; # Combine the datasets with useful variables location.county &lt;- features.county %&gt;% dplyr:: select(ID, Longitude, Latitude) county.top10.today &lt;- county.top10.long %&gt;% select(ID, County, State, Date, Count) %&gt;% filter(Date == &quot;2020-12-11&quot;) df &lt;- left_join(county.top10.today, location.county, key = &quot;ID&quot;) df ## # A tibble: 10 x 7 ## ID County State Date Count Longitude Latitude ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6037 LosAnge… Cali… 2020-12-11 501635 -118. 34.2 ## 2 17031 Cook Illi… 2020-12-11 346004 -87.6 41.9 ## 3 12086 Miami-D… Flor… 2020-12-11 253403 -80.5 25.6 ## 4 4013 Maricopa Ariz… 2020-12-11 245671 -112. 33.3 ## 5 48201 Harris Texas 2020-12-11 204850 -95.4 29.9 ## 6 48113 Dallas Texas 2020-12-11 156225 -96.8 32.8 ## 7 32003 Clark Neva… 2020-12-11 137100 -115. 36.2 ## 8 6071 SanBern… Cali… 2020-12-11 120186 -116. 34.9 ## 9 12011 Broward Flor… 2020-12-11 118512 -80.5 26.2 ## 10 48439 Tarrant Texas 2020-12-11 116931 -97.3 32.8 We start to draw a base map. dmap3 &lt;- leaflet() %&gt;% setView(-96, 37.8, zoom = 4) %&gt;% addTiles() dmap3 7.6.1 Adding circles We can add circles to the map to highlight the top ten counties in the data using addCircles(). Circles are similar to circle markers; the only difference is that circles have their radii specified in meters, while circle markers are specified in pixels. As a result, the size of circles will change as the user zooms in and out, while circle markers remain a constant size on the screen regardless of zoom level. dmap3 &lt;- dmap3 %&gt;% addCircles(data = df, lng = ~Longitude, lat = ~Latitude, weight = 1, radius = ~sqrt(Count)*200, popup = ~County ) dmap3 Each point can have text added to it using either a popup (appears only on click) or a label (appears either on hover or statically). We will describe the details below. 7.6.2 Add popups Popups are small boxes that contain HTML outputs such as texts or hyperlinks and point to a specific point or location on the map. Popups are frequently used so that they appear when markers or shapes are clicked. The Leaflet package’s marker and shape functions take a popup argument, where we can pass in HTML commands to easily attach a simple popup. For instance, we can label each county with the name of the county and state, and the reported cumulative infected cases. labels_cases.county &lt;- sprintf( &quot;&lt;strong&gt;%s&lt;/strong&gt;, &lt;strong&gt;%s&lt;/strong&gt; &lt;br/&gt; Cum. Infected Cases on 2020-12-11: %g &lt;br&gt;&quot;, df$County, df$State, df$Count) %&gt;% lapply(htmltools::HTML) Using lapply(htmltools::HTML), one can pass the information to Leaflet, so it knows to treat each label as HTML instead of as plain text. If we only want the information to appear when we click on the point, we should instead use popup = ~labels_cases.county like the following: dmap3 %&gt;% addMarkers(data = df, lng = ~Longitude, lat = ~Latitude, popup = ~labels_cases.county) 7.6.3 Add labels A label is textual or HTML content that is attached to markers and shapes and is visible at all times or when the mouse is moved over it. You don’t have to click a marker/polygon to see the label, unlike popups. dmap3 %&gt;% addMarkers(data = df, lng = ~Longitude, lat = ~Latitude, label = ~labels_cases.county) 7.7 Integrating Leaflet with R Shiny For integrating with Shiny applications, the Leaflet package includes powerful and convenient features. Most Shiny output widgets are integrated into an app by including a render function in the server function and including an output for the widget in the UI definition. You can call leafletOutput in the UI and assign a renderLeaflet call to the output on the server side to return a Leaflet map object for Leaflet maps. shinyServer(function (input, output) { output$map &lt;- renderLeaflet({}) }) server.R shinyServer(function (input, output) { # Prepare data location.county &lt;- slid::features.county %&gt;% dplyr:: select(ID, Longitude, Latitude) county.top10.today &lt;- slid::county.top10.long %&gt;% select(ID, County, State, Date, Count) %&gt;% filter(Date == &quot;2020-12-11&quot;) df &lt;- left_join(county.top10.today, location.county, key = &quot;ID&quot;) # Prepare popup label labels_cases.county &lt;- sprintf( &quot;&lt;strong&gt;%s&lt;/strong&gt;, &lt;strong&gt;%s&lt;/strong&gt; &lt;br/&gt; Cum. Infected Cases on 2020-12-11: %g &lt;br&gt;&quot;, df$County, df$State, df$Count ) %&gt;% lapply(htmltools::HTML) # Draw a spot map with popup output$map &lt;- renderLeaflet({ leaflet() %&gt;% setView(-96, 37.8, zoom = 4) %&gt;% addTiles() %&gt;% addCircles(data = df, lng = ~Longitude, lat = ~Latitude, weight = 1, radius = ~sqrt(Count)*200, popup = ~County) %&gt;% addMarkers(data = df, lng = ~Longitude, lat = ~Latitude, popup = ~labels_cases.county) }) }) ui.R library(shiny) library(dplyr) library(leaflet) shinyUI( # Use a fluid layout fluidPage( # Give the page a title titlePanel( HTML( paste(&quot;Top 10 counties with the largest cumulative&quot;, &#39;&lt;br/&gt;&#39;, &quot;infected count on December 11, 2020&quot;) )), mainPanel(leafletOutput(&quot;map&quot;)) )) If everything is working correctly, you will see the application appear in your browser looking something like this: 7.8 Exercises Create polygons based on the given coordinates: Let x1 &lt;- c(6, 8, 8, 6, 6) x2 &lt;- c(6, 6, 4, 4, 6) Let y1 &lt;- c(5, 6, 8, 10, 5) y2 &lt;- c(8, 3, 2, 8, 8) Draw the two polygons in (a) and (b) on the same map. The Washington Monument is located at longitude -77.0353 and latitude 38.8895. Draw base map and set the default view to and set zoom level 15; Add a popup “Washington Monument” to your base map; Add a label “Washington Monument” to your base map; We will use the state level COVID-19 data (I.state) available in the slid package, and the geospatial information from: library(geojsonio) urlRemote &lt;- &quot;https://raw.githubusercontent.com/&quot; pathGithub &lt;- &quot;PublicaMundi/MappingAPI/master/data/geojson/&quot; fileName &lt;- &quot;us-states.json&quot; geojson_read(x = paste0(urlRemote, pathGithub, fileName), what = &quot;sp&quot;) Calculate the weekly risk of each state and create a weekly_risk variable, which contains the number of newly infected cases in the past weeks (December 5, 2020, to December 11, 2020) for each state divided by its population. Draw a choropleth map to display the weekly risk for each state. You can change the opacity and the weight of the borderlines according to your aesthetic preferences. Use colorBin to color the states. Generate the highlighted label with the state name and the value of weekly_risk, and population, and display the label when the mouse moves over to the state. Save your leaflet map as an HTML file. We will use the county level COVID-19 data (I.county) available in the slid package, and the geospatial information from: urlRemote &lt;- &quot;https://raw.githubusercontent.com/&quot; pathGithub &lt;- &quot;plotly/datasets/master/&quot; fileName &lt;- &quot;geojson-counties-fips.json&quot; geojson_read(x = paste0(urlRemote, pathGithub, fileName), what = &quot;sp&quot;) Calculate the weekly risk of each county and create a weekly_risk variable, which contains the number of newly infected cases in the past weeks (December 5, 2020, to December 11, 2020) for each county divided by its population. Draw a choropleth map to display the weekly risk for each county. You can change the opacity and the weight of the borderlines according to your aesthetic preferences. Use colorQuantile to color the counties. Generate the highlighted label with the county, state name and the value of weekly_risk, and population, and display the label when the mouse moves over to the county. Draw a spot map and highlight the top 10 counties with the highest weekly risk in the past week (December 5, 2020, to December 11, 2020). Save each of the above leaflet maps as an HTML file. "],["modeling.html", "Chapter 8 Epidemic Modeling 8.1 An Introduction to Epidemic Modeling 8.2 Mechanistic Models 8.3 Phenomenological Models 8.4 Hybrid Models and Ensemble Methods 8.5 Epidemic Modeling: Mathematical and Statistical Perspectives 8.6 Some Terminologies in Epidemic Modeling", " Chapter 8 Epidemic Modeling 8.1 An Introduction to Epidemic Modeling The earliest disease transmission model dates back to Daniel Bernoulli (Dietz and Heesterbeek 2002). Bernoulli developed an epidemiological model in 1760 and used it to demonstrate the life expectancy increased due to the use of inoculation against smallpox. William Farr was perhaps the first to attempt to calibrate (or fit) a model of disease transmission when he noted the similarity between the time series of smallpox incidence and the normal distribution. In 1889, En’ko investigated heterogeneity in measles transmission, using models to explain the possible mechanisms that give rise to variable epidemic sizes; see En’Ko (1989). In 1927, Kermack and McKendrick expanded on existing models to develop the foundation of the modern-day susceptible-infected-recovered mass-action model using differential equations; see Kermack and McKendrick (1927). These models, and many others, have formed the basis of modern disease modeling, which has been a crucial tool in improving understanding of best practices for infectious disease control. By combining mathematical or statistical formalism with epidemiological data and an understanding of biological mechanisms, infectious disease models enable the evaluation of the effect of public health interventions like vaccines, projection of future disease burden in various contexts, and answering fundamental questions, such as why some people are uninfected in an outbreak. In epidemic modeling, there are mainly three types of models, including (i) mechanistic models, (ii) phenomenological models, and (iii) hybrid models. In the following, we will give a brief introduction to each type of models. More indepth studies will be covered in Chapters 9–13. 8.2 Mechanistic Models Mechanistic models explicitly try to model the mechanisms of interaction among system components. These models make explicit hypotheses about the biological mechanisms that drive infection dynamics. 8.2.1 Compartment Modeling A compartment model is characterized by a set of mathematical equations to represent how individuals move from one compartment to another or interact among compartments. A majority of the compartment models in epidemiology are based on the Susceptible-infected-removed (SIR) model Kermack and McKendrick (1927) and its variants to conceptualize the dynamic changes of the population. The conventional SIR model divides a population into three compartments: the susceptible class (S) indicates those who can become infected, the infected class (I) represents those who are infectious, and the removed class (R) corresponds to those with permanent infection-acquired immunity. Due to its simplicity with a small number of parameters, the SIR model has become popular and has been widely applied to track and forecast the trajectory of epidemic disease. For example, Miranda et al. (2019) calibrated the parameters of the SIR model for each season separately using the Nelder-Mead optimization algorithm to predict weekly influenza-like-illness (ILI) incidence. Chen et al. (2021) considered the SIR model for Canadian COVID-19 data. The estimates and bounds of model parameters were extracted by minimizing the squared prediction error of daily infection numbers. Based on those values, the prediction bounds for S, I, R, and daily infection numbers can be obtained. However, the SIR model often oversimplifies complex processes of disease. For many epidemic diseases, if an exposed period (infected but not yet infectious) is long enough or significant, its absence in the model could affect predictions Brauer (2008). The SEIR model additionally includes an exposed (E) compartment for the latent period. The SEIR has been applied to malaria transmission by formulating a system of equations for both human and mosquito population Mojeeb, Adu, and Yang (2017) and Ebola hemorrhagic fever by considering vaccination as a control strategy (Durojaye and Ajie 2017). Roda et al. (2020) considered both SIR and SEIR models and predicted the COVID-19 epidemic. Although a Poisson or negative binomial distribution is a common choice for count data, its probability model was approximated by normal distribution as newly confirmed cases grow quickly. Using uniform priors for model parameters, the affine invariant ensemble Markov Chain Monte Carlo algorithm was implemented for posterior distributions of the model parameters to lessen the impact of nonidentifiability in the model. In Chapter 9, we will study compartment models with more detailed structure and see how they can help study infectious diseases. 8.2.2 Agent-based Methods Agent-based models, also known as individual-based models, are a powerful simulation modeling technique with interacting autonomous agents. Agents are artificial decision-making entities and programmed to interact with other agents and the environment in specific ways. In the epidemic modeling, agents could stay either at the state of susceptibility, the state of infected, or any pre-defined states. Then, these agents interact with each other based on a specific environment, which is usually defined through a social contact network. Agents can transfer from a state to another. A typical agent-based model consists of three key elements: (1) a population generated based on the demographic characteristics of the studied population, (2) a social contact network among the agents (individuals) in the population, and (3) a disease model, which translates the edge weights in the social contact network into infection probability (Hoertel et al. 2020). By simulating a set of simple agents’ interactions, they can generate complex global patterns (behaviors) and visualize main properties from a global perspective. A good example of using this approach for COVID-19 is by a team from Imperial College London KRR et al. (2020) who applied an agent-based modeling approach to the UK data. The authors predicted the spread of COVID-19 after including many Non-Pharmaceutical Interventions. Hoertel et al. (2020) considers a stochastic agent-based model of the SARS-CoV-2 epidemic to predict the number of COVID-19 cases in France. A team of researchers from various universities proposed a Global Epidemic and Mobility Model (GLEAM) which employed an agent-based model to analyze the spatiotemporal spread of COVID-19 in the continental US. Agent-based models use more detailed descriptions of disease states and/or individual characteristics and behavior, which are not easily simplified into a compartmental form. It allows for exploring complex systems and capturing relationships among individuals and heterogeneity in their attributes (Rahmandad and Sterman 2008). Besides, it can model experiments that may be impossible or unethical to conduct in the real world, guiding public health interventions. Agent-based models require intensive computational burdens, constraining sensitivity analysis. Data on contact networks and the distribution of individual attributes are hard to obtain and highly uncertain, requiring extensive sensitivity analysis to ensure robust results. 8.3 Phenomenological Models Phenomenological models, sometimes also referred to as statistical models, attempt to characterize and forecast the observed effects of epidemics without incorporating biological mechanisms and postulating conjectures that explain the observed phenomena. 8.3.1 Time Series Analysis In time series analysis, we focus on using the past observations of a random variable to captures the underlying patterns and predict the future values. Within time series forecasting, exponential smoothing and autoregressive integrated moving average (ARIMA) models are the two most widely used approaches. Given a time series data, exponential smoothing models are based on a description of the trend and seasonality, while ARIMA models focus on describing the autocorrelations. Below, we will give a short introduction to ETS and ARIMA models. Chapter 10 will introduce the details of time series analysis and how to make the epidemic prediction using R. 8.3.1.1 Exponential Smoothing (ETS) Exponential smoothing was proposed by Brown (1959), Holt (1957), and Winters (1960) in the late 1950s. Traditional moving average method considers equal weights for the past observations, whereas exponential functions are used to assign exponentially decreasing weights as the observations get older in exponential smoothing. Therefore, the forecasts produced by exponential smoothing methods can be considered as weighted averages of past observations. Since the simple exponential smoothing framework usually can generate reliable forecasts quickly, the simple ETS method is suitable for forecasting data with no clear trend or seasonal pattern. Holt’s Linear Trend Method: To allow the forecasting of data with a trend, Holt (1957) proposed a linear trend method based on the simple exponential smoothing. This method involves three equations, including (i) a forecast equation, (ii) a smoothing equation for the level, and (iii) a smoothing equation for the trend. As a result, the \\(h\\)-step-ahead forecast equals to the sum of the last estimated level and \\(h\\) times the last estimated trend value, and thus the forecasts are linear to \\(h\\). The constant trend estimated by the Holt’s linear usually makes the long term forecast either indefinitely increasing or decreasing into the future. Damped Trend Method: Due to the limitation of constant trend, Gardner and Mckenzie (1985) proposed a “damped” method which includes a parameter that can “dampens” the trend to a flat line in the future. As pointed out in Hyndman and Athanasopoulos (2018a), the “damped” method is one of the most popular time-series methods for forecasting. Holt-Winters’ Seasonal Method: By considering a systematic trend or a seasonal component, Holt (1957) and Winters (1960) extended Holt’s method to the Holt-Winters’ Seasonal Method which can analysis the time series with seasonality. Instead of the three equations in Holt’s Linear Trend Method, the Holt-Winters seasonal method comprises one more smoothing equation for seasonal component. 8.3.1.2 AutoRegressive Integrated Moving Average (ARIMA) model provides another approach to time series forecasting, and it combines the autoregression (AR) and moving average (MA) model. In an ARIMA model, linear correlations among the time-series are assumed, then the ARIMA model exploits these linear dependencies to extract local patterns and remove high-frequency noise from the data. The ARIMA approach has three clear benefits. First of all, the interpretability level of the ARIMA model is very high. Therefore, researchers are able to gain a deep understanding of the relationship between the current and the past situations, and explore the influence of some exogenous variables. Secondly, the ARIMA model has an automated way to maximize prediction accuracy which can perform model selection efficiently. Thirdly, the ARIMA models have high accommodative ability due to the simplicity of model updates based on recent events. However, one drawback of the ARIMA models is that they cannot deal with nonlinear patterns or relationships. 8.3.2 Regression methods In epidemic modeling, regression methods are also popularly applied to estimate future prevalence or targets of interest. For example, Altieri et al. (2021) considered five regression models with different trends in COVID-19 death counts. In addition, the Los Alamos National Laboratory (LANL) has proposed a regression method called COVID-19 Forecasts using Fast Evaluations and Estimation (COFFEE); see Castro1 et al. (2020). We will introduce (generalized) linear regression models and other regression models in Chapter 11. We will also present how the analytical techniques of regression and discrimination as a means of quantifying the effect of a set of explanatory variables on the spatial distribution of a particular outcome. 8.3.3 Machine learning methods Machine learning methods are attractive in the COVID-19 prediction with their great flexibility to capture disease spread patterns. There are two major categories of machine learning methods in the prediction of COVID-19. The first kind of methods are based on epidemic models and trained by machine learning algorithms. Specifically, Zou et al. (2020) proposes a variant of the SEIR model accounting for the untested/unreported cases of COVID-19, and the model is estimated by the standard gradient-based optimizer. Arik et al. (2020) integrates machine learning into compartmental disease modeling. It uses learning mechanisms, such as masked supervision from partial observations and partial teacher-forcing to minimize error propagation, to improve model estimation with limited training data. The second kind of method is based on conventional machine learning methods. Sujath, Chatterjee, and Hassanien (2020) applied linear regression, multilayer perceptron, and vector autoregression method for predicting COVID-19 cases in India gathered from the website of Kaggle. In Arora, Kumar, and Panigrahi (2020), recurrent neural network (RNN) based long-short term memory (LSTM) is used to predict the number of COVID-19 reported cases for the state-level data in India. In Chapter 12, we will provide some implementation details of neural network models for forecasting. 8.4 Hybrid Models and Ensemble Methods Hybrid modeling often refers to the approaches where part of a model is formulated on the basis of mechanistic principles and part of the model has to be inferred from data because of a lack of understanding of the mechanistic details. In the past two decades, statistical and machine learning-inspired time series methods have been successfully implemented in studying the spread dynamics of infectious diseases such as seasonal influenza. In COVID-19 studies, we have seen a growing number of hybrid methods combining characteristics of mechanistic models and phenomenological models. Examples of hybrid models and will be given Chapter 13. Another hybrid method widely used in forecasting is the ensemble method, which combines multiple forecasting algorithms to improve predictive performance. The basic idea of ensemble methods is that when there is much uncertainty in finding the best model, combining different algorithms could improve prediction accuracy by reducing the instability of the forecast. We will also introduce some ensemble methods using multiple forecasting algorithms to improve the predictive performance in Chapter 13. 8.5 Epidemic Modeling: Mathematical and Statistical Perspectives Epidemic modeling has three main aims (Daley and Gani 2001): (1) to understand better the mechanisms by which diseases spread; (2) to identify which factors contribute to the spread of the epidemic, and therefore how we may control it; (3) to predict the future course of the epidemic. Although there are many epidemic modeling methods, mathematical and statistical models have played important roles in COVID-19studies. As illustrated in Figure 8.1, mathematical and statistical approaches are complementary, but their starting points are different, and the corresponding models tend to incorporate different details. Figure 8.1: Mathematical and statistical perspectives on epidemic modeling. As mentioned above, the fundamental concept of infectious disease epidemiology is investigating how the diseases spread. Mathematical models are undeniably useful in understanding the dynamics of infectious disease spread (e.g., when the peak will occur and whether resurgence will happen) and the effects of control measures (Keeling and Rohani 2008). An essential type of mathematical model is the class of mechanistic models such as the Susceptible - Infectious - Removed (SIR) compartmental model or the Susceptible - Exposed - Infectious - Recovered model (SEIR) as illustrated in Figure 8.2; see details in Brauer, Driessche, and Wu (2008) and Lawson et al. (2016). Mechanistic models make explicit hypotheses about the biological mechanisms that drive the dynamics of infection, and they function well if the aim is to evaluate the effectiveness of hypothetical NPIs in controlling disease spread (Lessler and Cummings 2016). Figure 8.2: An illustration of SIR and SEIR models. In the literature, statistical modeling has given the scientific field many successes in analyzing data and getting information about the mechanisms producing the data. Statistical modeling is a powerful tool for extracting information about the disease spread in epidemic studies (Held et al. 2020). Statistics starts with data, and statistical modeling allows data to speak for themselves. There are two cultures in statistical modeling (Breiman 2001): the data modeling culture and the algorithmic modeling culture. The first one assumes that the data are generated by a given stochastic data model, and it is usually designed for inference about the relationships between variables whilst also catering to prediction. Algorithmic models treat the data mechanism as unknown and are usually designed to make the most accurate predictions possible. When analyzing the spread of infectious diseases, other factors, such as demographic characteristics, socioeconomic status, and control policies, may also be responsible for temporal or spatial patterns. For example, the spread of the disease varies considerably across different geographical regions. Local area - features, like socioeconomic factors and demographic conditions, can dramatically influence the course of the epidemic. These data are usually supplemented with the population information at the county level. Moreover, the capacity of the health care system, and control measures, such as government - mandated social distancing, also have a significant impact on the spread of the epidemic. Regression is a widely used statistical modeling method in epidemic studies because it produces a combination of the variables with weights indicating the impact of the variable (Jewell 2003). It can help determine which factors matter most, which can be ignored, and how those factors interact with each other. The benefit of regression analysis is that it can be used to understand different patterns in data. These insights may often be very valuable in understanding which factors contribute to the spread of COVID-19. Predicting the spread speed and severity of COVID-19 is crucial to resource management, developing strategies to deal with the epidemic, and ultimately assisting in prevention efforts. Mathematical models are able to mimic the way disease spreads and can be used to project or simulate future transmission scenarios under various assumptions. Statistical models are more oriented towards predictions (Held et al. 2020). In fact, predictions are at the heart of statistical modeling. For example, time series analysis, one commonly used statistical forecasting approach, works by taking a series of historical observations and extrapolating the patterns into the future. Machine learning makes predictions based on known properties learned from the training data. However, purely statistical models only describe the observed data and give little information about the mechanism since they do not account for how transmission occurs. Therefore, they are generally not well - suited for long - term predictions, and a few weeks is usually close to being the ultimate prediction limit. Another advantage of statistical modeling is its ability to quantify uncertainty in the prediction, especially at an early phase of an epidemic with limited data. For example, statistical models can provide a prediction interval to understand the uncertainty surrounding the forecast (Brockwell and Davis 2016). In summary, mathematical models are usually constructed in a more principle - driven manner, while statistical models are more data driven. Although both mathematical and statistical models can be used to study the effect of NPIs and make predictions, the implementation details are different, and an understanding of the corresponding limitations is crucial. For maximal effectiveness, researchers working to advance epidemic modeling will need to appreciate and exploit the complementary strengths of mathematical and statistical models. 8.6 Some Terminologies in Epidemic Modeling In this section, we introduce some terminologies often used in infectious disease modeling. Incidence: number of new cases in a given time period expressed as percent infected per year (cumulative incidence) or number per person time of observation (incidence density). An example: Auckland in New Zealand, often has epidemics of meningococcal disease, with annual incidences of up to 16.9/ 100,000 people. Prevalence: number of cases at a given time expressed as a percent at a given time. An example: A recent Scottish study showed that the prevalence of obesity in a group of children aged from 3 to 4 years was 12.8% at the time. Attack rate: proportion of non-immune exposed individuals who become clinically ill. An example: In an outbreak of gastroenteritis with 50 cases among a population at risk of 2500, the attack rate of disease is 50/2500 = 0.02. Primary/secondary cases: The person who comes into and infects a population is the primary case. Those who subsequently contract the infection are secondary cases. Further spread is described as “waves” or “generations.” Case fatality: proportion of infected individuals who die of the infection. Mortality: proportion of the population that die yearly of the disease and is a product of incidence and the case fatality rate. As an example, consider two populations. Population 1 has 1,000 people; 300 have the specified disease, 100 of whom die from it. The mortality rate for the disease is 100/1,000 = 10%. The case fatality rate is 100/300 = 33%. Population 2 also has 1,000 people; 50 people have the disease and 40 die from it. The mortality rate is 40/1,000 = 4%; the case fatality rate, however, is 40/50 = 80%. Theincidence of death from the disease is higher in the first population, but the severity of disease is greater in the second. Reproductive rate: potential for an infectious disease to spread. Influential factors include: probability of transmission between an infected and a susceptible individual; frequency of population contact; duration of infection; population immune proportion. Transmission routes: direct (mucous membrane to mucous membrane, cross placental, blood or tissue, skin to skin, sneezes or coughs) and indirect (water, air borne, food borne, vectors or objects). Reservoir: the population of organisms or the specific environment in which an infectious pathogen naturally lives and reproduces, or upon which the pathogen primarily depends for its survival. Incubation period: time from exposure to development of disease (the time between inoculation and symptom expression). Infectious period: length of time a person can transmit disease. Latent period: period of infection without being infectious (the time between inoculation and infectiousness of the host). This may occur right after exposure or late in the disease. Epidemic: occurrence of cases of illness in excess of expectancy. An epidemic whose incidence remains stable for a long period is describes as endemic. A global outbreak is described as a pandemic. "],["compartment.html", "Chapter 9 Compartment Models 9.1 A Simple SIS Model (A Model for Diseases with No Immunity) 9.2 An SIR Model 9.3 SIR parameter estimation 9.4 Implementation of parameter estimation in R 9.5 Basic Reproduction Number 9.6 Herd Immunity 9.7 Exercises", " Chapter 9 Compartment Models Mathematical models of infectious disease dynamics have a rich history that dates back more than 100 years. Mathematically simple formulations that describe the transition of individuals in a population between ‘compartments’ that capture the infection status of individuals leads to surprisingly significant insight. Their elegance and simplicity allow the ease of expansion to more complexities through, for example, the addition of compartments. Expanding these models is often straight forward but the apparent simplicity can mask subtle, but important, model structure and parameterization choices. Furthermore, while there are many wonderful texts focused on infectious disease modeling, there exist several complexities that are rarely discussed in sufficient detail for a novice disease modeler. We formulate our descriptions of disease transmission as compartmental models, with the population under study being divided into compartments. At time \\(t\\), denote \\(S(t)\\): the number of susceptible people; \\(E(t)\\): the number of infected but not yet infectious people; \\(I(t)\\): the number of infectious people; \\(R(t)\\): the number of recovered people. 9.1 A Simple SIS Model (A Model for Diseases with No Immunity) In many diseases, infectives return to the susceptible class on recovery because the disease confers no immunity. Such models are appropriate for most diseases transmitted by bacterial or helminth agents, and most sexually transmitted diseases (including gonorrhea, but not such diseases as AIDS, from which there is no recovery). Figure 9.1: A simple SIS model. In an SIS model, the total population size \\(N = S(t) + I(t)\\). The simplest SIS model, due to Kermack and McKendrick, is \\[\\begin{eqnarray*} \\frac{dS(t)}{dt}&amp; = &amp; - \\beta I(t)\\frac{S(t)}{N} + \\gamma I(t), \\\\ \\frac{dI(t)}{dt}&amp; = &amp;\\beta I(t)\\frac{S(t)}{N} - \\gamma I(t) \\end{eqnarray*}\\] where \\(\\beta\\) is the effective contact rate, and \\[ \\beta \\propto \\left(\\frac{\\mathrm{infection}}{\\mathrm{contact}} \\right) \\times \\left(\\frac{\\mathrm{contact}}{\\mathrm{time}}\\right). \\] Assumptions of SIS models The rate of new infections is given by mass action incidence. Infectives leave the infective class at rate \\(\\gamma I\\) per unit time and return to the susceptible class. There is no entry into or departure from the population. There are no disease deaths, and the total population size is a constant \\(N\\). 9.2 An SIR Model Figure 9.2: An SIR model. Consider the SIR model in a population of size \\(N\\), and note \\(N = S(t) + I(t) + R(t)\\): \\[\\begin{eqnarray*} \\frac{dS(t)}{dt}&amp; = &amp; - \\beta I(t)\\frac{S(t)}{N}, \\\\ \\frac{dI(t)}{dt}&amp; = &amp;\\beta I(t)\\frac{S(t)}{N} - \\gamma I(t), \\\\ \\frac{dR(t)}{dt}&amp; = &amp;\\gamma I(t), \\end{eqnarray*}\\] where \\(\\beta\\) is the effective contact rate, and \\[ \\beta \\propto \\left(\\frac{\\mathrm{infection}}{\\mathrm{contact}} \\right) \\times \\left(\\frac{\\mathrm{contact}}{\\mathrm{time}}\\right), \\] i.e., (probability of transmission given a contact between a susceptible and an infectious individual) \\(\\times\\) (average rate of contact between susceptible and infected individuals); \\(\\gamma\\) is the removal rate, and \\(\\gamma^{-1}\\) is the average infectious period. The logic of the transmission term is that \\(\\beta\\) is the contact rate among hosts times the probability of infection given a contact. Let \\(s = S / N\\), \\(i = I / N\\) and \\(r = R / N\\). Dividing the equations for \\(S\\), \\(I\\) and \\(R\\) by \\(N\\) we get the deterministic SIR epidemic model for this process in the form: \\[\\begin{eqnarray*} \\frac{ds(t)}{dt}&amp; = &amp; - \\beta i(t)s(t), \\\\ \\frac{di(t)}{dt}&amp; = &amp;\\beta i(t)s(t) - \\gamma i(t), \\\\ \\frac{dr(t)}{dt}&amp; = &amp;\\gamma i(t). \\end{eqnarray*}\\] Assumptions of SIR models SIR models for diseases assume that infectives recover with immunity against reinfection. Constant (closed) population size: \\(N\\); Constant rates (e.g., transmission, removal rates); No demography (i.e., births and deaths); Well - mixed population: any infected individual has a probability of contacting any susceptible individual that is reasonably well approximated by the average. ## Load deSolve package library(deSolve) ## Create an SIR function sir &lt;- function(time, state, parameters) { with(as.list(c(state, parameters)), { dS &lt;- -beta * S * I dI &lt;- beta * S * I - gamma * I dR &lt;- gamma * I return(list(c(dS, dI, dR))) }) } Below, we consider a simulation example with \\(s(0)=0.999\\), \\(i(0)=0.001\\), \\(r(0)=0.0\\), \\(\\beta=0.3\\) and \\(\\gamma=0.1\\). # Set parameters # Proportion in each compartment: Susceptible 0.999, # Infected 0.001, Recovered 0 init &lt;- c(S = 0.999, I = 0.001, R = 0.0) # beta: infection parameter; gamma: recovery parameter parameters &lt;- c(beta = 0.3, gamma = 0.1) # Time frame times &lt;- seq(0, 300, by = 1) # Solve using ode # (General Solver for Ordinary Differential Equations) out &lt;- ode(y = init, times = times, func = sir, parms = parameters) # Change to data frame out &lt;- as.data.frame(out) # Show data head(out, 5) ## time S I R ## 1 0 0.9990000 0.001000000 0.0000000000 ## 2 1 0.9986676 0.001221467 0.0001109293 ## 3 2 0.9982619 0.001491698 0.0002463585 ## 4 3 0.9977671 0.001821286 0.0004116470 ## 5 4 0.9971641 0.002222741 0.0006131418 Figure 9.3 shows the evolution of the prevalence rate. Figure 9.3: Simulation example 1. Next, we consider another simulation example with \\(s(0)=0.999\\), \\(i(0)=0.001\\), \\(r(0)=0.0\\), \\(\\beta=0.075\\) and \\(\\gamma=0.1\\). # Set parameters # Proportion in each compartment: Susceptible 0.999, # Infected 0.001, Recovered 0 init &lt;- c(S = 0.999, I = 0.001, R = 0.0) # beta: infection parameter; gamma: recovery parameter parameters &lt;- c(beta = 0.075, gamma = 0.1) # Time frame times &lt;- seq(0, 300, by = 1) # Solve using ode # (General Solver for Ordinary Differential Equations) out &lt;- ode(y = init, times = times, func = sir, parms = parameters) # Change to data frame out &lt;- as.data.frame(out) Figure 9.4 shows the evolution of the prevalence rate. Figure 9.4: Simulation example 1. 9.2.1 An SIR model with births and deaths We have omitted births and deaths in our description of epidemic models because the time scale of an epidemic is generally much shorter than the demographic time scale. In effect, we have used a time scale on which the number of births and deaths in unit time is negligible. However, there are diseases that are endemic in many parts of the world and that cause millions of deaths each year. To model a disease that may be endemic, we need to think on a longer time scale and include births and deaths. Figure 9.5: An SIR model with birth and death. We consider the following SIR model with births and deaths: \\[\\begin{eqnarray*} \\frac{dS(t)}{dt}&amp; = &amp;\\mu\\{N - S(t)\\} - \\beta I(t)\\frac{S(t)}{N}, \\\\ \\frac{dI(t)}{dt}&amp; = &amp;\\beta I(t)\\frac{S(t)}{N} - (\\mu + \\gamma)I(t), \\\\ \\frac{dR(t)}{dt}&amp; = &amp;\\gamma I(t) - \\mu R(t), \\end{eqnarray*}\\] where \\(\\beta\\) is the effective contact rate, and \\[ \\beta \\propto \\left(\\frac{\\mathrm{infection}}{\\mathrm{contact}} \\right) \\times \\left(\\frac{\\mathrm{contact}}{\\mathrm{time}}\\right), \\] i.e., (probability of transmission given a contact between a susceptible and an infectious individual) \\(\\times\\) (average rate of contact between susceptible and infected individuals). \\(\\mu\\) is the birth/death rate; \\(\\gamma\\) is the removal rate, and \\(\\gamma^{-1}\\) is the average infectious period. At the early stage of an SIR outbreak, when \\(S(t) \\approx N\\), the number of infected individuals \\(I(t)\\) is approximated by \\[ I(t) \\approx I_0 \\exp \\{(\\beta - \\gamma - \\mu) t\\} = I_0 \\exp \\{(R_0 - 1)(\\gamma + \\mu)t\\}, \\] where \\(R_0 = \\beta/(\\gamma + \\mu)\\) is the basic reproduction number, \\(I_0\\) is the number of infectious people at time \\(0\\), \\(\\gamma^{-1}\\) is the infectious period and \\(\\mu^{-1}\\) is the host lifespan. An epidemic occurs if the number of infected individuals increases, \\[ \\beta I \\frac{S}{N} - (\\mu + \\gamma)I&gt;0 \\Rightarrow \\beta - (\\mu + \\gamma)&gt;0 \\Rightarrow R_0&gt;1. \\] 9.3 SIR parameter estimation Feature match At the early stage of an SIR outbreak, when \\(S(t) \\approx N\\), the number of infected individuals \\(I(t)\\) is approximated by \\[ I(t) \\approx I_0 \\exp \\{(\\beta - \\gamma - \\mu) t\\} = I_0 \\exp \\{(R_0 - 1)(\\gamma + \\mu)t\\}, \\] where \\(R_0\\) is the basic reproduction number, \\(I_0\\) is the number of infectious people at time \\(0\\), \\(\\gamma^{-1}\\) is the infectious period and \\(\\mu^{-1}\\) is the host lifespan. Taking logs of both sides, we get \\[ \\log(I_t)\\approx \\log(I_0)+(R_0-1)(\\gamma+\\mu)t, \\] which implies that we can obtain a quick estimate of \\(R_0\\) by a simple linear regression. Least squares The method of least squares gives us a way to quantify the discrepancy between the data and a model’s predictions. We can then search over all possible values of a model’s parameters to find the parameters that minimize this discrepancy. Let \\(\\mu = 0\\), for a given \\((\\beta, \\gamma)\\), we denote \\(I(t;\\beta, \\gamma) = I(t)\\) and \\[ (\\widehat{\\beta}, \\widehat{\\gamma}) = \\arg\\min_{(\\beta, \\gamma)}\\sum_{j = 1}^{n} \\{I(t_j;\\beta, \\gamma) - Y_j\\}^2, \\] where \\(Y_j\\) is the number of observed infected people at time \\(t_j\\). Maximum likelihood Let \\(Y_j\\) be the observed number of infected case. We can assume \\[ Y_j \\sim \\textrm{Poisson}\\{pI(t_j;\\beta, \\gamma, \\mu)\\}, \\] where the parameter \\(p\\) reflects a combination of sampling efficiency and the detectability of infections. 9.4 Implementation of parameter estimation in R Example code for implementing SIR to model the outbreak of measles in Niamey can be found in https://kingaa.github.io/short-course/parest/odes.html. We mainly show how to use R to find the parameters in SIR model by minimizing the sum of squared errors. Here is the data of the outbreak of measles in Niamey: http://kingaa.github.io/clim-dis/parest/niamey.csv. 9.4.1 Example 1. Outbreak of measles in Niamey library(dplyr) library(plotly) library(slid) niamey &lt;- read.csv(&quot;http://kingaa.github.io/clim-dis/parest/niamey.csv&quot;) head(niamey,5) ## biweek community measles ## 1 1 A 22 ## 2 2 A 27 ## 3 3 A 64 ## 4 4 A 84 ## 5 5 A 116 plot_ly(data = niamey) %&gt;% add_trace(x = ~biweek, y = ~measles, color = ~community, type = &#39;scatter&#39;, mode = &#39;lines+markers&#39;) 9.4.1.1 Least squares method The R package “pomp” provides facilities for dealing with the important special case of deterministic dynamics. The “pomp” package was developed for data analysis with partially observed Markov process (POMP) models; see details from https://cran.r-project.org/web/packages/pomp/index.html. library(pomp) niameyA &lt;- pomp( data = subset(niamey, community == &quot;A&quot;, select = - community), times = &quot;biweek&quot;, t0 = 0, skeleton = vectorfield( Csnippet(&quot; DS = - Beta*S*I/N; DI = Beta*S*I/N - gamma*I; DR = gamma*I;&quot;)), rinit = Csnippet(&quot; S = S_0; I = I_0; R = N - S_0 - I_0;&quot;), statenames = c(&quot;S&quot;, &quot;I&quot;, &quot;R&quot;), paramnames = c(&quot;Beta&quot;, &quot;gamma&quot;, &quot;N&quot;, &quot;S_0&quot;, &quot;I_0&quot;)) In the above pomp() function, Csnippet is for some C code snippets used to accelerate cumputations, and another two important arguments are: times: The times corresponding to the observations. times must be a strictly increasing numeric vector. If data is a data-frame, times should be the name of the column of observation times. t0: The zero-time. This must be no later than the time of the first observation, times[1]. Grid search method If we assume all the other parameters are known, one simple option to find optimal \\(\\beta\\) would be grid searching a value that minimizes the sum of squared error (SSE), which is the difference between the fitted infection curve with given \\(\\beta\\) compared to the true observations. For measles, the exposed infectious period is about two weeks, and persons who have had measles in the past have lifelong immunity. So we can safely assume that \\(\\gamma = 1\\). In addition, we assume that the initial numbers of susceptibles (\\(S(0)\\)), infectives \\(I(0)\\), and population (\\(N\\)) are 10000, 10, and 50000, respectively. Figure 9.6: The sum of squared errors function of beta. Next, we plugin the estimated \\(\\hat{\\beta}\\) to the process and visualize the fitted curve for \\(I(t)\\) compared to the observed path. ## S I R biweek .id ## 1 9978.113 18.19247 40003.69 1 1 ## 2 9938.514 32.93868 40028.55 2 1 ## 3 9867.521 59.12619 40073.35 3 1 ## 4 9742.315 104.52050 40153.16 4 1 ## 5 9527.706 179.91163 40292.38 5 1 ## 6 9177.101 296.18795 40526.71 6 1 ## 7 8646.275 454.62209 40899.10 7 1 ## 8 7926.435 631.17992 41442.38 8 1 ## 9 7078.425 771.98885 42149.59 9 1 ## 10 6221.882 822.41290 42955.70 10 1 ## 11 5471.119 769.49342 43759.39 11 1 ## 12 4882.162 646.60707 44471.23 12 1 ## 13 4453.766 501.01489 45045.22 13 1 ## 14 4156.351 366.47666 45477.17 14 1 ## 15 3955.396 257.70125 45786.90 15 1 ## 16 3821.694 176.48439 46001.82 16 1 Figure 9.7: The observed and estimated measles count based on the grid search. Optimization algorithm However, this grid-search approach won’t perform well when we have more than one parameter. For example, we would like to assume that \\(N = 50000\\), \\(S_0 = 10000\\), but \\(\\beta\\), \\(\\gamma\\) and \\(I_0\\) are all unknown, and we would like to estimate them simultaneously. In that case, we can use the optim function instead of the grid search method. ## $par ## [1] 9.892048 1.150991 1.042747 ## ## $value ## [1] 164836.4 ## ## $counts ## function gradient ## 253 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Figure 9.8: The observed and estimated measles count based on ptimization algorithm. 9.4.1.2 Maximum likelihood approach Another way to fit the model is using maximum likelihood approach. Note that the model can be simplified a little bit by defining \\(b = \\beta/N\\). We assume \\(S_0 = 10000\\), \\(I_0 = 10\\), \\(\\gamma = 1\\), and \\(p=0.2\\). Figure 9.9: The log likelihood function of b. Next, we plugin the maximum likelihood estimator \\(\\hat{b}\\) to the process and visualize the fitted curve for \\(I(t)\\) compared to the observed path. Figure 9.10: The observed and estimated measles count based on maximum likelihood estimator. Besides a gaussian distribution, it is also possible to choose other random compenent, for example, a poisson distribution. 9.4.2 Example 2. COVID-19 data In the COVID-19 case, the situation becomes slightly more complicated due to the unsatisfying data quality in terms of active cases and recovered cases. Therefore, we modify the algorithm to find the parameters that minimize the distance between the curve of fitted active cases (\\(I\\)) plus removed compartment \\(R\\) and the reported cumulative positive COVID-19 cases time series. If we use the data observed since the beginning of the pandemic, it is reasonable to set the initial value \\(I_{0}\\) to be 1, and focusing on the estimation of other parameters. However, if we choose a different training period, for example, the most recent 60 days, the initial value of active cases \\(I_{0}\\) may be unobserved or calculated inaccurately. Therefore, the initial value can also be treated as a parameter and estimated based on collected data. In this case, an optimizing function such as optim() might be preferred than the grid search. Here is an example of implementing the SIR model to fit the spread of COVID-19 in Orange County, California. data(&quot;I.county&quot;) data(&quot;pop.county&quot;) # the row storing the data for Orange County, CA i &lt;- which(I.county$County == &#39;Orange&#39; &amp; I.county$State == &#39;California&#39;) N0 &lt;- pop.county %&gt;% filter(County == &#39;Orange County&#39; &amp; State == &#39;California&#39;) %&gt;% pull(population) est.h &lt;- 60 pred.h &lt;- 12 date.start &lt;- as.Date(&#39;2020-10-01&#39;) # training period dates.train &lt;- date.start + 0:(est.h - 1) # prediction period dates.test &lt;- date.start + est.h + 0:(pred.h - 1) dates.all &lt;- date.start + 0:(est.h + pred.h - 1) P.train &lt;- data.frame(Date = 0:(est.h - 1), P_cases = as.numeric(I.county[i, paste0(&#39;X&#39;, as.character(dates.train, format = &#39;%Y.%m.%d&#39;))])) P.all &lt;- data.frame(Date = 0:(est.h + pred.h - 1), P_cases = as.numeric(I.county[i, paste0(&#39;X&#39;, as.character(dates.all, format = &#39;%Y.%m.%d&#39;))])) Now we define a pomp object encoding the model and the data and a loss function SSE that evaluate the loss function value based on the simulated trajectories with given parameters. process.tmp &lt;- pomp( data = P.train, times = &quot;Date&quot;, t0 = 0, skeleton = vectorfield( Csnippet(&quot; DS = - Beta*S*I/N; DI = Beta*S*I/N - gamma*I; DR = gamma*I;&quot;)), rinit = Csnippet(&quot; S = S_0; I = I_0; R = N - S_0 - I_0;&quot;), statenames = c(&quot;S&quot;, &quot;I&quot;, &quot;R&quot;), paramnames = c(&quot;Beta&quot;, &quot;gamma&quot;, &quot;N&quot;, &quot;S_0&quot;, &quot;I_0&quot;)) sse &lt;- function (params) { x &lt;- trajectory(process.tmp, params = params) discrep &lt;- x[&quot;I&quot;, , ] + x[&quot;R&quot;, , ] - obs(process.tmp) sum(discrep^2) } Next, we can find the parameters that minimize the SSE using optim function. # set initial value of the process S0 &lt;- N0 - P.train$P_cases[1] # assume that the removal rate is known R.rate &lt;- 0.05 f1 &lt;- function (par) { params &lt;- c(Beta = par[2], gamma = R.rate, N = N0, S_0 = S0, I_0 = par[1]) sse(params) } # initial value for parameters beta0 &lt;- 0.07 I0 &lt;- 1000 # we estimate I0 and beta0 fit1 &lt;- optim(fn = f1, par = c(I0, beta0)) I0.hat1 &lt;- fit1$par[1] beta.hat1 &lt;- fit1$par[2] Figure 9.11 shows the the reported, estimated and predicted reported cumulative number of positive cases of COVID-19 in Orange County, California. Figure 9.11: The reported, estimated and predicted reported cumulative number of positive cases of COVID-19. 9.5 Basic Reproduction Number A key concept is the basic reproduction number (\\(R_0\\)), which is defined as the expected number of secondary cases produced by a single (typical) infection in a completely susceptible population. It is important to note that \\(R_0\\) is a dimensionless number and not a rate, and \\[ R_0 \\propto \\left(\\frac{\\mathrm{infection}}{\\mathrm{contact}} \\right) \\times \\left(\\frac{\\mathrm{contact}}{\\mathrm{time}}\\right) \\times \\left(\\frac{\\mathrm{time}}{\\mathrm{infection}}\\right). \\] If \\(R_0 &lt; 1\\) the disease dies out, while if \\(R_0 &gt; 1\\) the disease becomes endemic. For the SIR model, when \\(S(t) \\approx N\\), \\(R_0 = \\frac{\\beta}{\\gamma + \\mu}\\). For the SEIR model, when \\(S(t) \\approx N\\), \\(R_0 = \\frac{\\sigma}{\\sigma + \\mu} \\times \\frac{\\beta}{\\gamma + \\mu}\\). If \\(R_0 &lt; 1\\), each existing infection causes less than one new infection. In this case, the disease will decline and eventually die out. If \\(R_0 = 1\\), each existing infection causes one new infection. The disease will stay alive and stable, but there won’t be an outbreak or an epidemic. If \\(R_0 &gt; 1\\), each existing infection causes more than one new infection. The disease will be transmitted between people, and there may be an outbreak or epidemic. COVID-19 \\(R_0\\) The \\(R_0\\) for SAS-Cov-2 is a median of 5.7, according to a study published online in Emerging Infectious Diseases. That’s about double an earlier R0 estimate of 2.2 to 2.7 Here, 5.7 means that one person with COVID-19can potentially transmit the coronavirus to 5 to 6 people, rather than the 2 to 3 researchers originally thought. With an \\(R_0\\) of 5.7, at least 82% of the population needs to be immune to COVID-19to stop its transmission through vaccination and herd immunity. 9.5.1 SEIR Model Consider the SIR model in a population of size \\(N\\), and note that \\(N = S(t) + I(t) + R(t)\\). \\[\\begin{eqnarray*} \\frac{dS(t)}{dt}&amp; = &amp;\\mu\\{N - S(t)\\} - \\beta I(t)\\frac{S(t)}{N} + \\omega R, \\\\ \\frac{dE(t)}{dt}&amp; = &amp;\\beta I(t)\\frac{S(t)}{N} - (\\mu + \\sigma)E(t), \\\\ \\frac{dI(t)}{dt}&amp; = &amp;\\sigma E(t) - (\\mu + \\gamma)I(t), \\\\ \\frac{dR(t)}{dt}&amp; = &amp;\\gamma I(t) - (\\mu + \\omega) R(t), \\end{eqnarray*}\\] \\(\\omega^{-1}\\) is the average duration of immunity; \\(\\sigma^{-1}\\) is the average latent period. 9.6 Herd Immunity In order to prevent a disease from becoming endemic it is necessary to reduce the basic reproduction number \\(R_0\\) below one. This may sometimes be achieved by immunization. If a fraction \\(p\\) of the newborn members per unit time of the population is successfully immunized, the effect is to replace \\(N\\) by \\(N(1 - p)\\), and thus to reduce the basic reproduction number to \\(R_0(1 - p)\\). The requirement \\(R_0(1 - p) &lt; 1\\) gives \\(1 - p &lt; 1/R_0\\), or \\[ p &gt; 1 - \\frac{1}{R_0}. \\] A population is said to have herd immunity if a large enough fraction has been immunized to assure that the disease cannot become endemic. The only disease for which this has actually been achieved worldwide is smallpox for which \\(R_0\\) is approximately 5, so that 80% immunization does provide herd immunity, and rinderpest, a cattle disease. For measles, epidemiological data in the United States indicate that \\(R_0\\) for rural populations ranges from 5.4 to 6.3, requiring vaccination of 81.5% to 84.1% of the population. In urban areas \\(R_0\\) ranges from 8.3 to 13.0, requiring vaccination of 88.0% to 92.3% percent of the population. 9.7 Exercises We have estimated the parameters by minimizing the SSE between model-predicted number of cases and observed data \\((\\text{prediction} - \\text{data})^2\\). What would happen if we would like to minimize the squared error on the log scale i.e., \\((\\log(\\text{prediction}) - \\log(\\text{data}))^2\\)? What would happen if we would like minimize the square-root scale, i.e., \\((\\sqrt{\\text{prediction}} - \\sqrt{\\text{data}})^2\\)? Try to fit the model with different definitions of loss function using the niamey data example. What’s the “correct” scale to choose? Change the optimization algorithm used by optim via the method argument. Investigate the effect on your parameter estimates. Try to use other optimizers, such as nlm, nlminb, constrOptim, or the nloptr package, etc. Suppose, alternatively, that the errors are log-normal with constant variance. Under what definition of SSE will least-squares and maximum likelihood give the same parameter estimates? Simulate and visualize the dynamics using SIR model with births and deaths. Consider time=seq(0,20,by=1/52) in years, N=100000, t_0 = 0, S_0=100000/12, I_0=100 and two sets of parameters: (a) \\(\\mu = 1/50\\), \\(\\gamma = 365/13\\), \\(\\beta = 400\\) and (b) \\(\\mu = 1/50\\), \\(\\gamma = 365/5\\), \\(\\beta = 1000\\). Under the assumptions of this model, the average host lifespan is \\(1/\\mu\\). Explore how host lifespan affects the dynamics by integrating the differential equations for lifespans of 20 and 200 years. Consider time=seq(0, 20, by=1/52), N=100000, t_0 = 0, S_0=100000/12, I_0=100 and parameters: \\(\\gamma = 365/13\\), \\(\\beta = 400\\). 6: Compare the dynamics of SIR and SEIR models for the parameters \\(\\mu = 1/50\\), \\(\\gamma = 365/13\\), \\(\\beta = 400\\) and assuming that, for example, \\(E(0)=100\\), the latent period has duration 8 days, and immunity lasts for 10 years in the SEIR model. "],["timeseries.html", "Chapter 10 Time Series Analysis of Infectious Disease Data 10.1 Datasets and R Packages 10.2 An Introduction to Time Series Analysis 10.3 Time Series Decomposition 10.4 Simple Time Series Forecasting Approaches 10.5 Residual Diagnostics and Accuracy Evaluation 10.6 ARIMA Models 10.7 Model Comparison 10.8 Ensuring Forecasts Stay Within Limits 10.9 Prediction and Prediction Intervals for Aggregates 10.10 Exercises", " Chapter 10 Time Series Analysis of Infectious Disease Data Several quantities are of interest in epidemic forecasting, such as the timing of and incidence in the peak week, cumulative incidence, and weekly incidence. The policy/decision-makers are also interested in evaluating outbreak size and duration, and employing the epidemic curve to identify the mode of transmission of the disease and measure its prevalence of the disease. Forecasting goals can also be classified as long-term or short-term forecasts. Long-term disease forecasts can predict COVID-19 peak or severity, while short-term forecasts can be used to guide resource allocation in the short term by local agencies or to anticipate the case burden by hospitals in the coming week; see Altieri et al. (2020). The projection can be made at different resolution levels, for example, national, regional or local. National-level or state-level long-term forecasts are of interest to policymakers regulating intervention strategies and deciding how much funding to allocate for resources. Prediction models with a finer resolution are needed to assess the local risk of COVID-19. Knowing more about the vulnerable communities and the reasons for those communities that are more likely to be infected are crucial for the policy and decision-makers to assist in prevention efforts (Altieri et al. 2020). We provide a predictive analysis of the spread of COVID-19, using the dataset made publicly available online by the Johns Hopkins University. Our main objective is to provide future predictions of the number of infected people for different countries. We use two well-known methods for prediction: time series analysis and neural network. 10.1 Datasets and R Packages Data The dataset from JHU contains the cumulative number of cases reported daily for different countries. We base our analysis on the state-level time series. For each state, we consider the time-series \\(y_n\\) starting from the day when the first case was reported. Given the current day index \\(n\\), we predict the number of cases for the day \\(n + h\\) by considering as input the number of cases reported for the past \\(w\\) days, that is, for the days \\(n-w+1\\) to \\(n\\). #install_github(&#39;covid19-dashboard-us/slid&#39;) library(slid) data(state.long) R Package: fable We will use the R packages fable, tsibble and dplyr, which together offer various functions for computing and visualizing basic time series components. library(dplyr) library(fable) library(tsibble) The R package “fable” provides a collection of commonly used univariate and multivariate time series forecasting models including exponential smoothing via state space models and autoregressive integrated moving average (ARIMA) modeling. The “fable” package provides the tools to evaluate, visualize, and combine models in a workflow consistent with the “tidyverse.” The Forecasting: Principles and Practices online textbook provides an introduction to time series forecasting using fable. 10.2 An Introduction to Time Series Analysis Time series is a set of observations recorded sequentially. The first thing to do in any time series analysis task is to plot the data. Graphs enable many features of the data to be visualized, including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data must then be incorporated, as much as possible, into the forecasting methods to be used. Just as the type of data determines what forecasting method to use, it also determines what graphs are appropriate. But before we produce graphs, we need to set up our time series in R. 10.2.1 “tsibble” objects A time series can be thought of as a list of numbers, along with some information about what times those numbers were recorded. This information can be stored as a “tsibble” object in R. Beyond the tibble-like representation, key comprised of single or multiple variables is introduced to uniquely identify observational units over time (index). Each observation should be uniquely identified by index and key in a valid “tsibble” object. Take the state.long data for example, state.long ## # A tibble: 15,925 x 7 ## State Region Division pop DATE Infected Death ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 Alaba… South East Sout… 4.89e6 2020-12-11 288775 4086 ## 2 Alaba… South East Sout… 4.89e6 2020-12-10 284922 4034 ## 3 Alaba… South East Sout… 4.89e6 2020-12-09 280187 3985 ## 4 Alaba… South East Sout… 4.89e6 2020-12-08 276665 3940 ## 5 Alaba… South East Sout… 4.89e6 2020-12-07 272228 3891 ## 6 Alaba… South East Sout… 4.89e6 2020-12-06 269877 3888 ## 7 Alaba… South East Sout… 4.89e6 2020-12-05 267589 3876 ## 8 Alaba… South East Sout… 4.89e6 2020-12-04 264199 3831 ## 9 Alaba… South East Sout… 4.89e6 2020-12-03 260359 3776 ## 10 Alaba… South East Sout… 4.89e6 2020-12-02 256828 3711 ## # … with 15,915 more rows We can turn this into a “tsibble” object using the as_tsibble() function: state.ts &lt;- as_tsibble(state.long, key = State) ## Using `DATE` as index variable. state.ts ## # A tsibble: 15,925 x 7 [1D] ## # Key: State [49] ## State Region Division pop DATE Infected Death ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; ## 1 Alaba… South East Sout… 4.89e6 2020-01-22 0 0 ## 2 Alaba… South East Sout… 4.89e6 2020-01-23 0 0 ## 3 Alaba… South East Sout… 4.89e6 2020-01-24 0 0 ## 4 Alaba… South East Sout… 4.89e6 2020-01-25 0 0 ## 5 Alaba… South East Sout… 4.89e6 2020-01-26 0 0 ## 6 Alaba… South East Sout… 4.89e6 2020-01-27 0 0 ## 7 Alaba… South East Sout… 4.89e6 2020-01-28 0 0 ## 8 Alaba… South East Sout… 4.89e6 2020-01-29 0 0 ## 9 Alaba… South East Sout… 4.89e6 2020-01-30 0 0 ## 10 Alaba… South East Sout… 4.89e6 2020-01-31 0 0 ## # … with 15,915 more rows The summary above shows that this is a “tsibble” object, which contains 15,925 rows and 7 columns. The object is uniquely identified by the key: State. It informs us that there are separate time series in the “tsibble” for each of the 49 states in the US. A “tsibble” allows multiple time series to be stored in a single object. The state.long dataset contains the infected count, death count for each mainland state in the US and District of Columbia. 10.2.2 Working with “tsibble” objects Several functions in the “dplyr” package can be used to work with “tsibble” objects, including mutate() and select(). To illustrate these further and some other useful functions, we will use the state.ts tsibble created in the above. state.ts &lt;- as_tsibble(state.long, key = State) %&gt;% group_by(State) %&gt;% mutate(Infected = Infected/1000) %&gt;% mutate(YDA_Infected = lag(Infected, order_by = DATE)) %&gt;% mutate(YDA_Death = lag(Death, order_by = DATE)) %&gt;% mutate(Y.Infected = Infected - YDA_Infected) %&gt;% mutate(Y.Death = Death - YDA_Death) %&gt;% dplyr::filter(!is.na(Y.Infected)) %&gt;% dplyr::filter(!is.na(Y.Death)) %&gt;% dplyr::select(-c(YDA_Infected, YDA_Death)) ## Using `DATE` as index variable. state.ts ## # A tsibble: 15,876 x 9 [1D] ## # Key: State [49] ## # Groups: State [49] ## State Region Division pop DATE Infected Death ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Alab… South East So… 4.89e6 2020-01-23 0 0 ## 2 Alab… South East So… 4.89e6 2020-01-24 0 0 ## 3 Alab… South East So… 4.89e6 2020-01-25 0 0 ## 4 Alab… South East So… 4.89e6 2020-01-26 0 0 ## 5 Alab… South East So… 4.89e6 2020-01-27 0 0 ## 6 Alab… South East So… 4.89e6 2020-01-28 0 0 ## 7 Alab… South East So… 4.89e6 2020-01-29 0 0 ## 8 Alab… South East So… 4.89e6 2020-01-30 0 0 ## 9 Alab… South East So… 4.89e6 2020-01-31 0 0 ## 10 Alab… South East So… 4.89e6 2020-02-01 0 0 ## # … with 15,866 more rows, and 2 more variables: ## # Y.Infected &lt;dbl&gt;, Y.Death &lt;int&gt; This tsibble use the mutate() to create two extra variables: Y.Infected and Y.Death, which are the daily new infected count and the daily new death count for each state. We can use the filter() function to extract the data for the state of Florida, for example: Florida.ts &lt;- state.ts %&gt;% dplyr::filter(State == &quot;Florida&quot;) Next, we can simplify the resulting object by selecting five variables we will need in subsequent analysis. Florida.ts &lt;- Florida.ts %&gt;% dplyr::select(Infected, Death, Y.Infected, Y.Death) ## Adding missing grouping variables: `State` Florida.ts ## # A tsibble: 324 x 6 [1D] ## # Key: State [1] ## # Groups: State [1] ## State Infected Death Y.Infected Y.Death DATE ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; ## 1 Florida 0 0 0 0 2020-01-23 ## 2 Florida 0 0 0 0 2020-01-24 ## 3 Florida 0 0 0 0 2020-01-25 ## 4 Florida 0 0 0 0 2020-01-26 ## 5 Florida 0 0 0 0 2020-01-27 ## 6 Florida 0 0 0 0 2020-01-28 ## 7 Florida 0 0 0 0 2020-01-29 ## 8 Florida 0 0 0 0 2020-01-30 ## 9 Florida 0 0 0 0 2020-01-31 ## 10 Florida 0 0 0 0 2020-02-01 ## # … with 314 more rows Note that the index variable DATE would be returned even if it was not explicitly selected as it is required for a tsibble. 10.2.3 Drawing time series plots To further examine the data, we now use the autoplot() to draw some time series plot; see Figure 10.1. Florida.ts %&gt;% autoplot(Y.Death) Figure 10.1: The time series plot of the daily new death count in Florida using the autoplot() function. We can also use the “ggplot2” to draw some time series plot. Figure 10.2 shows the time series plot with blue line. library(ggplot2) ggplot(Florida.ts, aes(x = DATE, y = Y.Death)) + geom_line(color = &quot;blue&quot;) + labs(title = &#39;Florida daily new death count&#39;) Figure 10.2: The time series plot of the daily new death count in Florida using the ggplot() function. We can draw multiple time series on the same plot using “ggplot2.” The time series plot shown in Figure 10.3 illusterate the daily new infected and death counts in Florida. ggplot(Florida.ts, aes(x = DATE)) + geom_line(aes(y = Y.Infected, colour = &quot;Y.Infected (thousand)&quot;)) + geom_line(aes(y = Y.Death, colour = &quot;Y.Death&quot;)) + ylab(&quot;Count&quot;) + xlab(&quot;Day&quot;) + guides(colour=guide_legend(title = &quot;Forecasts&quot;)) + theme(legend.position=&quot;bottom&quot;) Figure 10.3: The time series plot of the daily new infected and death counts in Florida. Figure 10.4 shows the time series plot of the daily new death count for each of the midwest states using the ggplot() function. state.ts %&gt;% dplyr::filter(Region== &quot;Midwest&quot;) %&gt;% ggplot(aes(x = DATE, y = Y.Death, group = State, color = State)) + geom_line() + theme(legend.position=&quot;bottom&quot;) Figure 10.4: The time series plot of the daily new death count for each of the midwest states using the ggplot() function. We can also use the autoplot() function to draw multiple time series on one plot. Figure 10.5 shows the time series plot of the daily new death count for each of the midwest states using the autoplot() function. state.ts %&gt;% dplyr::filter(Region== &quot;Midwest&quot;) %&gt;% autoplot(Y.Death) + ylab(&quot;Deaths&quot;) + xlab(&quot;Day&quot;) + ggtitle(&quot;Daily new death count for each of the midwest&quot;) ## `mutate_if()` ignored the following grouping variables: ## Column `State` Figure 10.5: The time series plot of the daily new death count for each of the midwest states using the autoplot() function. After making the time series plots, we look for Trend: upward or downward movement that might be extrapolated into future; it does not have to be linear. Periodicity: repetition in regular pattern (usually peaks and troughs; Seasonality: periodic behavior of known period (i.e., 12 months for monthly data); Heteroskedasticity: changing variance, particularly with changing level; Dependence: positive (successive observations are similar) or negative (successive observations are dissimilar); Missing data, structural breaks, outliers. Remark: Some readers confuse cyclic behavior with seasonal behavior, but they are really quite different. If the fluctuations are not of a fixed frequency, then they are cyclic; if the frequency is unchanging and associated with some aspect of the calendar, then the pattern is seasonal. Time series seasonal plot We can produce a time series seasonal plot using gg_season() function in the “feast” R package, which provides a collection of features, decomposition methods, statistical summaries and graphics functions for analyzing tidy time series data. A seasonal plot is similar to a regular time series plot, except the x-axis shows data from within each season. This plot type allows the underlying seasonal pattern to be seen more clearly, and is especially useful in identifying the time in which the pattern changes. Figure 10.6 shows the time series weekly plot of the daily new death count. library(feasts) Florida.ts %&gt;% gg_season(Y.Death, period = &quot;week&quot;) Figure 10.6: The time series weekly plot of the daily new death count. A lag plot We can draw a lag plot to show the time series against lags of itself using the gg_lag() in the R library feasts. library(feasts) Florida.ts %&gt;% gg_lag(y = Y.Death, geom = &quot;point&quot;) Figure 10.7: The lag plot of the daily new death count for Florida. It is often colored the seasonal period (here weekly cycle) to identify how each season (each date in a week) correlates with others. From Figure 10.7, one sees that the relationship is strongly positive at lag 7, reflecting the strong weekly cycle in the data. 10.2.4 Objectives of Time Series Analysis Provide an interpretable model of data often involves multivariate series allows testing of scientific hypotheses but, not always a major emphasis in time series analysis Predict future values of series very common application of time series analysis predictive models often do not try to explain Provide a compact description of data good predictive model can be used for data compression used extensively in telecommunications Modeling Strategy Take a probabilistic approach observations \\(=\\) realizations of random variables in much of statistics, random variables are assumed to be indepedent Difficulties in time series: random variables are typically not identically distributed different means due to trend, seasonality may be different variances as well random variables typically not independent dependence may be positive or negative Try to make things easier eliminate heterostedasticity via transformation (e.g. log) eliminate trend and seasonality model remainder as dependent but identically distributed 10.2.5 Stationarity Let \\(\\{X_t\\}\\) be a time series with \\(E(X_t^2)&lt;\\infty\\). The mean function of \\(\\{X_t\\}\\) is \\[ \\mu_X(t)=\\mathrm{E}(X_t). \\] The covariance function of \\(\\{X_t\\}\\) is \\[ \\gamma_X(s,t)=\\mathrm{Cov}(X_t,X_s) = \\mathrm{E}[(X_s-\\mu_X(s))(X_t-\\mu_X(t))] \\] for all integers \\(s\\) and \\(t\\). Modeling Dependence Hard to model dependence if dependence changes with time Easiest to model dependence in stationary case Roughly speaking, stationary means probabilistic properties of series do not change with time. There are two versions of interest. Strict stationarity: joint probability distributions do not change with time. A time series \\(\\{X_t\\}\\) is strictly stationary if for any positive integer \\(k\\) and integers \\(t_1, \\ldots, t_k\\) and \\(h\\), \\[ (X_{t_1},X_{t_2},\\ldots,X_{t_k}) \\overset{d}{=} (X_{t_1+h},X_{t_2+h},\\ldots,X_{t_k+h}) \\] where ``\\(\\overset{d}{=}\\)’’ denotes equality in probability distribution. Remark: covariances make sense only if variances exist If \\(\\{X_t\\}\\) is IID, then \\(\\{X_t\\}\\) is strictly stationary Strict stationarity is a very strong modeling assumption hard to verify in practice often stronger than necessary for useful results \\(\\Rightarrow\\) introduce next so-called weak stationarity Weak stationarity: 1st and 2nd order moment properties (i.e., mean and covariance structures) do not change over time A time series \\(\\{X_t\\}\\) is weakly stationary if for all integers \\(t\\) and \\(h\\): \\(\\mathrm{Var}(X_t)&lt;\\infty\\) \\(E[X_t]\\) does not depend on \\(t\\) \\(\\mathrm{Cov}(X_t,X_{t+h})\\) does not depend on \\(t\\) Notes implies all means, variances, and covariances exist implies means are constant (rules out trend, seasonality) with \\(h = 0\\), (c) implies variances are constant (rules out heteroskedasticity) Weakly stationary also known as covariance stationary, second order stationary, or just stationary * 1st and 2nd-order moments do not change with time * much weaker than strict stationarity 10.2.6 Autocorrelation Autocovariance Function (ACVF) For weakly stationary time series, \\(\\mathrm{Cov}(X_{t},X_{t+h})=\\gamma(h)\\), a function of \\(h\\) only. Autocorrelation Function (ACF) The autocorrelation function (ACF) of \\(\\{X_t\\}\\) is defined by \\[ \\rho(h)=\\frac{\\gamma(h)}{\\gamma(0)}=\\frac{\\mathrm{Cov}(X_{t},X_{t+h})}{\\sqrt{\\mathrm{Var}(X_{t})\\mathrm{Var}(X_{t+h})}} \\] Sample ACVF Let \\(\\{X_t\\}_{t=1}^n\\) be a time series and \\(\\bar{X}_{n}=\\frac{1}{n}\\sum_{t=1}^{n}X_t\\) be its sample mean. The sample ACVF (based on \\(\\{X_t\\}_{t=1}^n\\)): \\[ \\hat{\\gamma}(h)\\equiv \\frac{1}{n}\\sum_{t=1}^{n-|h|} (X_{t}-\\bar{X}_{n})(X_{t+|h|}-\\bar{X}_{n}), ~|h|&lt;n \\] which estimates \\(\\gamma(h)=E[(X_t-EX_t)(X_{t+|h|}-EX_t)]\\). Sample ACF The sample ACF based on \\(\\{X_t\\}_{t=1}^n\\) is defined as \\[ \\hat{\\rho}(h)\\equiv \\hat{\\gamma}(h)/\\hat{\\gamma}(0), ~|h|&lt;n \\] which estimates \\(\\rho(h)=\\gamma(h)/\\gamma(0)\\), \\(|\\hat{\\rho}(h)|\\leq 1\\), \\(\\hat{\\rho}(0)=1\\). The autocorrelation coefficients are plotted to show the autocorrelation function or ACF. Figure 10.8 shows the ACF plot of the daily new death count in Florida. Florida.ts %&gt;% ACF(Y.Death) %&gt;% autoplot() Figure 10.8: The ACF plot of the daily new death count in Florida. The dashed blue lines in this graph indicate whether the correlations are significantly different from zero, and \\(\\hat{\\rho}(4)\\) is higher than for the other lags. This is due to the weekly pattern in the data: the peaks tend to be four quarters apart, and the troughs tend to be four quarters apart. Remark: When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of the time series with a trend tends to have positive values that slowly decrease as the lags increase. When data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags. Florida.ts %&gt;% gg_tsdisplay(difference(Y.Death, 7), plot_type=&#39;partial&#39;, lag=36) + labs(y=&quot;Lag 7 differenced&quot;) Figure 10.9: Time plot, ACF plot and PACF plot of lag-7 differenced data. 10.3 Time Series Decomposition Time series data can exhibit various patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category. In the literature, a time series is usually decomposed into three components: a trend component, a seasonal component, and a remainder component. In the following, we consider the following classical decomposition: \\[ X_t=m_t+s_t+Y_t \\] where \\(m_t\\) is the trend at \\(t\\) (non-random often), \\(s_t\\): a function with known period \\(d\\) referred to as seasonality (non-random), and \\(Y_t\\): irregular and random noise that is stationary. Our aims are the following: to estimate and extract \\(m_t\\) and \\(s_t\\) so that the residual \\(Y_t\\) will turn out to be stationary; to find a satisfactory probabilistic model for \\(Y_t\\); use it in conjunction with \\(m_t\\) and \\(s_t\\) for prediction and simulation of \\(\\{X_t\\}\\). 10.3.1 Box-Cox transformations When decomposing a time series, it is sometimes helpful to first transform or adjust the series in order to make the decomposition (and later analysis) as simple as possible. Transformations help to stabilize the variance. So we will begin by discussing transformations and adjustments. Each of these transformations is close to a member of the family of Box-Cox transformations: \\[ W_t =\\left\\{\\begin{array}{ll} \\log(Y_t), &amp; \\lambda=0;\\\\ (Y_t^{\\lambda}-1)/\\lambda, &amp; \\lambda \\neq 0. \\end{array} \\right. \\] \\(\\lambda= 1\\): (No substantive transformation); \\(\\lambda= 1/2\\): (Square root plus linear transformation); \\(\\lambda= 0\\): (Natural logarithm). This is a simple way to force forecasts to be positive; \\(\\lambda= -1\\): (Inverse plus 1); \\(Y_t^{\\lambda}=1\\) for \\(\\lambda\\) close to zero behaves like logs; If some \\(Y_t = 0\\), then must have \\(\\lambda &gt; 0\\); If some \\(Y_t &lt; 0\\), no power transformation is possible unless all \\(Y_t\\) adjusted by adding a constant to all values; Choose a simple value of \\(\\lambda\\), which makes explanation easier; Results are relatively insensitive to value of \\(\\lambda\\). We must reverse the transformation (or back-transform) to obtain forecasts on the original scale. The reverse Box-Cox transformations are given by \\[ Y_t=\\left\\{\\begin{array}{ll} \\exp(W_t), &amp; \\lambda=0;\\\\ (\\lambda W_t+1)^{1/\\lambda}-1, &amp; \\lambda \\neq 0. \\end{array} \\right. \\] 10.3.2 Methods for estimating the trend Polynomial regression Simplest curve fit or approximation model, where the number of cases is approximated locally with polynomials of degree \\(d\\). \\[ Y_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\cdots + \\beta_d t^d + Z_t, \\] where \\(\\{Z_t\\}\\) are IID gaussian noise. Figure 10.10 of the 14 days ahead forecast of the daily death count for Florida using the linear regression method. death_lmfit &lt;- Florida.ts %&gt;% model(TSLM(Y.Death ~ DATE)) Figure 10.10: Two weeks ahead forecast of the daily death count for Florida using the linear regression method. Figure 10.10 shows the 14 days ahead forecast of the daily death count for Florida using the linear regression method with the seasonal component: season(7). death_lmsfit &lt;- Florida.ts %&gt;% model(TSLM(Y.Death ~ DATE + season(7))) Figure 10.11: Two weeks ahead forecast of the daily death count for Florida using the linear regression method with the seasonal component. Moving average filtering For an integer \\(q\\geq 0\\), and filtering parameters \\(\\{a_{-q},a_{-q+1},\\ldots, a_0,a_1,\\ldots,a_{q}\\}\\), we consider the following moving average filter: \\[ \\hat{m}_{t}=\\sum_{k=-q}^{q}a_{k}X_{t-k}, ~ q+1 \\leq t \\leq n-q. \\] For example, if \\(a_{k}=1/(1+2q)\\), then \\[ \\hat{m}_{t}=\\frac{1}{1+2q}\\sum_{k=-q}^{q} m_{t-k}+\\frac{1}{1+2q}\\sum_{k=-q}^{q}Y_{t-k} \\] is the two-sided moving average. We call this an \\(m\\)-MA smoothing, meaning a moving average of order \\(m=1+2q\\). This method allows linear trend function \\(m_t=c_0+c_1t\\) to pass without distortion: \\[ \\sum_{k=-q}^{q}a_{k}m_{t-k}=m_t. \\] Remarks: The \\(m\\)-MA can be easily done using slide_dbl() from the “slider” package which applies a function to “sliding” time windows. We can use the mean() function to specify the window size. For small \\(q\\), \\(\\hat{m}_{t}\\) is closer to \\(m_t\\), less bias, more variance; For large \\(q\\), more bias, less variance; This is so called variance-bias trade-off in filtering. For example, we consider \\(q=2\\) and \\(q=7\\) below to have a 5-MA smoothing and a 15-MA smoothing, respectively. Florida.ts.MA5 &lt;- Florida.ts %&gt;% mutate( `5-MA` = slider::slide_dbl(Y.Death, mean, .before = 2, .after = 2, .complete = TRUE) ) Florida.ts.MA15 &lt;- Florida.ts %&gt;% mutate( `15-MA` = slider::slide_dbl(Y.Death, mean, .before = 7, .after = 7, .complete = TRUE) ) To see what the trend-cycle estimate looks like, we plot the above two moving average trends along with the original data in Figure 10.12. Florida.ts %&gt;% autoplot(Y.Death) + autolayer(Florida.ts.MA5, `5-MA`, color = &quot;green&quot;) + autolayer(Florida.ts.MA15, `15-MA`, color = &quot;red&quot;) + labs(y = &quot;Death count&quot;, title = &quot;Florida daily new death count with moving average trend&quot;) + guides(colour = guide_legend(title = &quot;series&quot;)) Figure 10.12: Florida daily new death count with the 5-MA (green) and 15-MA (red) smoothing of the trend. Simple exponential smoothing Consider the following nonseasonal model with trend: \\[ Y_t=m_t+Z_t, \\] where \\(\\mathrm{E}Z_t=0\\). We can estimate \\(m_t\\) using simple exponential smoothing. For any fixed \\(0&lt;\\alpha&lt;1\\), consider \\(\\hat{m}_t\\) defined by the following recursions: \\[\\begin{eqnarray*} \\hat{m}_1 &amp;=&amp; Y_1, \\mathrm{~~and}\\\\ \\hat{m}_t &amp;=&amp; \\alpha Y_t+(1-\\alpha)\\hat{m}_{t-1}\\\\ &amp;=&amp; \\alpha Y_t+(1-\\alpha)\\{\\alpha Y_{t-1}+(1-\\alpha)\\hat{m}_{t-2}\\}\\\\ &amp;=&amp; \\alpha Y_t+(1-\\alpha)\\alpha Y_{t-1}+(1-\\alpha)^2\\alpha X_{t-2}+(1-\\alpha)^3\\hat{m}_{t-3}\\\\ &amp;=&amp; \\ldots \\end{eqnarray*}\\] with exponentially decreasing weights on previous observations: \\(\\alpha(1-\\alpha)^0\\) on \\(Y_t\\), \\(\\alpha(1-\\alpha)^1\\) on \\(Y_{t-1}\\), \\(\\alpha(1-\\alpha)^2\\) on \\(Y_{t-2}\\), \\(\\ldots\\) \\(\\alpha\\rightarrow 1\\), less bias, more variance; \\(\\alpha\\rightarrow 0\\), more bias, less variance. This method is suitable for forecasting data with no clear trend or seasonal pattern. The function ETS function returns forecasts and other information for the exponential smoothing forecasts. There are several versions of the ETS models; see Hyndman and Athanasopoulos (2018b). You can specify the details of the ETS model by choosing different options in the error, trend and season specials. The form of the error term: either additive (“A”) or multiplicative (“M”). If the error is multiplicative, the data must be non-negative. The form of the trend term: either none (“N”), additive (“A”), multiplicative (“M”) or damped variants (“Ad,” “Md”). The form of the seasonal term: either none (“N”), additive (“A”) or multiplicative (“M”) ets_fit &lt;- Florida.ts %&gt;% model(ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;), opt_crit = &quot;mse&quot;)) Figure 10.13: Two weeks ahead forecast of the daily death count for Florida using the simple exponential smoothing method. The ETS() also allows you to extend the simple exponential smoothing to allow the forecasting of data with a trend and seasonality. etss_fit &lt;- Florida.ts %&gt;% model(`ETS` = ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;))) Figure 10.14: Two weeks ahead forecast of the daily death count for Florida using the extended exponential smoothing method with trend and seasonality components. Figure 10.15 display the observed and fitted daily death count using the extended exponential smoothing method with trend and seasonality components. Figure 10.15: Time series plot of the observed and fitted daily death count. 10.3.3 Seasonal component The methods described for the estimation and elimination of trend can be adapted in a natural way to eliminate both trend and seasonality in the general model, specified as follows. Classical decomposition: \\[ X_t=m_t+s_t+Y_t \\] where \\(s_t\\) has cyclic behavior of known \\(d\\) (e.g., \\(d=12\\) months), i.e., there is a perfect repetition in \\(s_t\\): \\(s_{t}=s_{t+d}\\) (e.g., Jan 2010 \\(=\\) Jan 2011 \\(=\\) Jan 2012 \\(=\\) ). We assume that \\(\\sum_{j=0}^{d-1}s_{t-j}=0\\) for model identifiability. Note that if \\(\\sum_{j=0}^{d-1}s_{t-j}=c\\), we can always have \\(\\sum_{j=0}^{d-1}(s_{t-j}-c/d)=0\\). We first need to identify the seasonal components (what is the and/or ): Time series plot Smooth the series ACF plot Once we have identified the seasonal component we can model it: Simple differencing (e.g., \\(X_t-X_{t-12}\\)) Moving average Dummy variables \\[ s_{t}=\\left\\{\\begin{array}{ll} r_1 &amp; t=1,1+d,1+2d,1+3d,\\ldots \\\\ r_2 &amp; t=2,2+d,2+2d,2+3d,\\ldots \\\\ \\vdots &amp; \\\\ r_{d-1} &amp; t=d-1,2d-1,3d-1,\\ldots \\\\ -\\sum_{k=1}^{d-1} r_k &amp; t=d,2d,3d,\\ldots \\end{array} \\right. \\] Regression of \\(X_t\\), dummy variables \\(s_t\\) over \\(t\\) \\[ \\min \\sum_{t=1}^n (X_t-s_t)^2 \\] for \\(r_1,\\ldots,r_{d-1}\\). Harmonic function (series of sin/cos functions) 10.3.4 Trend and seasonal estimation: Step 1. Form preliminary estimate \\(\\hat{m}_t\\) of trend by passing data through filter/smoothing that eliminates \\(s_t\\) as much as possible. Step 2. Subtract trend estimate from data: \\(u_t = x_t - \\hat{m}_t\\) . Step 3. Obtain seasonal pattern estimate \\(\\{\\hat{s}_j : j = 1,\\ldots,d\\}\\). Step 4. Replicate \\(\\{\\hat{s}_j\\}\\) as need be to form estimate \\(\\{\\hat{s}_t\\}\\) of \\(\\{s_t\\}\\). Step 5. Form deseasonalized data: \\(d_t = x_t -\\hat{s}_t\\). Step 6. Use deseasonalized data to get final estimate \\(\\hat{m}_t\\) of trend. 10.3.5 Seasonal and trend decomposition using Loess (STL) STL developed by Cleveland et al. (1990) is a versatile and robust method for decomposing time series. STL is an acronym for “Seasonal and Trend decomposition using Loess,” while Loess is a method for estimating nonlinear relationships. The algorithm updates trend and seasonal components iteratively: The seasonal values are removed, and the remainder smoothed to find the trend. The overall level is removed from the seasonal component and added to the trend component. This process is iterated a few times. The remainder component is the residuals from the seasonal plus trend fit. Below we will demonstrate how to use the STL() to decompose the time series of the daily new death count in Florida. # Time series decomposition dcmp &lt;- Florida.ts %&gt;% model(STL(Y.Death)) components(dcmp) ## # A dable: 324 x 8 [1D] ## # Key: State, .model [1] ## # STL Decomposition: Y.Death = trend + season_week + ## # remainder ## State .model DATE Y.Death trend season_week ## &lt;fct&gt; &lt;chr&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Flor… STL(Y… 2020-01-23 0 -0.0944 0.415 ## 2 Flor… STL(Y… 2020-01-24 0 -0.0748 -0.139 ## 3 Flor… STL(Y… 2020-01-25 0 -0.0553 0.182 ## 4 Flor… STL(Y… 2020-01-26 0 -0.0407 -0.0670 ## 5 Flor… STL(Y… 2020-01-27 0 -0.0262 -0.364 ## 6 Flor… STL(Y… 2020-01-28 0 -0.0162 0.0220 ## 7 Flor… STL(Y… 2020-01-29 0 -0.00616 -0.0895 ## 8 Flor… STL(Y… 2020-01-30 0 -0.00120 0.497 ## 9 Flor… STL(Y… 2020-01-31 0 0.00376 -0.158 ## 10 Flor… STL(Y… 2020-02-01 0 0.00224 0.214 ## # … with 314 more rows, and 2 more variables: ## # remainder &lt;dbl&gt;, season_adjust &lt;dbl&gt; We will decompose the time series of the daily new deaths in Florida as shown in Figure 10.1. Figure 10.16 shows the trend of the time series. Florida.ts %&gt;% autoplot(Y.Death, color = &quot;gray&quot;) + autolayer(components(dcmp), trend, color = &quot;red&quot;) + labs(y = &quot;Death Count&quot;, title = &quot;Daily New Death Count with Trend&quot;) Figure 10.16: Trend of the daily new death count time series in Florida. Next, we can draw the STL decomposition: trend, days of week effect and remainder, using the autoplot() function. components(dcmp) %&gt;% autoplot() The STL() function also allows us to choose the trend window trend(window = ) and the seasonal window season(window = ), which controls how rapidly the trend and seasonal components can change. The smaller the value, the more rapid the changes. The trend window is the number of consecutive observations to be used when estimating the trend-cycle; the season window is the number of consecutive days to estimate each value in the seasonal component. For example, Figure 10.17 shows the trend, seasonality and residuals of the daily new death count time series in Florida based on trend(window = 15) and season(window = 7). Both trend and seasonal windows should be odd numbers. Florida.ts %&gt;% model(STL(Y.Death ~ trend(window = 15) + season(window = 7), robust = TRUE )) %&gt;% components() %&gt;% autoplot() Figure 10.17: The trend, seasonality and residuals of the daily new death count time series in Florida based on trend window = 14 and seasonal window = 7. 10.4 Simple Time Series Forecasting Approaches We use data-driven prediction approaches without considering any other aspect, such as the disease spread mechanism. We describe each approach in detail in the following subsections. If \\(\\{Z_t\\}\\) is a sequence of independent random variables that follow the same normal distribution with zero mean, we call \\(\\{Z_t\\}\\) IID Gaussian noise. 10.4.1 Average method Denote the time series data by \\(\\{Y_t\\}\\), and consider the following model: \\[ Y_t=\\mu+Z_t, \\] where \\(\\{Z_t\\}\\) are IID gaussian noise. The average method assumes that forecasts of all future values are equal to the average (or “mean”) of the historical data. If we let the historical data be denoted by \\(Y_1,\\ldots,Y_n\\), and let \\(\\hat{Y}_{n+h|n}\\) be the estimate of \\(Y_{n+h}\\) based on the historical data, then we can write the forecasts as \\[ \\hat{Y}_{n+h|n}=\\frac{1}{n}\\sum_{t=1}^n Y_t. \\] For a numeric vector or time series of class ts \\(y\\), the function MEAN(y) returns an i.i.d model applied to y. The forecast(h = ) returns the forecasts and prediction intervals for \\(Y_{n+h}\\) via the average method, and \\(h\\) is the number of periods for forecasting. Figure 10.18 shows the two weeks ahead forecast and 95% prediction intervals for the daily death count in Florida based on the average method. Florida.ts %&gt;% model(MEAN(Y.Death))%&gt;% forecast(h = 14) %&gt;% autoplot(Florida.ts, level = 95, title = &quot;Average Method&quot;) + labs(y = &quot;Death count&quot;, title = &quot;Average Method&quot;) Figure 10.18: Two weeks ahead forecast of the daily death count for Florida using the average method. 10.4.2 Random walk forecasts The random walk model assumes that \\[ Y_t=Y_{t-1}+Z_t, \\] where \\(\\{Z_t\\}\\) are IID gaussian noise. The random walk approach simply sets all forecasts to be the value of the last observation. That is, \\[ \\hat{Y}_{n+h|n}=Y_n. \\] The function RW(y) or NAIVE(y) together with forecast(h) provide the random walk forecasts and prediction intervals for \\(Y_{n+h}\\). Florida.ts %&gt;% model(RW(Y.Death))%&gt;% # NAIVE(Y.Death) is an equivalent alternative forecast(h = 14) %&gt;% autoplot(Florida.ts, level = 95) + labs(y = &quot;Death count&quot;, title = &quot;Random Work Method&quot;) Figure 10.19: Two weeks ahead forecast of the daily death count for Florida using the random walk method. 10.4.3 Seasonal random walk forecasts A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same time of the previous period. Formally, the forecast for time \\(n+h\\) is written as \\[ \\hat{Y}_{n+h|n}=Y_{n+h-m(k+1)}, \\] where \\(d=\\) the seasonal period, and \\(k=[(h-1)/d]\\), that is, the integer part of \\((h-1)/d\\). For COVID-19 data, we often observe the seven day cycle; see Wang et al. (2020). Then, \\(d=7\\) and \\(k\\) is the number of complete weeks in the forecast period prior to time \\(n+h\\). The function SNAIVE(y) with forecast(h) provides the seasonal random walk forecasts and prediction intervals for \\(Y_{n+h}\\). Florida.ts %&gt;% model(SNAIVE(Y.Death))%&gt;% forecast(h = 14) %&gt;% autoplot(Florida.ts, level = 95) + labs(y = &quot;Death count&quot;, title = &quot;Seasonal Random Work Method&quot;) Figure 10.20: Two weeks ahead forecast of the daily death count for Florida using the seasonal random walk method. 10.4.4 Random walk with drift method The random walk with drift model is \\[ Y_t=c+Y_{t-1}+Z_t \\] where \\(\\{Z_t\\}\\) are i.i.d and follow a normal distribution. A variation on the random walk method allows the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. Forecasts are given by \\[ \\hat{Y}_{n+h|n}=\\hat{c}h+Y_n=Y_n+h\\left(\\frac{Y_n-Y_1}{n-1}\\right). \\] We use the RW( ~ drift()) with forecast(h) provide to make an \\(h\\) step ahead forecast. Florida.ts %&gt;% model(RW(Y.Death ~ drift())) %&gt;% forecast(h = 14) %&gt;% autoplot(Florida.ts, level = 95) + labs(y = &quot;Death count&quot;, title = &quot;Random Work Method with Drift&quot;) Figure 10.21: Two weeks ahead forecast of the daily death count for Florida using the random walk with drift method. 10.4.5 Displaying all the forecasting results Now, let us display all the forecasting results based on the previous methods together. Figure 10.22 shows the comparison among different methods. # Fit the models using 4 different methods death_fit &lt;- Florida.ts %&gt;% model( Mean = MEAN(Y.Death), `RW` = RW(Y.Death), `Seasonal naïve` = SNAIVE(Y.Death), `RW-Drift` = RW(Y.Death ~ drift()) ) # Generate forecasts for the next 2 weeks death_fc &lt;- death_fit %&gt;% forecast(h = 14) # Show the forecasts in one plot death_fc %&gt;% #Show the point forecasts only without prediction intervals autoplot(Florida.ts, level = NULL) + labs(y = &quot;Death count&quot;, title = &quot;Simple Time Series Forecasting Methods&quot;) + guides(colour = guide_legend(title = &quot;Forecast&quot;)) Figure 10.22: Two weeks ahead forecast of the daily death count for Florida using four different methods. 10.4.6 Distributional forecasts and prediction intervals 10.4.6.1 Forecast distributions A forecast \\(\\hat{Y}_{n+h|n}\\) is (usually) the mean of the conditional distribution \\(Y_{n+h} \\mid Y_1, \\dots, Y_{n}\\). Most time series models produce normally distributed forecasts. The forecast distribution describes the probability of observing any future value. Assuming residuals are normal, uncorrelated, with standard deviation \\(\\hat\\sigma\\): Mean: \\(\\hat{Y}_{n+h|n} \\sim N(\\bar{Y}, (1 + 1/n)\\hat{\\sigma}^2)\\); Random walk: \\(\\hat{Y}_{n+h|n} \\sim N(Y_n, h\\hat{\\sigma}^2)\\); Seasonal random walk: \\(\\hat{Y}_{n+h|n} \\sim N(Y_{n+h-m(k+1)}, (k+1)\\hat{\\sigma}^2)\\), where \\(k\\) is the integer part of \\((h-1)/m\\); Drift: \\(\\hat{Y}_{n+h|n} \\sim N(Y_n + \\frac{h}{n-1}(Y_n - Y_1),h\\frac{n+h}{n}\\hat{\\sigma}^2)\\). Note that when \\(h=1\\) and \\(n\\) is large, these all give the same approximate forecast variance: \\(\\hat{\\sigma}^2\\). 10.4.6.2 Prediction intervals A prediction interval gives a region within which we expect \\(Y_{n+h}\\) to lie with a specified probability. Assuming forecast errors are normally distributed, then a 95% PI is \\[ \\hat{Y}_{n+h|n} \\pm 1.96 \\hat\\sigma_h \\] where \\(\\hat\\sigma_h\\) is the standard deviation of the \\(h\\)-step distribution. When \\(h=1\\), \\(\\hat\\sigma_h\\) can be estimated from the residuals. We can use the hilo() function to convert the forecast distributions into intervals. By default, 80% and 95% prediction intervals are returned. We can use level argument to control coverage. The function unpack_hilo() allows a hilo column to be unpacked into its component columns: “lower,” “upper,” and “level.” fc_result &lt;- Florida.ts %&gt;% model(RW(Y.Death ~ drift())) %&gt;% forecast(h = 14) %&gt;% hilo(level = 95) unpack_hilo(fc_result, `95%`) ## # A tsibble: 14 x 7 [1D] ## # Key: State, .model [1] ## State .model DATE Y.Death .mean `95%_lower` ## &lt;fct&gt; &lt;chr&gt; &lt;date&gt; &lt;dist&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Flor… RW(Y.… 2020-12-12 N(123, 1810) 123. 40.0 ## 2 Flor… RW(Y.… 2020-12-13 N(124, 3631) 124. 5.66 ## 3 Flor… RW(Y.… 2020-12-14 N(124, 5463) 124. -20.7 ## 4 Flor… RW(Y.… 2020-12-15 N(125, 7307) 125. -43.0 ## 5 Flor… RW(Y.… 2020-12-16 N(125, 9161) 125. -62.7 ## 6 Flor… RW(Y.… 2020-12-17 N(125, 11027) 125. -80.5 ## 7 Flor… RW(Y.… 2020-12-18 N(126, 12904) 126. -97.0 ## 8 Flor… RW(Y.… 2020-12-19 N(126, 14792) 126. -112. ## 9 Flor… RW(Y.… 2020-12-20 N(126, 16692) 126. -127. ## 10 Flor… RW(Y.… 2020-12-21 N(127, 18603) 127. -141. ## 11 Flor… RW(Y.… 2020-12-22 N(127, 20524) 127. -154. ## 12 Flor… RW(Y.… 2020-12-23 N(128, 22458) 128. -166. ## 13 Flor… RW(Y.… 2020-12-24 N(128, 24402) 128. -178. ## 14 Flor… RW(Y.… 2020-12-25 N(128, 26357) 128. -190. ## # … with 1 more variable: `95%_upper` &lt;dbl&gt; 10.5 Residual Diagnostics and Accuracy Evaluation 10.5.1 Residual diagnostics The fitted values and residuals from a model can be obtained using the augment() function. In the above example, we saved the fitted models as beer_fit. So we can simply apply augment() to this object to compute the fitted values and residuals for all models. augment(death_lmfit) ## # A tsibble: 324 x 7 [1D] ## # Key: State, .model [1] ## State .model DATE Y.Death .fitted .resid .innov ## &lt;fct&gt; &lt;chr&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Flori… TSLM(Y.D… 2020-01-23 0 4.66 -4.66 -4.66 ## 2 Flori… TSLM(Y.D… 2020-01-24 0 5.01 -5.01 -5.01 ## 3 Flori… TSLM(Y.D… 2020-01-25 0 5.35 -5.35 -5.35 ## 4 Flori… TSLM(Y.D… 2020-01-26 0 5.70 -5.70 -5.70 ## 5 Flori… TSLM(Y.D… 2020-01-27 0 6.05 -6.05 -6.05 ## 6 Flori… TSLM(Y.D… 2020-01-28 0 6.40 -6.40 -6.40 ## 7 Flori… TSLM(Y.D… 2020-01-29 0 6.75 -6.75 -6.75 ## 8 Flori… TSLM(Y.D… 2020-01-30 0 7.09 -7.09 -7.09 ## 9 Flori… TSLM(Y.D… 2020-01-31 0 7.44 -7.44 -7.44 ## 10 Flori… TSLM(Y.D… 2020-02-01 0 7.79 -7.79 -7.79 ## # … with 314 more rows Residuals are useful in checking whether a model has adequately captured the information in the data. If patterns are observable in the residuals, the model can probably be improved. A good forecasting method will yield residuals with the following properties: The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals that should be used in computing forecasts. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased. The residuals have constant variance. The residuals are normally distributed. Any forecasting method that does not satisfy these properties can be improved. The residuals obtained from forecasting this series using the linear regression method with seasonal components are shown in Figure 10.23. augment(death_lmsfit) %&gt;% autoplot(.resid) + labs(x = &quot;Day&quot;, y = &quot;Residual&quot;, title = &quot;Residuals from linear regression with seasonal component.&quot;) Figure 10.23: Residual plot based on the linear regression method with seasonal components. A convenient shortcut for producing these residual diagnostic graphs is the gg_tsresiduals() function, which will produce a time plot, ACF plot and histogram of the residuals. gg_tsresiduals(death_lmsfit) Figure 10.24: Time plot, ACF plot and histogram of the residuals based on the linear regression method with seasonal components. The residuals plots obtained from forecasting this series using the extended exponential smoothing method with trend and seasonality components are shown in Figure 10.25. gg_tsresiduals(etss_fit) Figure 10.25: Time plot, ACF plot and histogram of the residuals based on the extended ETS method with trend and seasonal components. These graphs show that the ETS method produces forecasts that appear to account for all available information. The residuals’ mean is close to zero, and there is no significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, apart from the one outlier. Therefore the residual variance can be treated as constant. This can also be seen on the histogram of the residuals. The histogram suggests that the residuals seem to be normal. Consequently, forecasts from this method will probably be quite good, but prediction intervals that are computed assuming a normal distribution seem to be reasonable. Instead of checking to see whether each sample autocorrelation falls inside the bounds defined in Figure 10.25, it is also possible to consider the portmanteau test such as the Ljung–Box test. The hypothesis of iid data is then rejected at level \\(\\alpha\\) if the p-value is smaller than \\(\\alpha\\). augment(etss_fit) %&gt;% features(.resid, ljung_box, lag=10, dof=0) ## # A tibble: 1 x 4 ## State .model lb_stat lb_pvalue ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Florida ETS 16.9 0.0765 10.5.2 Forecasting accuracy Cross-validation (CV) is a popular technique for model comparison. First, we split the dataset into a subset called the training set, and another subset called the test set; see Figure 10.26. Then, we train the model on the training set, and record the forecast error on the test set. Figure 10.26: Training and test sets. Forecast error is defined as the difference between an observed value and its forecast: \\[ e_{n+h} = Y_{n+h} - \\hat{Y}_{n+h|n}, \\] where the training data is given by \\(\\{Y_1,\\ldots,Y_n\\}\\). For any \\(h=1,\\dots,H\\), let \\(Y_{n+h}\\) be the \\((n+h)\\)th observation. Next, let \\(\\hat{Y}_{n+h|n}\\) be its forecast based on data up to time \\(n\\). Denote \\(e_{n+h}=Y_{n+h} - \\hat{Y}_{n+h|n}\\). For a continuous or discrete time series, a point forecast is usually preferred, such as the number of daily confirmed cases. To evaluate the quality of these point forecasts, several measures have been proposed: \\[\\begin{align*} \\text{Mean Absolute Error (MAE)} &amp;= \\text{mean}(|e_{n+h}|)\\\\ \\text{Mean Squared Error (MSE)} &amp;= \\text{mean}(e_{n+h}^2) \\\\ \\text{Root Mean Squared Error (RMSE)} &amp;= \\sqrt{\\text{mean}(e_{n+h}^2)} \\\\ \\text{Mean Absolute Percentage Error (MAPE)} &amp;= 100\\text{mean}(|e_{n+h}|/ |Y_{n+h}|)\\\\ \\text{Mean Absolute Scaled Error (MASE)} &amp;= \\text{mean}(|e_{n+h}|/Q), \\end{align*}\\] where \\(Q = (n-1)^{-1}\\sum_{t=2}^n |Y_t-Y_{t-1}|\\). Remark: MAE, MSE, RMSE are all scale dependent. MAPE and MASE are scale independent, but MAPE is more sensible than MASE. MAPE can be infinite or undefined if \\(Y_t=0\\) for any \\(t\\) in the period of interest, and having extreme values if any \\(Y_t\\) is close to zero. In the following, we divide the data Florida.ts into two parts: a training set (JAN 23 to NOV 27) and a validation set or test set (NOV 28 to DEC 11). The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the test set. Figure 10.27 shows the forecasts of the daily death count for NOV 28–DEC 11 based on the training data. library(tsibble) # Set training data from JAN 23 to NOV 27 train &lt;- Florida.ts %&gt;% filter_index(&quot;2020-01-23&quot; ~ &quot;2020-11-27&quot;) Reg_fit &lt;- train %&gt;% model(`LM` = TSLM(Y.Death ~ DATE + season(7)), `ETS` = ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)) ) Reg_fc &lt;- Reg_fit %&gt;% forecast(h = 14) Reg_fc %&gt;% autoplot(train, level = 95) + autolayer(filter_index(dplyr::select(Florida.ts, Y.Death), &quot;2020-11-28&quot; ~ .), color = &quot;black&quot;) + labs(y = &quot;Death count&quot;, title = &quot;Exponential smoothing vs linear regression forecast&quot;) Figure 10.27: Forecasts of the daily death count for NOV 28–DEC 11 based on the training data. The accuracy() function can be used to obtain the accuracy of the forecast, which is able to automatically extract the relevant periods from the data (Florida.ts in this example) to match the forecasts when computing the various accuracy measures. accuracy(Reg_fc, Florida.ts) ## # A tibble: 2 x 11 ## .model State .type ME RMSE MAE MPE MAPE MASE ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ETS Florida Test 15.1 24.0 18.4 13.8 17.9 0.847 ## 2 LM Florida Test -21.3 29.1 23.7 -25.2 27.7 1.09 ## # … with 2 more variables: RMSSE &lt;dbl&gt;, ACF1 &lt;dbl&gt; The Evaluating modelling accuracy chapter from the Forecasting: Principles and Practices textbook provides more detail in how modeling and forecasting accuracy is evaluated. 10.6 ARIMA Models 10.6.1 Differencing When working with time series lags, it is useful to introduce the backward shift operator. Let \\(B\\) be the backward shift operator such that \\(B^{k}f(t)=f(t-k)\\). For example, \\[\\begin{eqnarray*} B^{1}t &amp;=&amp; t-1,\\\\ B^{2}t &amp;=&amp; t-2,\\\\ B^{2}t^2 &amp;=&amp; (t-2)^2,\\\\ Bc &amp;=&amp; c~~~~~~(c \\mathrm{~is~a~constant}) \\end{eqnarray*}\\] Next we define \\(1-B\\) as the difference operator. Using the difference operator, is easier to describe the process of differencing, e.g. \\[ (1-B)X_t=X_t-BX_t=X_t-X_{t-1}. \\] Second-order differerence is denoted \\((1-B)^2\\), which is not the same as a second difference \\(1-B^2\\). In general, a \\(d\\)th-order difference can be written as \\[ (1-B)^d X_t. \\] Here are two more examples: If \\(m_t=\\alpha+\\beta t\\), then the first-order difference of \\(m_t\\) is \\((1-B)m_t=\\beta\\). If \\(m_t=\\alpha+\\beta t+\\gamma t^2\\), then the second-order differences of \\(m_t\\) is \\((1-B)^2 m_t=2\\gamma\\). The backshift notation is particularly useful when combining differences, as the operator can be treated using ordinary algebraic rules. In particular, terms involving \\(B\\) can be multiplied together. Let \\(1-B^k\\) be the lag-\\(k\\) difference operator, that is, \\[ (1-B^k) X_t = X_t -X_{t-k}. \\] Applying lag-\\(d\\) differencing to remove seasonal, we have \\[ (1-B^d)s_t=s_{t}-B^{d}s_{t}=s_t-s_{t-d}=0. \\] Remark: In fact, \\[ \\underset{}{(1-B^d)=}\\underset{\\mathrm{difference~operator}}{\\underbrace{(1-B)}} \\underset{\\mathrm{seasonal~summation~filter}}{\\underbrace{[1+B+B^2+\\cdots+B^{d-1}]}} \\] Note: \\((1+B+B^2+\\cdots+B^{d-1})s_{t}=s_{t}+s_{t-1}+\\cdots+s_{t-(d-1)}=\\sum_{j=0}^{d-1}s_{t-j}=0\\). We can use the difference() function in the tsibble package, and two important arguments are: lag: A positive integer indicating which lag to use. differences: A positive integer indicating the order of the difference. Florida.ts %&gt;% autoplot(Death) Florida.ts %&gt;% autoplot(Death %&gt;% difference(1)) Florida.ts %&gt;% autoplot(Death %&gt;% difference(7)) Florida.ts %&gt;% autoplot(Death %&gt;% difference(1) %&gt;% difference(7)) The following provides another method to eliminate the trend and seasonality using differencing. Step 1. Apply a lag-\\(d\\) differencing operator to \\(X_t\\), then \\[\\begin{eqnarray*} (1-B^d)X_t&amp;=&amp;m_t-m_{t-d}+s_t-s_{t-d}+Y_t-Y_{t-d}\\\\ &amp;=&amp;m_t-m_{t-d}+Y_t-Y_{t-d}. \\end{eqnarray*}\\] Step 2. Resulting model has a trend component defined by \\(m_t-m_{t-d}\\) and a stochastic component given by \\(Y_t-Y_{t-d}\\). Step 3. Trend component can be eliminated by applying an appropriate power of differencing operator, say \\((1-B)^{d^{\\prime}}\\). Thus, \\[ \\underbrace{(1-B)^{d^{\\prime}}}{}\\underbrace{(1-B^d)}{}X_t =\\underbrace{(1-B)^{d^{\\prime}}}{}\\underbrace{(1-B^d)}{}m_t +\\underbrace{(1-B)^{d^{\\prime}}}{}\\underbrace{(1-B^d)}{}Y_t \\] is a model for a series related to \\(\\{x_t\\}\\) that is free of trend and seasonal components. 10.6.2 ARMA models This sections introduce an important parametric family of stationary time series, the autoregressive moving-average, or ARMA, processes. A process \\(\\{X_t\\}\\) is said to be ARMA\\((p,q)\\) (for integers \\(p,q \\geq 0\\)), or an AutoRegressive (AR) Moving Average (MA), with \\[\\begin{eqnarray*} \\mathrm{AR~polynomial~} \\phi(z) &amp;=&amp; 1 -\\phi_1 z -\\phi_2 z^2 - \\cdots -\\phi_p z^p \\\\ \\mathrm{MA~polynomial~} \\theta(z) &amp;=&amp; 1 + \\theta_1 z +\\theta_2 z^2 + \\cdots +\\theta_q z^q \\end{eqnarray*}\\] if \\(\\{X_t\\}\\) satisfies \\[ \\phi(B)X_t = \\theta(B)Z_t \\quad \\mathrm{for~all~integers~} t, \\] with respect to some \\(\\{Z_t\\}\\sim \\mathrm{WN}(0,\\sigma^2)\\), where \\(B\\) is the backward shift operator. That is, \\[\\begin{eqnarray*} \\phi(B)X_t &amp;=&amp; X_t -\\phi_1 X_{t-1} -\\phi_2 X_{t-2} - \\cdots -\\phi_p X_{t-p}\\\\ &amp;=&amp;Z_t+\\theta_1 Z_{t-1} +\\theta_2 Z_{t-2} + \\cdots +\\theta_q Z_{t-q}=\\theta(B)Z_t \\end{eqnarray*}\\] The time series \\(\\{X_t\\}\\) is said to be an autoregressive process of order \\(p\\) (or AR\\((p)\\)) if \\(\\theta(z) \\equiv 1\\), and a moving-average process of order \\(q\\) (or MA\\((q)\\)) if \\(\\phi(z) \\equiv 1\\). 10.6.3 Simple examples of ARMA models AR(1) or ARMA(1,0) \\[ X_t=\\phi X_{t-1}+Z_{t} \\] with AR polynomial: \\(\\phi(z)=1-\\phi z\\) and MA polynomial: \\(\\theta(z)=1\\). MA(1) or ARMA(0,1) \\[ X_t=Z_{t}+\\theta Z_{t-1} \\] with AR polynomial: \\(\\phi(z)=1\\) and MA polynomial: \\(\\theta(z)=1 +\\theta z\\). 10.6.4 ARIMA models We consider a generalization of the ARMA models to incorporate a wide range of nonstationary series. A process \\(\\{Y_t\\}\\) is said to be ARIMA\\((p,d,q)\\) (for integers \\(p,d,q \\geq 0\\)), or an AutoRegressive (AR) Integrated (I) Moving Average (MA), if \\(\\{Y_t\\}\\) satisfies \\[\\begin{eqnarray*} (1-B)^d Y_t = X_t \\sim \\mathrm{ARMA}(p,q) \\mathrm{~for~all~integers~} t. \\end{eqnarray*}\\] Example: \\(Y_t \\sim\\) ARIMA\\((p,1,q)\\): suppose \\(X_t \\sim \\mathrm{ARMA}(p,q)\\) \\[ Y_t=Y_{t-1}+X_t=\\cdots=Y_{0}+\\sum_{j=1}^t X_j, ~ t=1,2, \\ldots \\] Remarks: For any positive integer \\(d=1,2,\\ldots\\), \\(\\{Y_t\\}\\) is not weakly stationary. For \\(d = 0\\), \\(\\{Y_t\\}\\) is just \\(\\mathrm{ARMA}(p, q)\\) [typically weakly stationary] with so-called short-memory dependence. For \\(0&lt;d&lt;1\\), \\(\\{Y_t\\}\\) is weakly stationary with so-called long-memory dependence. Example 1: Suppose \\(\\{X_t\\}\\) is an ARIMA(1,1,0): \\[ (1-\\phi B)(1-B)X_t=Z_t, ~ Z_t\\sim \\mathrm{WN}(0,\\sigma^2) \\] One can solve this as \\[ X_t=X_0+\\sum_{j=1}^{t}Y_j, \\mathrm{~with} \\] \\[ Y_t=(1-B)X_t=\\sum_{j=0}^{\\infty}\\phi^jZ_{t-j}. \\] Example 2: Suppose \\(\\{X_t\\}\\) is an ARIMA(0,1,1): \\[ X_t=X_{t-1}+W_t-\\theta_1W_{t-1} \\] If \\(|\\theta_1|&lt;1\\), we can show \\[ X_t=\\sum_{j=1}^{\\infty}(1-\\theta_1)\\theta_1^jX_{t-j}+W_t, \\mathrm{~and~so} \\] \\[\\begin{eqnarray*} \\tilde{X}_{n+1}&amp;=&amp;\\sum_{j=1}^{\\infty}(1-\\theta_1)\\theta_1^jX_{n+1-j}\\\\ &amp;=&amp; (1-\\theta_1)X_n+\\sum_{j=2}^{\\infty}(1-\\theta_1)\\theta_1^jX_{n+1-j}\\\\ &amp;=&amp; (1-\\theta_1)X_n+\\theta_1\\tilde{X}_{n}, \\end{eqnarray*}\\] which behaves like exponentially weighted moving average. Building ARIMA models Step 1. Plot the time series. Look for trends, seasonal components, step changes, outliers. Step 2. Nonlinearly transform data, if necessary. Step 3. Identify preliminary values of \\(d\\), \\(p\\), and \\(q\\). Step 4. Estimate parameters. Step 5. Use diagnostics to confirm residuals are white noise/iid/normal. Step 6. Model selection. Identifying \\(d, p, q\\) For identifying preliminary values of \\(d\\), a time series plot can also help. Too little differencing: not stationary. Too much differencing: extra dependence introduced. For identifying \\(p, q\\), look at sample ACF, PACF of \\((1 - B)^dX_t\\): Table 10.1: ACF and PACF for ARMA models Model ACF PACF AR(p) decays zero for h &gt; p MA(q) zero for h &gt; q decays ARMA(p,q) decays decays 10.6.5 Seasonal ARIMA (SARIMA) model A SARIMA model is formed by including additional seasonal terms in the ARIMA models we have seen so far. If \\(d\\) and \\(D\\) are nonnegative integers, then \\(\\{X_t\\}\\) is a seasonal ARIMA\\((p, d, q)\\times (P, D, Q)_s\\) process with period \\(s\\) if the differenced series \\[ Y_t =(1-B)^d(1-B^s)^DX_t \\] is a causal ARMA process defined by \\[ \\phi(B)\\Phi(B^s)Y_t=\\theta(B)\\Theta(B^s)Z_t, ~~Z_t\\sim WN(0,\\sigma^2), \\] where \\[\\begin{eqnarray*} \\phi(z) &amp;=&amp; 1 -\\phi_1 z -\\phi_2 z^2 - \\cdots -\\phi_p z^p, \\\\ \\Phi(z) &amp;=&amp; 1 -\\Phi_1 z -\\Phi_2 z^2 - \\cdots -\\Phi_p z^P, \\\\ \\theta(z) &amp;=&amp; 1 + \\theta_1 z +\\theta_2 z^2 + \\cdots +\\theta_q z^q,\\\\ \\Theta(z) &amp;=&amp; 1 + \\Theta_1 z +\\Theta_2 z^2 + \\cdots +\\Theta_q z^Q. \\end{eqnarray*}\\] Pure seasonal ARMA Models For \\(P,Q \\geq 0\\) and \\(s &gt; 0\\), we say that a time series \\(\\{X_t\\}\\) is an ARMA(P,Q)\\(_s\\) process if \\(\\Phi(B^s)X_t = \\Theta(B^s)Z_t\\), where \\[ \\Phi(B^s) = 1 - \\sum_{j=1}^{P}\\Phi_jB^{js}, \\] \\[ \\Theta(B^s) = 1+ \\sum_{j=1}^{Q}\\Theta_jB^{js}, \\] Example: \\(P=0\\), \\(Q=1\\), \\(s=12\\). \\(X_t=Z_t+\\Theta_1Z_{t-12}\\). \\[\\begin{align*} \\gamma(0) &amp;= (1+\\Theta_1^2)\\sigma^2,\\\\ \\gamma(12)&amp;= \\Theta_1 \\sigma^2,\\\\ \\gamma(h) &amp;= 0, \\mathrm{~for~} h = 1, 2,\\ldots, 11, 13, 14, \\ldots. \\end{align*}\\] Example: \\(P=1\\), \\(Q=0\\), \\(s=12\\). \\(X_t=\\Phi_1X_{t-12}+Z_{t}\\). \\[\\begin{align*} \\gamma(0) &amp;= \\frac{\\sigma^2}{1-\\Phi_1^2},\\\\ \\gamma(12i)&amp;= \\frac{\\sigma^2\\Phi_1^i}{1-\\Phi_1^2},\\\\ \\gamma(h) &amp;= 0, \\mathrm{~for~other~} h. \\end{align*}\\] The ACF and PACF for a seasonal ARMA\\((P,Q)\\)s are zero for \\(h \\neq si\\). For \\(h = si\\), they are analogous to the patterns for ARMA\\((p,q)\\): Table 10.2: My caption Model ACF PACF AR(P)s decays zero for i &gt; P MA(Q)s zero for i &gt; Q decays ARMA(P,Q)s decays decays We can estimate an ARIMA model using the ARIMA function, which searches through the model space specified in the specials to identify the best ARIMA model which has lowest AIC, AICc or BIC value. We can specify an ARIMA model via the formula argument. If the right hand side of the formula is left blank, the default search space is given by pdq() + PDQ(): that is, a model with candidate seasonal and nonseasonal terms, but no exogenous regressors. To specify a model fully (avoid automatic selection), the intercept and pdq(), PDQ() values must be specified: for example, formula = response ~ 1 + pdq(1, 1, 1) + PDQ(1, 0, 0) In the above, the pdq special is used to specify non-seasonal components of the model, and the PDQ special is used to specify seasonal components of the model. To force a nonseasonal fit, specify PDQ(0, 0, 0) in the right hand side of the model formula. The period argument is used in PDQ to specify the periodic nature of the seasonality. 10.6.6 Building SARIMA Models The seasonal part of an AR or MA model can be seen in the seasonal lags of the PACF and ACF. For Florida.ts, after a lag 7 differencing, Figure 10.9 shows the sample ACF and PACF plots. From Figure 10.9, we observe a spike at lag 7 in the ACF anda spike at lag 7 at the PACF but no other significant spikes. Therefore, we can specify PDQ(1, 1, 1) in the RHS of the model formula as below. sarima111 &lt;- Florida.ts %&gt;% model(ARIMA(Y.Death ~ PDQ(1,1,1))) We can use the function report() to obtain the formatted model-specific display. report(sarima111) ## Series: Y.Death ## Model: ARIMA(1,0,2)(1,1,1)[7] ## ## Coefficients: ## ar1 ma1 ma2 sar1 sma1 ## 0.9733 -0.7395 -0.0746 0.2611 -0.8071 ## s.e. 0.0166 0.0631 0.0598 0.1197 0.0946 ## ## sigma^2 estimated as 873.1: log likelihood=-1522.67 ## AIC=3057.33 AICc=3057.6 BIC=3079.89 Thus, the fitted SARIMA\\((1,0,2)(1,1,1)_7\\) can be written as \\[ (1-\\phi_1B)(1-\\Phi_1B^7)(1-B^7)X_t=(1+\\theta_1B+\\theta_2B^2)(1+\\Theta_1B^7)Z_t, \\] where \\(Z_t\\sim WN(0,873.1)\\), and \\(\\phi_1=0.9733\\), \\(\\theta_1=-0.7395\\), \\(\\theta_2=-0.0746\\), \\(\\Phi_1=0.2611\\), \\(\\Theta_1=-0.8071\\). Below, we try to fit more SARIMA models for Florida.ts, and conduct the two weeks ahead forecast. death_sarima &lt;- Florida.ts %&gt;% model( sarima011 = ARIMA(Y.Death ~ PDQ(0,1,1)), sarima111 = ARIMA(Y.Death ~ PDQ(1,1,1)), stepwise = ARIMA(Y.Death), search = ARIMA(Y.Death, stepwise=FALSE) ) # Generate forecasts for the next 2 weeks death_fc &lt;- death_sarima %&gt;% forecast(h = 14) # Plot forecasts against actual values death_fc %&gt;% autoplot(Florida.ts, level = NULL) + labs(y = &quot;Death count&quot;, title = &quot;Different ARIMA Forecasts&quot;) + guides(colour = guide_legend(title = &quot;Methods&quot;)) + theme(legend.position=&quot;bottom&quot;) Figure 10.28: Two weeks ahead forecast of the daily death count for Florida using different ARIMA models. The glance() function provides a one-row summary of each model, and commonly includes descriptions of the model’s fit, such as the residual variance and information criteria. It is worth noticing that the information criteria (AIC, AICc, BIC) are only comparable between the same model class and only if those models share the same response (after transformations and differencing). death_sarima %&gt;% glance() %&gt;% arrange(AICc) ## # A tibble: 4 x 9 ## State .model sigma2 log_lik AIC AICc BIC ar_roots ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 Flor… search 841. -1517. 3047. 3048. 3074. &lt;cpl [3… ## 2 Flor… sarim… 873. -1523. 3057. 3058. 3080. &lt;cpl [8… ## 3 Flor… sarim… 884. -1525. 3058. 3058. 3073. &lt;cpl [1… ## 4 Flor… stepw… 884. -1525. 3058. 3058. 3073. &lt;cpl [1… ## # … with 1 more variable: ma_roots &lt;list&gt; 10.7 Model Comparison 10.7.1 Exponential smoothing and ARIMA models Exponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data. As discussed in Hyndman and Athanasopoulos (2018b), the ETS model describes how unobserved components of the data (error, trend, and seasonality) change over time, while ARIMA focuses on the autocorrelations in the data. Furthermore, the additive ETS models are all special cases of ARIMA models, while the non-additive ETS models do not have any equivalent ARIMA counterparts. On the other hand, many ARIMA models have no exponential smoothing counterparts. Thirdly, all ETS models are non-stationary, but some ARIMA models are stationary. Note that we can not use AIC or AICc to compare ETS and ARIMA because they are in different model classes, and the likelihood is computed in different ways. Instead, we can use the validation comparison by dividing the data into two parts: a training set (JAN 23 to NOV 27) and a validation set or test set (NOV 28 to DEC 11). To compare how well the models fit the data, we can consider some common accuracy measures. death_fit &lt;- train %&gt;% model( `ETS` = ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), `ARIMA` = ARIMA(Y.Death, stepwise=FALSE) ) # Model fitting results of ARIMA death_fit %&gt;% dplyr::select(ARIMA) %&gt;% report() ## Series: Y.Death ## Model: ARIMA(3,0,2)(0,1,1)[7] ## ## Coefficients: ## ar1 ar2 ar3 ma1 ma2 sma1 ## 2.0725 -1.3305 0.2443 -1.8798 0.9449 -0.6705 ## s.e. 0.0671 0.1278 0.0657 0.0341 0.0377 0.0648 ## ## sigma^2 estimated as 858.6: log likelihood=-1452.72 ## AIC=2919.44 AICc=2919.82 BIC=2945.43 # Evaluate the modeling and forecasting accuracy death_fit %&gt;% accuracy() %&gt;% arrange(MASE) ## # A tibble: 2 x 11 ## State .model .type ME RMSE MAE MPE MAPE MASE ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Florida ARIMA Traini… 1.00 28.7 17.7 NaN Inf 0.813 ## 2 Florida ETS Traini… 0.138 29.9 18.2 NaN Inf 0.835 ## # … with 2 more variables: RMSSE &lt;dbl&gt;, ACF1 &lt;dbl&gt; It seems that the ARIMA model slightly out-performs the ETS for the series based on the MASE of the training set. # Generate forecasts for the next 2 weeks death_fc &lt;- death_fit %&gt;% forecast(h = 14) # Plot forecasts against actual values death_fc %&gt;% autoplot(train, level = 95) + autolayer(filter_index( dplyr::select(Florida.ts, Y.Death), &quot;2020-11-28&quot; ~ .), color = &quot;black&quot;) + labs(y = &quot;Death count&quot;, title = &quot;ETS vs ARIMA Forecasts&quot;) + guides(colour = guide_legend(title = &quot;Forecasts&quot;)) + theme(legend.position=&quot;bottom&quot;) Figure 10.29: Two weeks ahead forecast of the daily death count for Florida using ETS and ARIMA models. # Evaluate the forecasting accuracy based on test set death_fc %&gt;% accuracy(Florida.ts) %&gt;% arrange(MASE) ## # A tibble: 2 x 11 ## .model State .type ME RMSE MAE MPE MAPE MASE ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ETS Florida Test 15.1 24.0 18.4 13.8 17.9 0.847 ## 2 ARIMA Florida Test 26.5 32.8 26.8 25.7 26.1 1.23 ## # … with 2 more variables: RMSSE &lt;dbl&gt;, ACF1 &lt;dbl&gt; Based on the MASE of the test set and Figure 10.29, it seems that the ETS model slightly out-performs the ARIMA. 10.7.2 Cross-validation for Time series analysis As described in the previous sections, we can use the validation approach for model comparison. If any parameters need to be tuned, we split the training set into a training subset and a validation set. The model is trained on the training subset and the parameters that minimize error on the validation set are chosen. Finally, the model is trained on the full training set using the chosen parameters, and the error on the test set is recorded. Figure 10.30: An illustration of traditional time series validation. However, you may notice that the choice of the test set in Figure 10.31 is fairly arbitrary, and that choice may mean that our test set error is a poor estimate of error on an independent test set. Cross-validation (CV) is a popular technique for tuning hyperparameters and producing robust measurements of model performance. Two of the most common types of cross-validation are leave-one out cross-validation and \\(k\\)-fold cross-validation. Figure 10.31 illustrates the idea of \\(k\\)-fold cross-validation. First, we split randomly data into \\(k\\) folds, a subset called the training set, and another subset called the test set based on one fold. Focus on train set, which contains \\(k-1\\) folds, to train the model and test on the \\(k\\)th fold. Repeat the above \\(k\\) times to get \\(k\\) accuracy measures on 10 different and separate folds. Compute the average of the \\(k\\) accuracies which is the final reliable number telling us how the model is performing. Figure 10.31: An illustration of k-fold cross-validation. When dealing with time series data, traditional cross-validation (like \\(k\\)-fold) should not be used because of the temporal dependencies. In the case of time series, the cross-validation is not trivial. We cannot choose random samples and assign them to either the test set or the train set because it makes no sense to use the values from the future to forecast values in the past. In order to accurately simulate the “real world forecasting environment, in which we stand in the present and forecast the future” (Tashman 2000), the forecaster must withhold all data about events that occur chronologically after the events used for fitting the model. So, rather than use \\(k\\)-fold cross-validation, for time series data we utilize hold-out cross-validation where a subset of the data (split temporally) is reserved for validating the model performance. For example, see Figure 10.32 where the test set data comes chronologically after the training set. Similarly, the validation set comes chronologically after the training subset. The inner loop works exactly as discussed before: the training set is split into a training subset and a validation set, the model is trained on the training subset, and the parameters that minimize error on the validation set are chosen. However, in the outer loop which splits the dataset into multiple different training and test sets chronologically, and the forecast accuracy is computed by averaging over the test sets. Figure 10.32: An illustration of strech rolling cross-validation for time series. There are three main rolling types which can be used. Stretch: extends a growing length window with new data; see Figure 10.32. Slide: shifts a fixed length window through the data; see Figure 10.33. Tile: moves a fixed length window without overlap; see Figure 10.34. and we can apply the following functions to roll a “tsibble”: stretch_tsibble() slide_tsibble() tile_tsibble() Figure 10.33: An illustration of slide rolling cross-validation for time series. Figure 10.34: An illustration of tile rolling cross-validation for time series. These functions provide fast and shorthand for rolling over a tsibble by observations. They all return a tsibble including a new column .id as part of the key. The output dimension will increase considerably with slide_tsibble() and stretch_tsibble(), which is likely to run out of memory when the data is large. For time series cross-validation, stretching windows are most commonly used. # Split training and test using stretching window Florida.train &lt;- Florida.ts %&gt;% filter_index(&quot;2020-04-01&quot; ~ &quot;2020-11-27&quot;) %&gt;% # Stretch with a minimum length of 60 # growing by 7 each step stretch_tsibble(.init = 60, .step = 7) %&gt;% relocate(DATE, State, .id) Florida.train ## # A tsibble: 3,835 x 7 [1D] ## # Key: State, .id [26] ## # Groups: State [1] ## DATE State .id Infected Death Y.Infected Y.Death ## &lt;date&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2020-04-01 Flori… 1 6.95 85 0.214 0 ## 2 2020-04-02 Flori… 1 9.00 144 2.05 59 ## 3 2020-04-03 Flori… 1 10.3 163 1.26 19 ## 4 2020-04-04 Flori… 1 11.5 194 1.28 31 ## 5 2020-04-05 Flori… 1 12.3 221 0.81 27 ## 6 2020-04-06 Flori… 1 13.3 235 0.971 14 ## 7 2020-04-07 Flori… 1 14.5 254 1.23 19 ## 8 2020-04-08 Flori… 1 15.5 309 0.908 55 ## 9 2020-04-09 Flori… 1 16.4 353 0.911 44 ## 10 2020-04-10 Flori… 1 17.5 390 1.17 37 ## # … with 3,825 more rows # Training set model fit FL.fit &lt;- Florida.train %&gt;% model( `ETS` = ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), `ARIMA` = ARIMA(Y.Death ~ 1 + pdq(1,0,1) + PDQ(0,1,1)) ) # Training set accuracy FL.fit %&gt;% accuracy() ## # A tibble: 52 x 12 ## State .id .model .type ME RMSE MAE MPE MAPE ## &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Flori… 1 ETS Train… -2.25 15.7 12.4 -Inf Inf ## 2 Flori… 1 ARIMA Train… 0.984 17.5 13.1 -Inf Inf ## 3 Flori… 2 ETS Train… -1.55 15.8 12.2 -Inf Inf ## 4 Flori… 2 ARIMA Train… 1.28 16.9 12.8 -Inf Inf ## 5 Flori… 3 ETS Train… -1.16 15.5 11.9 -Inf Inf ## 6 Flori… 3 ARIMA Train… 1.33 16.2 12.4 -Inf Inf ## 7 Flori… 4 ETS Train… -1.33 14.9 11.4 -Inf Inf ## 8 Flori… 4 ARIMA Train… 1.14 16.0 12.0 Inf Inf ## 9 Flori… 5 ETS Train… -1.26 14.5 10.8 -Inf Inf ## 10 Flori… 5 ARIMA Train… 1.19 15.5 11.7 Inf Inf ## # … with 42 more rows, and 3 more variables: MASE &lt;dbl&gt;, ## # RMSSE &lt;dbl&gt;, ACF1 &lt;dbl&gt; # 7-day forecast accuracy period.fc &lt;- 7 FL.fc &lt;- FL.fit %&gt;% forecast(h = period.fc) %&gt;% group_by(.id) %&gt;% mutate(h = row_number()) %&gt;% ungroup() FL.accuracy &lt;- FL.fc %&gt;% accuracy(Florida.ts, by = c(&quot;h&quot;, &quot;.model&quot;)) FL.accuracy$h &lt;- rep(1:period.fc, 2) FL.accuracy %&gt;% ggplot(aes(x = h, y = RMSE)) + geom_line(aes(color = .model)) + geom_point(aes(color = .model)) Figure 10.35: Two weeks ahead forecast of the daily death count for Florida using ETS and ARIMA models. 10.8 Ensuring Forecasts Stay Within Limits In epidemic forecasting, it is common to set forecasts to be positive, especially when forecasting count time series, or to require them to be within some specified range \\([a,b]\\). Both of these situations are relatively easy to handle using transformations. 10.8.1 Positive forecasts To impose a positivity constraint, we can simply work on the log scale. The following is an example using ETS models applied to the daily new death count time series for Florida. # Obtain the ETS fit with/without log transformation ETS.fit &lt;- train %&gt;% model( `ETS` = ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), `logETS` = ETS(log(Y.Death+1) ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)) ) # Generate forecasts for the next 2 weeks ETS_fc &lt;- ETS.fit %&gt;% forecast(h = 14) # Plot forecasts against actual values ETS_fc %&gt;% autoplot(train, level = 95) + autolayer( filter_index(dplyr::select(Florida.ts, Y.Death), &quot;2020-11-28&quot; ~ .), color = &quot;black&quot;) + labs(y = &quot;Death count&quot;, title = &quot;ETS Forecasts with/without log transformation&quot;) + guides(colour = guide_legend(title = &quot;Forecasts&quot;)) + theme(legend.position=&quot;bottom&quot;) Figure 10.36: Two weeks ahead forecast of the daily death count for Florida using ETS with/without log transformation. # Residual plot for the ETS without the log transformation ETS.fit %&gt;% select(ETS) %&gt;% gg_tsresiduals(lag=36) # Residual plot for the log transformed EST method ETS.fit %&gt;% select(logETS) %&gt;% gg_tsresiduals(lag=36) 10.8.2 Forecasts constrained to an interval Sometimes it makes sense to assume that the number of deaths are constrained to lie within \\([a,b]\\). To handle data constrained to an interval, we can transform the data using a scaled logit transform as follows: \\[ y=\\log\\left(\\frac{x-a}{b-x}\\right), \\] where \\(x\\) is on the original scale and \\(y\\) is the transformed data. To reverse the transformation, we will use \\[ x=\\frac{(b-a)e^y}{1+e^y}+a. \\] This is not a built-in transformation, so we will need to first setup the transformation functions. scaled_logit &lt;- function(x, lower = 0, upper = 1){ log((x - lower) / (upper - x)) } inv_scaled_logit &lt;- function(x, lower = 0, upper = 1){ (upper - lower) * exp(x) / (1 + exp(x)) + lower } my_scaled_logit &lt;- new_transformation(scaled_logit, inv_scaled_logit) Now, we can make the prediction based on the transfromed time series. Let us consider the forecast within \\([0,300]\\). train %&gt;% model(ETS(my_scaled_logit(Y.Death + 1, lower = 0, upper = 300) ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;))) %&gt;% forecast(h = 14) %&gt;% autoplot(train, level = 95) Figure 10.37: Two weeks ahead forecast of the daily death count for Florida using ETS, constrained to be within [0,300]. 10.9 Prediction and Prediction Intervals for Aggregates We considered the daily new death count in the above sections, but we may want to forecast the cumulative count. If the point forecasts are means, then adding them up will give a reasonable estimate of the cumulative count. However, prediction intervals are more tricky due to the correlations between forecast errors. A general solution is to use simulations. For example, we consider the forecast of the cumulative death count in the next two weeks. d_ets_fit &lt;- train %&gt;% model( `ETS` = ETS(Y.Death ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)) ) d.pred.paths &lt;- d_ets_fit %&gt;% # Simulate 10000 future sample paths, each of length 14 generate(times = 10000, h = 14) %&gt;% # Sum the results for each sample path as_tibble() %&gt;% group_by(.rep) %&gt;% mutate(.sim = as.integer(.sim)) %&gt;% mutate(.sim = replace(.sim, which(.sim &lt; 0), 0)) %&gt;% mutate(.csum = cumsum(.sim)) We can compute the mean of the simulations, and extract a specified prediction interval at a particular level. For example, d.pred.report &lt;- d.pred.paths %&gt;% group_by(DATE) %&gt;% summarize(.mean = mean(.csum), .lpi95 = quantile(.csum, .025), .upi95 = quantile(.csum, .975), ) %&gt;% mutate(.mean = .mean + tail(train$Death,1), .lpi95 = .lpi95 + tail(train$Death,1), .upi95 = .upi95 + tail(train$Death,1) ) Figure 10.38 shows the two weeks ahead forecast of the cumulative death count for Florida using ETS. ggplot(train, aes(DATE, Death)) + geom_line() + labs(x = &quot;Days&quot;, y = &quot;Count&quot;, title = &#39;Cumulative deaths and prediction&#39;) + # Add prediction intervals geom_ribbon(mapping = aes(x = DATE, y = .mean, ymin = .lpi95, ymax = .upi95, fill = &#39;95% Prediction intervals&#39;), data = d.pred.report, alpha = 0.4) + # Add line for predicted values geom_line(mapping = aes(x = DATE, y = .mean, colour = &#39;Predicted Value&#39;), linetype = &quot;dashed&quot;, data = d.pred.report, # Set the line type in legend key_glyph = &quot;timeseries&quot;) + scale_colour_manual(&quot;&quot;, values = &quot;red&quot;) + scale_fill_manual(&quot;&quot;, values = &quot;pink&quot;) + theme(legend.position = &quot;bottom&quot;) Figure 10.38: Two weeks ahead forecast of the cumulative death count for Florida using ETS. 10.10 Exercises From state.long in the R package slid, choose a state and convert the data to time series. Construct time series plots for the daily new death, daily new infection, cumulative deaths and cumulative infection. For the daily new death series and daily new infection series, use the following graphics functions: autoplot(), gg_season(), gg_subseries(), gg_lag(), ACF() and explore features from the following time series: + Can you spot any seasonality and trend? + What do you learn about the series? + What can you say about the seasonal patterns? + Can you identify any unusual pattern? Conduct an STL decomposition for the daily new death series. For the daily new death series, create a training dataset consisting of observations from JAN 23 to NOV 27. Calculate 14-day ahead forecasts using SNAIVE() and ETS applied to your training data. Compare the accuracy of your forecasts against the actual values. Check the residuals. Do the residuals appear to be uncorrelated and normally distributed? Let \\(\\{Z_t\\}\\) be a sequence of independent normal random variables, each with mean 0 and variance \\(\\sigma^2\\), and let \\(a\\), \\(b\\), and \\(c\\) be constants. Which, if any, of the following processes are stationary? For each stationary process specify the mean and autocovariance function. \\(Y_t = a + bZ_t + cZ_{t-2}\\) \\(Y_t = Z_1 \\cos(ct) + Z_2 \\sin(ct)\\) \\(Y_t = Z_t \\cos(ct) + Z_{t−1} \\sin(ct)\\) \\(Y_t = a + bZ_0\\) \\(Y_t = Z_0 \\cos(ct)\\) \\(Y_t = Z_tZ_{t−1}\\) Use the tsibble() function to generate a time series of length 200 of the following simple ARIMA models. Produce a time plot for each simulated series, and draw the sample ACF and PACF for the simulated time series. AR(1) model with \\(\\phi_1=0.9\\) and \\(\\sigma^2=1\\). MA(1) model with \\(\\theta_1=0.8\\) and and \\(\\sigma^2=1\\). MA(2) model with \\(\\theta_1=0.3\\), \\(\\theta_2=-0.4\\) and \\(\\sigma^2=1\\). From state.long in the R package slid, choose a state and convert the data to time series. Use the automatic search in ARIMA() to find an ARIMA model. What model was selected? Write the model in terms of the backshift operator. "],["regression.html", "Chapter 11 Regression Methods 11.1 Parametric Regression Methods 11.2 Nonparametric Regression Methods 11.3 Example: CDC FluView Portal Data 11.4 Poisson regression 11.5 Logistic regression", " Chapter 11 Regression Methods In Chapter 10 we introduces the time series analysis tools, which treat the value of a target variable at each time point as a random variable and uses the covariance function between these random variables to represent the correlation; another approach uses a deterministic smooth surface function to describe the variations and connections among values at different locations. This chapter introduces the latter approach. In this chapter the analytical techniques of regression and discrimination are introduced as a means of quantifying the effect of a set of explanatory variables on the spatial distribution of a particular outcome. The material presented here is similar to that which might be presented in standard statistical texts, but includes an overview of the modifications needed to account for the temporal dependency frequently associated with disease data. The use of nonparametric techniques has a long tradition in time series analysis. We present some nonparametric methods to forecast a seasonal univariate time series, and propose some dynamic updating methods to improve point forecast accuracy. Perhaps the first aspect to consider is the type of outcome variable under investigation. In epidemiology interest lies in understanding patterns of disease in populations, so it is often the case that the outcome variable is either a count of disease events for area units, or more simply a binary response indicating the presence or absence of disease at a given location. Knowledge of the type of outcome variable is important since it determines the regression technique to be used and the options available to account for spatial dependence. 11.1 Parametric Regression Methods Linear regression is a technique that can be used to model the broad-scale (first-order) trend. It allows the mean value of a continuous response variable (also known as a dependent variable) \\(Y_t\\) to be represented as a function of \\(d\\) explanatory (also known as covariate, predictor, or independent) variables: \\[ Y_{t}=\\beta_0+\\beta_1X_{1t}+\\beta_2X_{2t}+\\cdots+\\beta_dX_{dt}+\\epsilon_t, \\] where \\(\\beta_{0}, \\beta_{1},\\ldots, \\beta_d\\) are unknown parameters or coefficients, and \\(\\varepsilon\\) is the error term. Note the following key assumptions behind this type of regression analysis: The relationship between the response and the regressors is linear, at least approximately. The errors all have the same variance: \\(\\mathrm{Var}(\\epsilon_t) = \\sigma^2\\) for all \\(t\\). The errors are independent of each other. That is, the value of the observation at any point is not affected by the value of observations at any other point. The residuals \\(\\epsilon_t\\) are normally distributed with a mean of zero. 11.1.1 Examples To predict the number of deaths for COVID-19, Altieri et al. (2021) considered various regression models for modeling the death count at the county level in the US. The death count is assumed to follow a linear or exponential relationship with time or the current death count. A separate-county exponential predictor: a series of predictors built for predicting cumulative death counts for each county using only past death counts from that county. \\[ E(D_{t+1}|t) = \\exp{\\left\\{\\beta_0 +\\beta_1(t+1)\\right\\}}. \\] A separate-county linear predictor: a predictor similar to the separate county exponential predictors, but uses a simple linear format, rather than the exponential format. \\[ E(D_{t+1}|t) = \\beta_0 + \\beta_1\\times(t + 1). \\] A shared-county exponential predictor: a single predictor built using death counts from all counties, used to predict death counts for individual counties. \\[ E(D_{t+1}|t) = \\exp\\left\\{\\beta_0 + \\beta_1 \\log\\left(D_t + 1\\right)\\right\\}. \\] An expanded shared-county exponential predictor: a predictor similar to the shared-county exponential predictor, which also includes COVID-19 case numbers and neighboring county cases and deaths as predictive features. \\[ E(D_{t+1}|t) = \\exp\\left\\{\\beta_0 + \\beta_1 \\log\\left(D_t + 1\\right)\\right\\}. \\] A demographics shared-county exponential predictor: a predictor also similar to the shared-county exponential predictor, but which also includes various county demographic and health-related predictive features. \\[\\begin{eqnarray*} E(D_{t+1}|t) =&amp; \\exp\\left\\{\\beta_0 + \\beta_1 \\log\\left(D_t + 1\\right) + \\beta_2\\log\\left(I_{t-k+1}+1\\right)\\right. \\\\ &amp; \\left. + \\beta_3\\log\\left(D_{t-k+1}^\\mathrm{neighbor}\\right) + \\beta_4\\log\\left(I_{t-k+1}^\\mathrm{neighbor}\\right) \\right\\}. \\end{eqnarray*}\\] In the following, we illustrate Methods 1 and 2 using the data Florida.ts. We are interested in making predictions for the period December 05-December 11, 2020, and we use a training set from the past week (November 28 to December 04, 2020). The linear regression model and exponential model will be fitted on the training set, and the fitted model is used to make predictions. Figure 11.1 shows the forecasts of the daily death count. # set training data from NOV 28 to DEC 04 train &lt;- Florida.ts %&gt;% filter_index(&quot;2020-11-28&quot; ~ &quot;2020-12-04&quot;) # both linear and exponential trend fit_trends &lt;- train %&gt;% model( linear = TSLM(Y.Death ~ trend()), exponential = TSLM(log(Y.Death + 1) ~ trend()), ) fc_trends &lt;- fit_trends %&gt;% forecast(h = 7) # make predictions for the next week Florida.ts %&gt;% autoplot(Y.Death) + geom_line(data = fitted(fit_trends), aes(y = .fitted, color = .model)) + autolayer(fc_trends, alpha = 0.5, level = 95) + labs(y = &quot;Death Count&quot;, title = &quot;Florida daily new deaths&quot;) Figure 11.1: Reported death count vs linear and exponential fit. 11.1.2 Model adequacy checking We should consider the validity of the assumptions mentioned before. Violations of the assumptions may yield an unstable model in the sense that a different sample could lead to a totally different model with opposite conclusions. Graphical analysis of residuals (original or scaled) is a very effective way to investigate the adequacy of the fit. Normal probability plot of residuals; Plot of residuals against the fitted values; Plot of residuals against each regressor variable; Plot of residuals in time series (if time series data were collected). Goodness of Fit How well does the model fit the data? One measure is \\(R^2\\), the so-called coefficient of determination or percentage of variance explained \\[ R^2 = 1- \\frac{\\Sigma (Y_i-\\hat{Y}_i)^2}{\\Sigma (Y_i-\\bar{Y}^2)} = 1- \\frac{\\mbox{RSS}}{\\mbox{Total SS (corrected for mean)}}. \\] The range is \\(0 \\le R^2 \\le 1\\), and values closer to 1 indicates better fits. For simple linear regression \\(R^2=r^2\\) where \\(r\\) is the correlation between \\(x\\) and \\(y\\). An equivalent definition is \\[ R^2 = \\frac{\\Sigma (\\hat{Y}_i-\\bar{Y})^2}{\\Sigma (Y_i-\\bar{Y})^2} = \\frac{\\mbox{Regression Sum of Squares}}{\\mbox{Total SS (corrected for mean)}}. \\] After selecting the regression variables and fitting a regression model, it is necessary to plot the residuals to check that the assumptions of the model have been satisfied. There are a series of plots that should be produced in order to check different aspects of the fitted model and the underlying assumptions. We will now discuss each of them in turn. ACF plot of residuals With time series data, it is highly likely that the value of a variable observed in the current time period will be similar to its value in the previous period, or even the period before that, and so on. Therefore when fitting a regression model to time series data, it is common to find autocorrelation in the residuals. In this case, the estimated model violates the assumption of no autocorrelation in the errors, and our forecasts may be inefficient — there is some information left over which should be accounted for in the model in order to obtain better forecasts. The forecasts from a model with autocorrelated errors are still unbiased, and so are not “wrong,” but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals. Histogram of residuals It is always a good idea to check whether the residuals are normally distributed. As we explained earlier, this is not essential for forecasting, but it does make the calculation of prediction intervals much easier. Residual plots against predictors We would expect the residuals to be randomly scattered without showing any systematic patterns. A simple and quick way to check this is to examine scatterplots of the residuals against each of the predictor variables. If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly. It is also necessary to plot the residuals against any predictors that are not in the model. If any of these show a pattern, then the corresponding predictor may need to be added to the model (possibly in a nonlinear form). 11.2 Nonparametric Regression Methods It is common for time series data to be trending. A linear trend can be modelled by simply using \\(x_{1t}=t\\) as a predictor, \\[ Y_t=\\beta_0+\\beta_1 t+\\varepsilon_t, \\] where \\(t=1,\\ldots,n\\). A trend variable can be specified in the TSLM() function using the trend() special. In this section, we further extend the linear regression to model the nonlinear trends. A nonparametric regression model assumes \\[ y_{t}=m\\left(t\\right)+\\varepsilon_{t}, \\ \\ t=1,\\cdots,n, \\] where \\(\\varepsilon_{t}\\) are assumed to be i.i.d. with mean \\(0\\) and variance \\(\\sigma^{2}\\), \\(m(\\cdot)\\) is a nonparametric regression function. Linear regression is a special case: \\[ m(t)=\\beta_{0}+\\beta_{1}t. \\] 11.2.1 Piecewise constant splines We break the time domain into bins, and fit a different constant in each bin. We create cut-off points \\(k_1\\), \\(k_2\\), , \\(k_{N}\\) in the time domain, and then construct \\(N + 1\\) piecewise constant basis functions: \\[\\begin{eqnarray*} B_0(t) &amp;=&amp; I(t &lt;k_1) \\\\ B_1(t) &amp;=&amp; I(k_1 \\leq t &lt;k_2) \\\\ B_2(t) &amp;=&amp; I(k_2 \\leq t &lt;k_3) \\\\ \\cdots &amp;=&amp; \\cdots \\\\ B_{N-1}(t) &amp;=&amp; I(k_{N-1} \\leq t &lt;k_N) \\\\ B_N(t) &amp;=&amp; I(k_N \\leq t) \\end{eqnarray*}\\] where \\(I(\\cdot)\\) is an indicator function that returns a \\(1\\) if the condition is true, indicator and returns a \\(0\\) otherwise. Figure 11.2 shows a fit for the daily new death count for Florida using piecewise constant splines. n &lt;- nrow(Florida.ts) t &lt;- 1:n y &lt;- Florida.ts$Y.Death # knots N &lt;- 21 knots &lt;- 1 + (n-1)/(N+1) * (0:N) # piecewise constant spline basis t.rep &lt;- matrix(rep(t, N), n, N) knot.L &lt;- matrix(rep(knots[-(N + 1)], each = n), n, N) knot.R &lt;- matrix(rep(knots[-1], each = n), n, N) B &lt;- 1*((knot.L &lt;= t.rep) &amp; (t.rep &lt; knot.R)) X &lt;- cbind(B, knots[N] &lt; t &amp; t &lt;= n) # piecewise constant spline fit M &lt;- t(X) %*% X beta &lt;- solve(M) %*% t(X) %*% y yhat &lt;- X %*% beta Florida.ts$pcs_preds &lt;- yhat # plot of reported vs piecewise constant spline fit Florida.ts %&gt;% ggplot(aes(x = DATE)) + geom_line(aes(y = Y.Death, color = &quot;Reported&quot;)) + geom_line(aes(y = pcs_preds, color = &quot;Fitted&quot;)) + scale_color_manual( values = c(Reported = &quot;black&quot;, Fitted = &quot;red&quot;) ) + labs(y = &quot;Death count&quot;, title = &quot;Reported vs piecewise constant spline fit&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) Figure 11.2: Piecewise constant spline smoothing for the daily new death count for Florida. 11.2.2 Truncated power splines For univariate data, polynomial splines of degree \\(p\\) can be represented by an appropriate sequence of \\(N+p+1\\) spline basis functions, determined in turn by \\(N\\) interior knots. These produce functions that are piecewise polynomials of degree \\(p\\) between the knots, and joined up with continuity of degree \\(p-1\\) at the knots. As an example consider linear splines, or piecewise linear functions. Let \\(k_1 &lt; k_2 &lt; \\ldots &lt; k_N\\) be knots in the time domain, for example, \\([1, n]\\). Let \\(\\phi_0(t) = 1\\), \\(\\phi_1(t) = t\\), \\(\\phi_j(t) = (t-k_{j-1})_{+}\\) for \\(j = 2, \\ldots, N+1\\) be the basis functions, where \\(x_{+}\\) denotes positive part of \\(x\\). Then, the regression is piecewise linear with bends at the knots. y &lt;- Florida.ts$Y.Death n &lt;- nrow(Florida.ts) t &lt;- 1:n # knots N &lt;- 10 knots &lt;- 1 + (n-1)/(N+1) * (0:N) # truncated power spline basis functions X &lt;- matrix(1, n, N + 2) X[, 2] &lt;- t t.rep &lt;- matrix(rep(t, N), n, N) tmp &lt;- t.rep - matrix(rep(knots[2:(N + 1)], each = n), n, N) X[, 3:(N+2)] &lt;- tmp * (tmp &gt; 0) # truncated power spline fit M &lt;- t(X) %*% X beta &lt;- solve(M) %*% t(X) %*% y yhat &lt;- X %*% beta Florida.ts$tps_preds &lt;- yhat # plot of reported vs truncated power spline fit Florida.ts %&gt;% ggplot(aes(x = DATE)) + geom_line(aes(y = Y.Death, color = &quot;Reported&quot;)) + geom_line(aes(y = tps_preds, color = &quot;Fitted&quot;)) + scale_color_manual( values = c(Reported = &quot;black&quot;, Fitted = &quot;red&quot;) ) + labs(y = &quot;Death count&quot;, title = &quot;Reported vs truncated power spline fit&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) In general, let \\[ (t-k_{j})_{+}^{p}=\\left\\{\\begin{array}{cc}(t-k_{j})^{p} &amp; \\mathrm{for} \\ t\\geq k_{j}\\\\ 0 &amp; \\mathrm{for} \\ t&lt;k_{j}\\end{array}\\right. \\] Then, the \\(N+p+1\\) truncated power basis functions are: \\[\\begin{eqnarray*} \\phi_{1}(t)&amp;=&amp;1, \\\\ \\phi_{2}(t)&amp;=&amp;t, \\\\ &amp;\\vdots&amp; \\\\ \\phi_{p}(t)&amp;=&amp;t^{p},\\\\ \\phi_{p+1}(t)&amp;=&amp;(t-k_{1})_{+}^{p},\\\\ &amp;\\vdots&amp; \\\\ \\phi_{p+N}(t)&amp;=&amp;(t-k_{N})_{+}^{p}. \\end{eqnarray*}\\] We define the spline estimator for the regression function \\(m(x)\\) as \\[ \\hat{m}\\left(t\\right) =\\sum_{k=0}^{p} \\hat{\\beta}_{k} t^{k}+\\sum_{j=1}^{N}\\hat{\\gamma}_{j}\\left(t-\\kappa_{j}\\right)_{+}^{p}, \\] where \\(\\{\\hat{\\beta}_{0},\\ldots,\\hat{\\beta}_{p},\\hat{\\gamma}_{1},\\ldots,\\hat{\\gamma}_{N}\\}\\) are the least square estimators of \\(\\{\\beta_{0},\\ldots,\\beta_{p},\\gamma_{1},\\ldots,\\gamma_{N}\\}\\) based on the data. The shape of the basis functions is determined by the position of the knots \\(k_{1}&lt;\\ldots&lt;k_{N}\\), which can, for example, be uniformly spread over the time domain. 11.2.3 B-splines and natural splines Both B-splines and natural splines similarly define a basis over the time domain. They are made up of piecewise polynomials of a given degree, and have defined derivatives similarly to the piecewise defined functions. To introduce the space of splines, we pre-select an integer \\(N\\), and divide the time domain \\([a,b]\\) into \\((N+1)\\) subintervals \\(J_{j}=[k_{j},k_{j+1})\\), \\(j=0,\\ldots,N-1\\), \\(J_{N}=[k_{N},b]\\), where \\(\\{k_{j}\\}_{j=1}^{N}\\) is a sequence of equally-spaced points, called interior knots, given as \\[ k_{1-p}=...=k_{-1}=k_{0}=a&lt;k_{1}&lt;\\ldots&lt;k_{N}&lt;b=k_{N+1}=\\ldots=k_{N+p}, \\] in which \\(k_{j}=j(b-a)/(N+1)\\), \\(j=0,1,\\ldots,N+1\\). The \\(j\\)-th B-spline of order \\(r\\) denoted by \\(b_{j,r}\\) is recursively defined by de Boor (2001) as follows: For \\(r=1\\), basis \\(b_{1,1},\\ldots, b_{N,1}\\): \\[ b_{j,1}(t)=I\\left\\{t\\in[k_{j},k_{j+1})\\right\\}. \\] Given \\(b_{j,r-1},j=-(r-1),\\ldots,N\\), construct \\(b_{j,r}\\) \\[ b_{j,r}(t)=\\frac{t-k_{j}}{k_{j+r-1}-k_{j}}b_{j,r-1}(t)+\\frac{k_{j+r}-t}{k_{j+r}-k_{j+1}}b_{j+1,r-1}(t) \\] E.g., \\(r=2\\), basis \\(b_{-1,2},\\ldots, b_{N,2}\\): \\[ b_{j,2}(t)=\\frac{t-k_{j}}{k_{j+1}-k_{j}}b_{j,1}(t)+\\frac{k_{j+2}-t}{k_{j+2}-k_{j+1}}b_{j+1,1}(t). \\] It is well known that the behavior of polynomials fit to data tends to be erratic near the boundaries, and extrapolation can be dangerous. These problems are exacerbated with splines. The polynomials fit beyond the boundary knots behave even more wildly than the corresponding global polynomials in that region. This can be conveniently summarized in terms of the pointwise variance of spline functions fit by least squares. A natural cubic spline adds additional constraints, namely that the function is linear beyond the boundary knots. This frees up four degrees of freedom (two constraints each in both boundary regions), which can be spent more profitably by sprinkling more knots in the interior region. # natural spline fit library(splines) n &lt;- nrow(Florida.ts) t &lt;- 1:n ns_fit &lt;- lm(Y.Death ~ ns(t, df = 6), data = Florida.ts) summary(ns_fit) ## ## Call: ## lm(formula = Y.Death ~ ns(t, df = 6), data = Florida.ts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -129.894 -14.531 -1.122 14.886 136.643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.947 10.467 -0.473 0.636843 ## ns(t, df = 6)1 50.740 13.185 3.848 0.000144 *** ## ns(t, df = 6)2 41.051 16.893 2.430 0.015649 * ## ns(t, df = 6)3 214.442 15.009 14.288 &lt; 2e-16 *** ## ns(t, df = 6)4 11.851 13.081 0.906 0.365637 ## ns(t, df = 6)5 88.438 26.606 3.324 0.000992 *** ## ns(t, df = 6)6 104.854 11.976 8.756 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 38.72 on 317 degrees of freedom ## Multiple R-squared: 0.5771, Adjusted R-squared: 0.5691 ## F-statistic: 72.09 on 6 and 317 DF, p-value: &lt; 2.2e-16 Florida.ts$ns_preds &lt;- predict(ns_fit) # natural spline prediction and prediction intervals h &lt;- 14 t.new &lt;- t[n] + (1:h) ns_PI &lt;- predict(ns_fit, newdata = data.frame(t = t.new), interval = &quot;prediction&quot;, level = 0.95) ns_PI &lt;- as.data.frame(ns_PI) %&gt;% mutate(DATE = (Florida.ts$DATE)[n] + 1:h) # reported values vs fitted values Florida.ts %&gt;% ggplot(aes(x = ns_preds, y = Y.Death)) + geom_point() + xlab(&quot;Fitted (predicted values)&quot;) + ylab(&quot;Data (reported values)&quot;) + ggtitle(&quot;Observed vs fitted&quot;) + geom_abline(intercept=0, slope=1) # plot of reported vs natural spline fit ns_p &lt;- Florida.ts %&gt;% ggplot(aes(x = DATE)) + geom_line(aes(y = Y.Death, color = &quot;Reported&quot;)) + geom_line(aes(y = ns_preds, color = &quot;Fitted&quot;)) + scale_color_manual( values = c(Reported = &quot;black&quot;, Fitted = &quot;red&quot;) ) + labs(y = &quot;Death count&quot;, title = &quot;Reported vs natural spline regression fit&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) ns_p # plot of natural spline fit and its prediction intervals ns_p &lt;- ns_p + geom_ribbon( mapping = aes(y = fit, ymin = lwr, ymax = upr, fill = &#39;95% Prediction Intervals&#39;), data = ns_PI, alpha = 0.2) + geom_line(mapping = aes(y = fit, color = &quot;Fitted&quot;), data = ns_PI, key_glyph = &quot;timeseries&quot;) ns_p 11.2.4 Smoothing splines A different idea to estimate the regression function \\(m(\\cdot)\\) by \\[ \\min_{m}\\frac{1}{n}\\sum_{t=1}^{n}\\left(Y_{t}-m(x_{t})\\right)^{2}+\\lambda \\int \\left(m^{\\prime\\prime}(t)\\right)^{2}dt, \\] where \\(\\lambda&gt;0\\) is a smoothing parameter, and it controls large values of the second derivative of \\(m\\). For \\(\\lambda = 0\\) no penalty is imposed, and any interpolating function will do, while for \\(\\lambda = \\infty\\) only functions linear in \\(t\\) are permitted. # smoothing spline fit library(mgcv) ss_fit &lt;- gam(Y.Death ~ s(t, bs = &quot;cr&quot;), data = Florida.ts) summary(ss_fit) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Y.Death ~ s(t, bs = &quot;cr&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 60.846 2.038 29.86 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(t) 8.715 8.976 57.55 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.613 Deviance explained = 62.4% ## GCV = 1387.4 Scale est. = 1345.8 n = 324 Florida.ts$ss_preds &lt;- predict(ss_fit) # plot of reported vs smoothing spline fit Florida.ts %&gt;% ggplot(aes(x = DATE)) + geom_line(aes(y = Y.Death, color = &quot;Reported&quot;)) + geom_line(aes(y = ss_preds, color = &quot;Fitted&quot;)) + scale_color_manual( values = c(Reported = &quot;black&quot;, Fitted = &quot;red&quot;) ) + labs(y = &quot;Death count&quot;, title = &quot;Reported vs smoothing spline fit&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) 11.3 Example: CDC FluView Portal Data The CDC FluView Portal provides in-season and past seasons’ national, regional, and state-level outpatient illness and viral surveillance data from both ILINet (Influenza-like Illness Surveillance Network) and WHO/NREVSS (National Respiratory and Enteric Virus Surveillance System). FluView, a weekly influenza surveillance report, and FluView Interactive, an online application which allows for more in-depth exploration of influenza surveillance data, are updated each week. R Package “cdcfluview” retrieves Flu Season Data from the CDC “FluView” Portal. We can use the function ilinet() to retrieve state, regional or national influenza statistics from the CDC. library(cdcfluview) library(dplyr) library(lubridate) # prepare data usflu.raw &lt;- ilinet(&quot;national&quot;, years = 2010:2018) names(usflu.raw) ## [1] &quot;region_type&quot; &quot;region&quot; ## [3] &quot;year&quot; &quot;week&quot; ## [5] &quot;weighted_ili&quot; &quot;unweighted_ili&quot; ## [7] &quot;age_0_4&quot; &quot;age_25_49&quot; ## [9] &quot;age_25_64&quot; &quot;age_5_24&quot; ## [11] &quot;age_50_64&quot; &quot;age_65&quot; ## [13] &quot;ilitotal&quot; &quot;num_of_providers&quot; ## [15] &quot;total_patients&quot; &quot;week_start&quot; usflu &lt;- usflu.raw %&gt;% mutate( date = as.Date(paste0(year, sprintf(&quot;%02d&quot;, week), &quot;00&quot;), format=&quot;%Y%W%w&quot;), dec_date = decimal_date(week_start), week = yearweek(week_start), time_in_year = dec_date%%1) %&gt;% dplyr::filter(!is.na(dec_date)) usflu.ts &lt;- as_tsibble(usflu, index = week) Figure 11.3 displays the weekly time series of weighted_ili from 2010 to 2019. usflu.ts %&gt;% autoplot(weighted_ili) + labs(x = &quot;week&quot;, title = &quot;National influenza-like illness&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) Figure 11.3: National influenza-like illness weekly time series. Trigonometric regression For the \\(i\\)th year and \\(j\\)th reported time point \\(t_{ij}\\), \\(i=1,2,\\ldots,n\\), we have \\[ Y_{ij} = \\beta_0 + \\beta_1\\sin(2\\pi t_{ij}) + \\beta_2\\cos(2\\pi t_{ij}) + \\varepsilon_{ij}, \\] where \\(t_{ij} \\in [0,1]\\), \\(\\beta_0\\) and \\(\\beta_1\\) are unknown coefficients. We will consider linear regression method to estimate \\(\\beta_0\\) and \\(\\beta_1\\). trig_fit &lt;- lm(weighted_ili ~ sin(dec_date*2*pi) + cos(dec_date*2*pi), data = usflu) usflu$trig_preds &lt;- predict(trig_fit) Figure 11.4 displays the reported and trigonometric regression fit of weighted_ili from 2010 to 2019. ggplot(usflu, aes(x = week)) + geom_line(aes(y = weighted_ili, color = &quot;Reported&quot;)) + geom_line(aes(y = trig_preds, color = &quot;Trigonometric&quot;)) + scale_color_manual( values = c(Reported = &quot;black&quot;, Trigonometric = &quot;red&quot;) ) + labs(x = &quot;week&quot;, title = &quot;Reported vs trigonometric regression fit&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) Figure 11.4: Reported vs trigonometric regression fit. Smoothing splines For the \\(i\\)th year and \\(j\\)th reported time point \\(t_{ij}\\), we have \\[ Y_{ij} = m(t_{ij}) + \\varepsilon_{ij}, \\] where \\(t_{ij} \\in [0,1]\\), \\(i=1,2,\\ldots,n\\). In the following, we use the smoothing splines to estimate \\(m\\) function. library(mgcv) ss_fit &lt;- gam(weighted_ili ~ s(time_in_year, bs = &quot;cc&quot;), data = usflu) usflu$ss_preds &lt;- predict(ss_fit) Figure 11.5 displays the reported, the fitted weighted_ili from 2010 to 2019 based on the trigonometric regression method and spline smoothing method. ggplot(usflu, aes(x = week)) + geom_line(aes(y = weighted_ili, color = &quot;Reported&quot;)) + geom_line(aes(y = trig_preds, color = &quot;Trigonometric&quot;)) + geom_line(aes(y = ss_preds, color = &quot;Spline&quot;)) + scale_color_manual( values = c(Reported = &quot;black&quot;, Trigonometric = &quot;red&quot;, Spline = &quot;blue&quot;)) + labs(x = &quot;week&quot;, title = &quot;Spline smoothing vs trigonometric regression fit&quot;) + guides(color = guide_legend(title = &quot;Series&quot;)) Figure 11.5: Spline smoothing vs trigonometric regression fit. 11.4 Poisson regression When the outcome of interest is a count of the number of events occurring in a population of a given size, or a count of the number of events in relation to the number of person- or animal-years at risk, a reasonable assumption is that these counts follow a Poisson distribution (especially for diseases that are either non-contagious or rare). To illustrate the regression technique appropriate for Poisson-distributed data, the county-level COVID-19 infected count is considered. We assume that county-level infected counts (\\(Y_i\\)) follow a Poisson distribution. We assume that the conditional mean value of daily new positive cases (\\(\\mu_i\\)) for county \\(i\\) can be modeled via a \\(\\log(\\cdot)\\) function as follows \\[\\begin{equation} \\log(\\mu_i) = \\beta_0+\\beta_1X_{1i} +\\cdots +\\beta_pX_{pi}. \\tag{11.1} \\end{equation}\\] In (11.1), the terms (\\(\\beta_0+\\beta_1X_{1i} +\\cdots +\\beta_pX_{pi}\\)) represent an adjustment to account for disease counts that are either above or below that expected, based on time at risk. 11.5 Logistic regression As the level of resolution of our analyses becomes greater, the spatial unit of interest typically shifts from areas to points. Instead of describing and explaining disease counts summarized by area, the objective here is to identify factors that influence the risk of disease being present or absent at specific locations (e.g. farm or household) using the binary labels ‘positive’ (i.e. disease present) or ‘negative’ (i.e. disease absent). When modelling binary data, explanatory variables are used to predict the probability of a study subject being disease positive (i.e. a ‘case’). Rather than modeling the response \\(Y\\) directly, logistic regression models the probability that \\(Y\\) belongs to a particular category. The logistic model solves the following problems: \\[ \\log\\left\\{\\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\\right\\}=\\beta_0+\\beta_1x, \\] for some unknown \\(\\beta_0\\) and \\(\\beta_1\\), which we will estimate directly. \\(P(Y=0|X=x)=1-P(Y=1|X=x) \\Rightarrow \\log\\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1x\\) \\(p\\) is the probability that event \\(Y\\) occurs (range \\(=0\\) to 1). \\(p/(1-p)\\) is the odds ratio (range \\(=0\\) to \\(\\infty\\)). \\(\\log\\left\\{p/(1-p)\\right\\}\\) is log odds ratio or logit (range \\(=-\\infty\\) to \\(\\infty\\)). Odds and Odds Ratios The definitions of an odds: \\(odds=\\frac{p}{1-p}\\). The odds has a range from 0 to \\(\\infty\\) with values greater than 1 associated with an event being more likely to occur than not occur and values less than 1 associated with an event that is less likely to occur than not occur. The logit is defined as the log of the odds: \\[ \\log (\\mathrm{odds})=\\log\\left(\\frac{p}{1-p}\\right)=\\log{(p)}-\\log{(1-p)}. \\] This transformation is useful because it creates a variable with a range from \\(-\\infty\\) to \\(\\infty\\). Hence, this transformation solves the problem we encountered in fitting a linear model to probabilities. Because probabilities only range from 0 to 1, we can get linear predictions that are outside of this range. The interpretation of logits is simple – take the exponential of the logit and you have the odds for the two groups in question. Interpretation The logit distribution constrains the estimated probabilities to lie between 0 and 1. The estimated probability is \\[ p=P(Y=1|X=x)=\\frac{\\exp(\\beta_0+\\beta_1x)}{1+\\exp(\\beta_0+\\beta_1x)}. \\] If \\(\\beta_0+\\beta_1x=0\\), then \\(p=0.5\\). As \\(\\beta_0+\\beta_1x\\) gets really big, \\(p\\) approaches 1. As \\(\\beta_0+\\beta_1x\\) gets really small, \\(p\\) approaches 0. Estimating Logistic Regression Coefficients Suppose that we are given a sample \\((x_{i},y_{i})\\), \\(i=1,\\ldots,n\\). Here \\(y_{i}\\) denotes the class \\(\\in\\{0,1\\}\\) of the \\(i\\)th observation. Assume that the classes are conditionally independent given \\(x_{1},\\ldots,x_{n}\\), then \\[ \\mathbb{L}(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}P(Y=y_{i}|X=x_{i}), \\] the likelihood of these \\(n\\) observations, so the log likelihood is \\[ l(\\beta_0,\\beta_1)=\\sum_{i=1}^{n}\\log{P(Y=y_{i}|X=x_{i})}. \\] For convenience, we define the indicator \\(u_{i}=\\left\\{\\begin{array}{cc} 1 &amp;\\mathrm{if}~y_{i}=1\\\\ 0 &amp;\\mathrm{if}~y_{i}=0 \\end{array}\\right.\\) Estimating Logistic Regression Coefficients The log-likelihood can be written as \\[\\begin{eqnarray*} l(\\beta_0,\\beta_1)&amp;=&amp;\\sum_{i=1}^{n}\\log{P(Y=y_{i}|x=x_{i})}\\\\ &amp;=&amp;\\sum_{i=1}^{n}[u_{i}(\\beta_0+ \\beta_1 x_{i})-\\log{\\{1+\\exp(\\beta_0+ \\beta_1 x_{i})\\}}] \\end{eqnarray*}\\] The coefficients are estimated by maximizing the likelihood, \\[ \\sum_{i=1}^{n}[u_{i}(\\beta_0+ \\beta_1 x_{i})-\\log{\\{1+\\exp(\\beta_0+ \\beta_1 x_{i})\\}}] \\] "],["NN.html", "Chapter 12 Neural Networks 12.1 A Single Neuron 12.2 Neural Network Structure 12.3 Overfitting 12.4 Neural Network Auto-Regressive (NNAR) models 12.5 COVID-19 Forecasting", " Chapter 12 Neural Networks Artificial Neural Network is computing system inspired by biological neural network that constitute animal brain. Such systems “learn” to perform tasks by considering examples, generally without being programmed with any task-specific rules. 12.1 A Single Neuron The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (\\(w\\)), which is assigned on the basis of its relative importance to other inputs. The node applies a function \\(f\\) (defined below) to the weighted sum of its inputs as shown in Figure 12.1 below: Figure 12.1: An illustration of a single neuron. The above network takes numerical inputs \\(X_0,X_1,X_2, X_3\\) and has weights \\(w_0,w_1,w_2,w_3\\) associated with those inputs. The output \\(Y\\) from the neuron is computed as shown in the Figure 12.1. The function \\(f\\) is non-linear and is called the Activation Function. The purpose of the activation function is to introduce non-linearity into the output of a neuron. This is important because most real world data is non linear and we want neurons to learn these non linear representations. Every activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice: Sigmoid: takes a real-valued input and squashes it to range between 0 and 1 \\[ \\sigma(x) = 1 / (1 + \\exp(-x)) \\] tanh: takes a real-valued input and squashes it to the range \\([-1, 1]\\) \\[ \\tanh(x) = 2\\sigma(2x) - 1 \\] ReLU (Rectified Linear Unit): it takes a real-valued input and thresholds it at zero (replaces negative values with zero) \\[ f(x) = \\max(0, x) \\] Figure 12.2 show each of the above activation functions. Figure 12.2: An illustration of activation functions. As the number of training data for each country is limited, we use a single-layer neural network called the extreme learning machine (ELM) to avoid over-fitting. Due to the non-stationary nature of the time-series, a sliding window approach is used to provide a more accurate prediction. 12.2 Neural Network Structure Figure 12.3 shows a neural network constructed from 3 type of layers: Input Layer: The Input layer has four nodes. The other two nodes take \\(X_1\\), \\(X_2\\) and \\(X_3\\) as external inputs (which are numerical values depending upon the input dataset). As discussed above, no computation is performed in the Input layer, so the outputs from nodes in the Input layer are \\(X_0\\), \\(X_1\\), \\(X_2\\) and \\(X_3\\) respectively, which are fed into the Hidden Layer. Hidden Layer: The Hidden layer also has four nodes with the bias node \\(a_0^{(2)}\\). The output of the other two nodes in the Hidden layer depends on the outputs from the Input layer \\((X_0, X_1, X_2, X_3)\\) as well as the weights associated with the connections (edges). Figure 4 shows the output calculation for one of the hidden nodes (highlighted). Similarly, the output from other hidden node can be calculated. Remember that \\(f\\) refers to the activation function. These outputs are then fed to the nodes in the Output layer. Output Layer: The Output layer has one nodes which takes inputs from the Hidden layer and perform similar computations as shown for the highlighted hidden node. The value calculated \\(a_1^{(3)}\\) as a result of these computations act as the output. Figure 12.3: A neural network with one hidden layer. In Figure 12.3, \\(a^{(j)}_i\\): activation of unit \\(i\\) in layer \\(j\\). \\(\\boldsymbol{\\beta}^{(j)}\\): weight matrix stores parameters from layer \\(j\\) to layer \\(j+1\\). If network has \\(s_j\\) units in layer \\(j\\) and \\(s_{j+1}\\) units in layer \\(j+1\\), then \\(\\boldsymbol{\\beta}^{(j)}\\) has dimension \\(s_{j+1} \\times (s_j+1)\\). \\[ \\mathbf{X}= \\begin{bmatrix} X_0 \\\\ X_1 \\\\ X_2 \\\\ X_3 \\\\ \\end{bmatrix}, \\mathbf{A}= \\begin{bmatrix} a_0^{(2)} \\\\ a_1^{(2)} \\\\ a_2^{(2)} \\\\ a_3^{(2)} \\\\ \\end{bmatrix}, \\] In the regression, our objective function is \\[ \\ell(\\boldsymbol{\\beta}^{(1)}, \\ldots, \\boldsymbol{\\beta}^{(L)})= \\sum_{i=1}^{N}(Y_i - \\widehat{Y}_i)^2 \\] Gradient Descent: \\[ \\boldsymbol{\\beta}^{(j)} \\leftarrow \\boldsymbol{\\beta}^{(j)} - \\alpha \\nabla_{\\boldsymbol{\\beta}^{(j)}} \\ell(\\boldsymbol{\\beta}^{(1)}, \\ldots, \\boldsymbol{\\beta}^{(L)}), \\] for all \\(j\\). 12.2.1 Forward propagation Figure 12.4: A neural network with one hidden layer. We start with one single observation point \\((Y_i, X_i)\\). \\(a^{(1)} = X_i\\) \\(z^{(2)} = \\boldsymbol{\\beta}^{(1)}a^{(1)}\\) \\(a^{(2)} = g(z^{(2)})\\), (add \\(a^{(2)}_0\\)) \\(z^{(3)} = \\boldsymbol{\\beta}^{(2)}a^{(2)}\\) \\(a^{(3)} = g(z^{(3)})\\), (add \\(a^{(3)}_0\\)) \\(z^{(4)} = \\boldsymbol{\\beta}^{(3)}a^{(3)}\\) \\(a^{(4)} = g(z^{(4)})\\) Backpropagation: top layer Figure 12.5: A neural network with one hidden layer. We want to find \\[ \\frac{\\partial \\ell}{\\partial \\beta_{01}^{(2)}}, \\ldots, \\frac{\\partial \\ell}{\\partial \\beta_{31}^{(2)}}, \\] \\[\\begin{align*} \\frac{\\partial \\ell}{\\partial \\beta_{01}^{(2)}} &amp; = \\frac{\\partial \\ell}{\\partial a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{\\partial z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{\\partial \\beta_{01}^{(2)}}\\\\ &amp; = 2(Y-a_1^{(3)})g&#39;_0(z_1^{(3)})a_0^{(2)}, \\end{align*}\\] where \\(g&#39;_0\\) is the derivative fo the activation function in the output layer. Update \\(\\beta_{01}^{(2)}\\) by \\[ \\beta_{01}^{(2)} = \\beta_{01}^{(2)} - \\alpha \\frac{\\partial \\ell}{\\partial \\beta_{01}^{(2)}}. \\] Backpropagation: next layer Figure 12.6: a neural network with one hidden layer. We want to find \\[ \\frac{\\partial \\ell}{\\partial \\beta_{01}^{(1)}}, \\cdots, \\frac{\\partial \\ell}{\\partial \\beta_{03}^{(1)}}, \\] \\[\\begin{align*} \\frac{\\partial \\ell}{\\partial \\beta_{01}^{(2)}} &amp; = \\frac{\\partial \\ell}{\\partial a_1^{(3)}} \\frac{\\partial a_1^{(3)}}{\\partial z_1^{(3)}} \\frac{\\partial z_1^{(3)}}{\\partial \\partial a_1^{(2)}} \\frac{\\partial a_1^{(2)}}{\\partial z_1^{(2)}} \\frac{\\partial z_1^{(2)}}{\\partial \\beta_{01}^{(2)}}\\\\ &amp; = 2(Y-a_1^{(3)})g&#39;_0(z_1^{(3)}) \\beta^{(2)}_{11} g&#39;_1(z_1^{(2)}) X_0, \\end{align*}\\] where \\(g&#39;_0\\) is the derivative fo the activation function in the output layer and \\(g&#39;_1\\) is the derivative fo the activation function in the Layer 2. Update \\(\\beta_{01}^{(1)}\\) by \\[ \\beta_{01}^{(1)} = \\beta_{01}^{(1)} - \\alpha \\frac{\\partial \\ell}{\\partial \\beta_{01}^{(1)}}. \\] Back Propagation: Jacobian Matrix The Jacobian matrix of a vector-valued function in several variables is the matrix of all its first-order partial derivatives. Suppose \\(\\boldsymbol{f}: R^{n} \\to R^{m}\\), the Jacobian matrix of function \\(\\boldsymbol{f}\\) is \\[ \\boldsymbol{J} = \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\ldots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\ldots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}. \\] Chain rule: \\[ \\frac{\\partial \\boldsymbol{f}\\{{\\boldsymbol{g}(\\boldsymbol{x})\\}}}{\\partial \\boldsymbol{x}} = \\frac{\\partial \\boldsymbol{f}\\{{\\boldsymbol{g}(\\boldsymbol{x})\\}}}{\\partial \\boldsymbol{g}(\\boldsymbol{x})} \\times \\frac{\\boldsymbol{g}(\\boldsymbol{x}) }{\\partial \\boldsymbol{x}} \\] 12.3 Overfitting Often neural networks have too many weights and will overfit the data, so we add a penalty to the error function: \\[ \\sum_{j = 1}^{L} \\|\\boldsymbol{\\beta}^{(j)}\\|_F,~\\|\\boldsymbol{\\beta}^{(j)}\\|_F= \\sum_{k=1}^{s_j} \\sum_{k=1}^{s_{j+1}}(\\beta^{(j)}_{k,k&#39;})^2, \\] where \\(\\|\\boldsymbol{\\beta}^{(j)}\\|_F\\) is the Frobenius norm of matrix \\(\\boldsymbol{\\beta}^{(j)}\\). We minimize the following objective function \\[ \\ell(\\boldsymbol{\\beta}^{(1)}, \\ldots, \\boldsymbol{\\beta}^{(L)}) + \\lambda \\sum_{j = 1}^{L} \\|\\boldsymbol{\\beta}^{(j)}\\|_F, \\] where \\(\\|\\cdot\\|_F\\) denotes the Frobenius norm. 12.4 Neural Network Auto-Regressive (NNAR) models Lagged values of the time series can be used as inputs to a neural network. In this section, we only consider feed-forward networks with one hidden layer, and we use the notation NNAR\\((p,k)\\) to indicate there are \\(p\\) lagged inputs and \\(k\\) nodes in the hidden layer. For example, a NNAR(9,5) model is a neural network with the last nine observations \\((y_{t-1}, y_{t-2}, \\ldots, y_{t-9})\\) used as inputs for forecasting the output \\(y_t\\), and with five neurons in the hidden layer. NNAR\\((p,0)\\) model is equivalent to an ARIMA\\((p,0,0)\\) model but without stationarity restrictions, but without the restrictions on the parameters to ensure stationarity. For seasonal data, we can consider seasonal NNAR\\((p,P,k)\\), which use inputs \\((Y_{t-1},Y_{t-2},\\ldots, Y_{t-p}, Y_{t-m}, Y_{t-2m},\\ldots, Y_{t-Pm})\\) and \\(k\\) neurons in the hidden layer. For example, an NNAR\\((3,1,2)_{7}\\) model has inputs \\((Y_{t-1},Y_{t-2}, Y_{t-3}, Y_{t-7})\\), and two neurons in the hidden layer. An NNAR\\((p,P,0)_m\\) model is equivalent to an ARIMA\\((p,0,0)(P,0,0)_m\\) model but without stationarity restrictions. NNAR models in R The nnetar() function fits an NNAR\\((p,P,k)_m\\) model. If \\(p\\) and \\(P\\) are not specified, they are automatically selected. For non-seasonal time series, default \\(p =\\) optimal number of lags (according to the AIC) for a linear AR\\((p)\\) model. For seasonal time series, defaults are \\(P = 1\\) and \\(P\\) is chosen from the optimal linear model fitted to the seasonally adjusted data. Default \\(k = (p + P + 1)=2\\) (rounded to the nearest integer). When it comes to forecasting, the network is applied iteratively. For forecasting one step ahead, we simply use the available historical inputs. For forecasting two steps ahead, we use the one-step forecast as an input, along with the historical data. This process proceeds until we have computed all the required forecasts. 12.5 COVID-19 Forecasting The number of cases, we define \\(x_n = (y_{n-w+1}, ..., y_{n-1}, y_n)^{\\top}\\in R^{w}\\) and \\(t_n = y_{n+h}\\in R\\). We use cross-validation to find the proper window size \\(w\\), number of hidden neurons \\(L\\), and the regularization hyperparameter \\(\\lambda\\). The nnetar function in the forecast package for R fits a feed-forward neural networks model with a single hidden layer and lagged inputs for forecasting univariate time series. It uses lagged values of the time series as inputs (and possibly some other exogenous inputs). To use the forecast package, it is better to store the time series as a ts object in R. Let us consider the time series of daily new death counts for Florida, and we first turn this into a ts object using the ts function. Since we have observed the 7-day cycle for this data, we can simply add a frequency = 7 argument. library(slid) library(dplyr) data(state.ts) Florida.ts &lt;- state.ts %&gt;% dplyr::filter(State == &quot;Florida&quot;) y &lt;- ts(Florida.ts$Y.Death, start = 2020-01-23, frequency = 7) Simply applying the default nnetar() function, we obtain feed-forward neural networks model NNAR\\((21,1,11)_7\\). It is based on an average of 20 networks, each of which is NNAR\\((21,1,11)_7\\) with 254 weights. library(forecast) set.seed(2020) fit &lt;- nnetar(y) fit ## Series: y ## Model: NNAR(21,1,11)[7] ## Call: nnetar(y = y) ## ## Average of 20 networks, each of which is ## a 21-11-1 network with 254 weights ## options were - linear output units ## ## sigma^2 estimated as 6.598 autoplot(forecast(fit, h = 14), xlab = &quot;Days&quot;, ylab = &quot;Daily new deaths&quot;) Figure 12.7: Two weeks ahead forecast of the daily new death count for Florida using nnetar. We can also specify the repeats and size options in the nnetar function. The repeats option of nnetar shows the number of networks (default = 20) to fit with different random starting weights. These networks are then averaged when producing forecasts. The size option provides number of nodes in the hidden layer. Default is half of the number of input nodes (including external regressors, if given) plus 1. We can consider a Box-Cox transformation for the data. For example, if we want to restrict to positive values, we can consider a Box-Cox transformation parameter lambda. If lambda = auto, then a transformation is automatically selected using BoxCox.lambda. The transformation is ignored if NULL. Otherwise, data transformed before model is estimated. fit &lt;- nnetar(y + 1, lambda = 0, size = 5) autoplot(forecast(fit, h = 14), xlab = &quot;Days&quot;, ylab = &quot;Daily new deaths&quot;) Figure 12.8: Two weeks ahead forecast of the daily new death count for Florida using nnetar with predictions constrained to positive values. Note that the NNAR model is a nonlinear autogressive model, and it is not possible to analytically derive prediction intervals. fcast &lt;- forecast(fit, PI = TRUE, h = 14, npaths = 500) autoplot(fcast, xlab = &quot;Days&quot;, ylab = &quot;Daily new deaths&quot;) Figure 12.9: Two weeks ahead forecast and prediction intervals of the daily new death count for Florida using nnetar with predictions constrained to positive values. Because it is a little slow, PI = FALSE is the default, so prediction intervals are not computed unless requested. The npaths argument in forecast.nnetar controls how many simulations are done (default 1000). By default, the errors are drawn from a normal distribution. The bootstrap argument allows the errors to be “bootstrapped” (i.e., randomly drawn from the historical errors). "],["ensemble.html", "Chapter 13 Hybrid Mdels 13.1 Ensembling Time Series Models 13.2 Model Diagnostics 13.3 Forecasting 13.4 Weights Selection Using Cross Validation", " Chapter 13 Hybrid Mdels In the previous chapters, we introduced some time series models, an autoregressive model (ARIMA), an exponential smoothing state space model (ETS), a neural network autoregression model (NNAR). Hybrid models are identified by using the hybridModel function included in the package forecastHybrid (in R environment). Installation The stable release of the package is hosted on CRAN and can be installed as usual. # install.packages(&quot;forecastHybrid&quot;) # Load the package library(forecastHybrid) 13.1 Ensembling Time Series Models In statistics and machine learning, ensemble methods use multiple forecasting algorithms to improve the predictive performance. Ensemble forecasting can overcome accuracy of simple prediction and to avoid possible overfit. The idea is that, when there is much uncertainty in finding the best model as is the case in many applications, combining may reduce the instability of the forecast and therefore improve prediction accuracy. In a linear combination technique, the combined forecast for the associated time series is calculated through a linear function of the individual forecasts from the contributing models. Let, \\(\\{y_1, y_2, \\ldots, y_n\\}\\) be the actual time series, which is to be forecasted using \\(K\\) different models and \\(\\{\\hat{y}_{n+1}^{(k)}, \\hat{y}_{n+2}^{(k)}, \\ldots, \\hat{y}_{h}^{(k)}\\}\\) be its forecast obtained from the \\(k\\)th model (\\(k=1, 2,\\ldots, K\\)). Then, a linear combination of these \\(K\\) forecasted series of the original time series produces \\(\\{\\hat{y}_{n+1}^{*}, \\hat{y}_{n+2}^{*}, \\ldots, \\hat{y}_{h}^{*}\\}\\), where \\[ \\hat{y}_{n+j}^{*}=f\\left(\\hat{y}_{n+j}^{(1)},\\ldots,\\hat{y}_{n+j}^{(K)}\\right) \\] for \\(j=1,2,\\ldots,h\\). If \\(f\\) is some linear function, then we have \\[ \\hat{y}_{n+j}^{*}=w_1\\hat{y}_{n+j}^{(1)}+w_2\\hat{y}_{n+j}^{(2)}+\\cdots+w_K\\hat{y}_{n+j}^{(K)} =\\sum_{k=1}^{K}w_k\\hat{y}_{n+j}^{(k)}, \\] for \\(j=1,2,\\ldots,h\\). Here, \\(w_k\\) is the weight assigned to the \\(k\\)th forecasting method. To ensure unbiasedness, it is often assumed that the weights add up to unity. Some widely used linear combination techniques are briefly described below: In the simple average, all models are assigned equal weights, i.e., \\(w_k=1⁄K\\), for \\(k=1, 2,\\ldots, K\\). In the trimmed average, individual forecasts are combined by a simple arithmetic mean, excluding the worst performing \\(\\alpha\\)% of the models. A trimming of 10% ~ 30% is usually recommended. In the Winsorized average, the \\(m\\) smallest and \\(m\\) largest forecasts are selected and respectively set as the \\((m+1)\\)th smallest and \\((m+1)\\)th largest forecasts. In the median-based combining, the combination function \\(f\\) is the median of the individual forecasts. Median is sometimes preferred over simple average as it is less sensitive to extreme values. In the error-based combining, the weight to each model is assigned to be the inverse of the past forecast error (e.g. MSE, MAE, MAPE, etc.) of the corresponding model. The hybridModel function in the “forecastHybrid” R package ensembles forecasts in R combining approaches from the ‘forecast’ package. Forecasts generated from the following models: auto.arima(): the best ARIMA model according to either AIC, AICc or BIC value; ets(): exponential smoothing state space model; stlm(): applies an STL decomposition, and models the seasonally adjusted data using the model passed as modelfunction or specified using method; nnetar(): feed-forward neural networks with a single hidden layer and lagged inputs; tbats(): TBATS model (exponential smoothing state space model with Box-Cox transformation, ARMA errors, trend and seasonal components) proposed by De Livera, Hyndman, and Snyder (2011); thetaf(): the theta method proposed by Assimakopoulos and Nikolopoulos (2000), which is equivalent to simple exponential smoothing with drift. snaive(): Seasonal naive method (random walk method) These models can be combined with equal weights, weights based on in-sample errors introduced by Bates and Granger (1969) or cross-validated weights. Cross validation for time series data with user-supplied models and forecasting functions is also supported to evaluate model accuracy. We consider the daily new death count for Forida. To use the “forecastHybrid” package, it is better to store the time series as a “ts” object in R. Let us consider the time series of daily new death counts for Florida, and we first turn this into a “ts” object using the ts() function. Since we have observed the 7-day cycle for this data, we can simply add a frequency = 7 argument. library(slid) library(dplyr) data(state.ts) Florida.ts &lt;- state.ts %&gt;% dplyr::filter(State == &quot;Florida&quot;) y &lt;- ts(Florida.ts$Y.Death, start = 2020-01-23, frequency = 7) Using the default setting, we simply apply the hybridModel() function, and obtain the combined forecast from auto.arima, ets, thetam, nnetar, stlm, and tbats model. hmod &lt;- hybridModel(y) forecast(hmod) ## Point Forecast Lo 80 Hi 80 Lo 95 ## 2042.286 106.59821 59.99658 154.8136 39.379958 ## 2042.429 92.79875 25.58865 136.8427 4.158083 ## 2042.571 104.66484 16.99737 149.2371 -4.861073 ## 2042.714 132.91459 76.29264 177.2457 55.328106 ## 2042.857 128.86821 66.55646 176.3123 45.370109 ## 2043.000 133.53789 72.11097 182.2055 50.721112 ## 2043.143 145.27774 76.98625 213.7300 53.494111 ## 2043.286 127.33009 56.08554 202.2511 32.202438 ## 2043.429 109.45873 21.00009 180.0201 -3.267690 ## 2043.571 122.95030 12.40980 218.9806 -12.236645 ## 2043.714 149.76991 72.15436 236.0417 46.834814 ## 2043.857 143.77561 62.19004 224.8698 36.608705 ## 2044.000 148.34727 67.52823 231.8187 41.706667 ## 2044.143 157.04804 73.45265 248.4459 47.346402 ## Hi 95 ## 2042.286 174.3183 ## 2042.429 156.7485 ## 2042.571 169.5050 ## 2042.714 194.6245 ## 2042.857 192.1269 ## 2043.000 203.3384 ## 2043.143 215.3923 ## 2043.286 204.3949 ## 2043.429 182.9325 ## 2043.571 221.2806 ## 2043.714 238.1171 ## 2043.857 226.8261 ## 2044.000 234.0601 ## 2044.143 250.3336 plot(forecast(hmod), xlab = &quot;Days&quot;, ylab = &quot;Daily new deaths&quot;, main = &quot;Ensembled forecast from different models&quot;) Figure 13.1: Forecast from auto.arima, ets, thetam, nnetar, stlm, and tbats model. The individual component models are stored inside the hybridModel objects and be can viewed in their respective slots, and all the regular methods from the forecast package could be applied to these individual component models. #View the individual models hmod$auto.arima ## Series: y ## ARIMA(1,0,1)(0,1,1)[7] ## ## Coefficients: ## ar1 ma1 sma1 ## 0.9627 -0.7927 -0.5724 ## s.e. 0.0198 0.0396 0.0614 ## ## sigma^2 estimated as 883.9: log likelihood=-1524.9 ## AIC=3057.8 AICc=3057.93 BIC=3072.83 # See forecasts from the auto.arima model plot(forecast(hmod$auto.arima), xlab = &quot;Days&quot;, ylab = &quot;Daily new deaths&quot;) Figure 13.2: Forecast from auto.arima model. 13.2 Model Diagnostics The hybridModel() function produces an S3 object of class forecastHybrid. The print() and summary() methods print information about the ensemble model including the weights assigned to each individual component model. print(hmod) ## Hybrid forecast model comprised of the following models: auto.arima, ets, thetam, nnetar, stlm, tbats ## ############ ## auto.arima with weight 0.167 ## ############ ## ets with weight 0.167 ## ############ ## thetam with weight 0.167 ## ############ ## nnetar with weight 0.167 ## ############ ## stlm with weight 0.167 ## ############ ## tbats with weight 0.167 summary(hmod) ## Length Class Mode ## auto.arima 18 forecast_ARIMA list ## ets 19 ets list ## thetam 23 thetam list ## nnetar 15 nnetar list ## stlm 9 stlm list ## tbats 25 tbats list ## weights 6 -none- numeric ## frequency 1 -none- numeric ## x 324 ts numeric ## xreg 3 -none- list ## models 6 -none- character ## fitted 324 -none- numeric ## residuals 324 ts numeric Two types of plots can be created for the created ensemble model: either a plot showing the actual and fitted value of each component model on the data or individual plots of the component models as created by their regular S3 plot() methods. Note that a plot() method does not exist in the “forecast” package for objects generated with stlm(), so this component model will be ignored when type = \"models\", but the other component models will be plotted regardless. plot(hmod, type = &quot;fit&quot;) Figure 13.3: Actual and fitted value of each component model. plot(hmod, type = &quot;models&quot;) Figure 13.4: the actual and fitted value of each component model. Figure 13.5: the actual and fitted value of each component model. Figure 13.6: the actual and fitted value of each component model. Figure 13.7: the actual and fitted value of each component model. 13.3 Forecasting Now’s let’s forecast future values. The forecast() function produce an S3 class forecast object for the next 14 days from the ensemble model. The prediction intervals are preserved from the individual component models and currently use the most extreme value from an individual model, producing a conservative estimate for the ensemble’s performance. hfc &lt;- forecast(hmod, h = 14) # View the point forecasts hfc$mean ## Time Series: ## Start = c(2042, 3) ## End = c(2044, 2) ## Frequency = 7 ## [1] 106.59821 92.79875 104.66484 132.91459 128.86821 ## [6] 133.53789 145.27774 127.33009 109.45873 122.95030 ## [11] 149.76991 143.77561 148.34727 157.04804 # View the upper prediction interval hfc$upper ## 80% 95% ## [1,] 154.8136 174.3183 ## [2,] 136.8427 156.7485 ## [3,] 149.4553 169.5050 ## [4,] 176.9789 194.6245 ## [5,] 176.0517 192.1269 ## [6,] 181.4409 203.3384 ## [7,] 213.5458 216.2529 ## [8,] 201.9121 204.1527 ## [9,] 180.0530 182.6926 ## [10,] 218.6720 221.5546 ## [11,] 235.7841 237.3367 ## [12,] 225.0711 227.1369 ## [13,] 231.8018 234.0672 ## [14,] 248.0386 250.0807 # View the lower prediction interval hfc$lower ## 80% 95% ## [1,] 59.99658 39.379958 ## [2,] 25.58865 4.158083 ## [3,] 16.99737 -4.861073 ## [4,] 76.29264 55.328106 ## [5,] 66.55646 45.370109 ## [6,] 72.11097 50.721112 ## [7,] 76.98625 53.494111 ## [8,] 56.08554 32.202438 ## [9,] 21.00009 -3.267690 ## [10,] 12.40980 -12.236645 ## [11,] 72.15436 46.834814 ## [12,] 62.19004 36.608705 ## [13,] 67.52823 41.706667 ## [14,] 73.45265 47.346402 Now, we plot the forecast for the next 14 days with the prediction intervals. # Plot the forecast plot(hfc, xlab = &quot;Days&quot;, ylab = &quot;Daily new deaths&quot;, main = &quot;Ensembled forecast from different methods&quot;) Figure 13.8: Two weeks ahead ensemble forecast from different methods. After the model is fit, these weights are stored in the weights attribute of the model. The user can view and manipulated these weights after the fit is complete. Note that the hybridModel() function automatically scales weights to sum to one, so a user should similar scale the weights to ensure the forecasts remain unbiased. Furthermore, the vector that replaces weights must retain names specifying the component model it corresponds to since weights are not assigned by position but rather by component name. Similarly, individual components may also be replaced. hmod$weights ## auto.arima ets thetam nnetar stlm ## 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 ## tbats ## 0.1666667 13.3.1 Perform cross validation on a time series It can be useful to perform cross validation on a forecasting model to estimate a model’s out-of-sample forecasting performance. Cross validation of time series data is more complicated than regular k-folds or leave-one-out cross validation of datasets without serial correlation since observations \\(x_t\\) and \\(x_{t+n}\\) are not independent. The cvts() function overcomes this obstacle using two methods: 1) rolling cross validation where an initial training window is used along with a forecast horizon and the initial window used for training grows by one observation each round until the training window and the forecast horizon capture the entire series; or 2) a non-rolling approach where a fixed training length is used that is shifted forward by the forecast horizon after each iteration. For example, let’s perform cross validation for a stlm() model and a naive() model on the woolyrnq time series. The most important cvts() arguments that commonly need adjusting are rolling (if TRUE, the model will always be fit on a fixed windowSize instead of growing by one new observation for each new model fit during cross validation), windowSize (starting length of time series to fit a model), and maxHorizon (the forecast horizon for predictions from each model). Since a naive forecast is a good baseline that any decent model should surpass, let’s see how the stlm() model compares. stlmMod &lt;- cvts(y, FUN = stlm, windowSize = 100, maxHorizon = 7) naiveMod &lt;- cvts(y, FUN = naive, windowSize = 100, maxHorizon = 7) accuracy(stlmMod) ## ME RMSE MAE ## Forecast Horizon 1 -0.9068629 31.27580 23.69878 ## Forecast Horizon 2 -2.6490705 43.91947 27.06993 ## Forecast Horizon 3 -1.2864292 37.85213 28.29804 ## Forecast Horizon 4 -2.2917024 36.27865 27.61824 ## Forecast Horizon 5 -2.9254922 30.23136 22.62086 ## Forecast Horizon 6 -6.4138820 51.77928 35.64043 ## Forecast Horizon 7 -1.7238578 38.63659 29.09958 accuracy(naiveMod) ## ME RMSE MAE ## Forecast Horizon 1 -20.78125 39.26791 28.46875 ## Forecast Horizon 2 -58.06250 74.62448 61.87500 ## Forecast Horizon 3 -52.71875 68.61054 54.09375 ## Forecast Horizon 4 5.06250 35.96526 27.18750 ## Forecast Horizon 5 6.09375 34.76394 27.09375 ## Forecast Horizon 6 6.37500 52.37246 37.00000 ## Forecast Horizon 7 2.37500 39.79871 27.31250 13.4 Weights Selection Using Cross Validation Previously we explored fitting hybridModel() objects with weights = \"equal\" or weights = \"insample.errors\", but we can now leverage the process conducted in cvts() to select the appropriate weights intelligently based on the expected out-of-sample forecast accuracy of each component model. While this is the methodologically-sound weight procedure, it also comes at significant computational cost since the cross validation procedure necessitates fitting each model several times for each cross validation fold in addition to the final fit on the whole dataset. Fortunately this process can be conducted in parallel if multiple cores are available. Some of the arguments explained above in cvts() such as windowSize and the cvHorizon can also be controlled here. cvMod &lt;- hybridModel(y, weights = &quot;cv.errors&quot;, windowSize = 100, cvHorizon = 8, num.cores = 4) cvMod "],["references.html", "References", " References "],["appendix.html", "Appendix", " Appendix "],["appendix-a.html", "Chapter 14 Appendix A 14.1 R Introduction and Preliminaries 14.2 Starting R 14.3 Export/Import Data", " Chapter 14 Appendix A This Appendix willl introduce you to the basics programming skills in R that are generally unrelated to the use of R as a statistical software such as downloading, reading, manipulating and writing data. 14.1 R Introduction and Preliminaries 14.1.1 The R Environment and Language R is an integrated suite of software facilities for data manipulation, calculation and graphical display. The benefits of R for an introductory student R is free. R is open-source and runs on UNIX, Windows and Macintosh. R has an excellent built-in help system. R has excellent graphing capabilities. Students can easily migrate to the commercially supported S-Plus program if commercial software is desired. R’s language has a powerful, easy to learn syntax with many built-in statistical functions. The language is easy to extend with user-written functions. R is a computer programming language. For programmers it will feel more familiar than others and for new computer users, the next leap to programming will not be so large. What is R lacking compared to other software solutions? There is no commercial support. (Although one can argue the international mailing list is even better) The command language is a programming language so students must learn to appreciate syntax issues etc. R can be regarded as an implementation of the S language which was developed at Bell Laboratories by Rick Becker, John Chambers and Allan Wilks, and also forms the basis of the S-Plus systems. 14.1.2 R and Statistics Many people use R as a statistics system. We prefer to think of it as an environment within which many classical and modern statistical techniques have been implemented. A few of these are built into the base R environment, but many are supplied as packages. There are about 25 packages supplied with R (called “standard” and “recommended” packages) and many more are available through the CRAN family of Internet sites (via http://CRAN.R-project.org) and elsewhere. More details on packages are given later. Most classical statistics and much of the latest methodology is available for use with R, but users may need to be prepared to do a little work to find it. 14.1.3 Obtaining R and installation Obtaining R Sources, binaries, and documentation for R can be obtained via CRAN, the “Comprehensive R Archive Network” whose current members are listed at http://cran.r-project.org/mirrors.html. Installing R under Windows (via http://CRAN.R-project.org) The bin/windows directory of a CRAN site contains binaries for a base distribution and many add-on packages from CRAN to run on Windows 2000 or later on ix86 CPUs (including AMD64/EM64T chips and Windows x64). Your file system must allow long file names (as is likely except perhaps for some network-mounted systems). Installation is straightforward. Just double-click on the icon and follow the instructions. You can uninstall R from the Control Panel or the (optional) R program group on the Start Menu. Installing R under Macintosh (via http://CRAN.R-project.org) Visit the Comprehensive R Archive Network (CRAN) and select a mirror site near you; a list of CRAN mirrors appears at the upper left of the CRAN home page. Click on the link Download R for Mac OS X, which appears near the top of the page; then click on R-X.X.X.pkg (or whatever is the current version of R), which assumes that you are using Mac OS X 10.9 (Mavericks) or higher. You will also find an older version of R if you have an older version of Mac OS X (10.6, Snow Leopard, or higher). Once it is downloaded, double-click on the R installer. You may take all of the defaults. Installing RStudio After you install R, you can install R Studio. Download and install RStudio at https://www.rstudio.com/products/rstudio/download/. Scroll down to “Installers for Supported Platforms” near the bottom of the page. Click on the download link corresponding to your computer’s operating system. 14.2 Starting R RStudio is most easily used in an interactive manner. After installing R and RStudio on your computer, you’ll have two new programs (also called applications) you can open. We’ll always work in RStudio and not in the R application. Figure 14.1 below shows what icon you should be clicking on your computer. Figure 14.1: Icons of R versus RStudio on your computer. After you open RStudio, you should see something similar to Figure 14.2 below. Figure 14.2: RStudio interface to R. Note the three panels divide the screen: the console panel, the files panel, and the environment panel. Throughout this chapter, you’ll come to learn what purpose each of these panels serves. Console: This is the place to write any code that needs to be run. Environment: This lists what variables and objects (referred to in R) are currently available in your working environment. Within the environment window, there are also other tabs such as ‘history,’ which shows a history of all code typed in the past. It also has a tab called ‘connection,’ which is meant for connecting to specific databases. This tab is not useful to a beginner. Viewer: For lack of a better way to refer to the third pane, it is referred to here as ‘viewer.’ However, the third pane has several tabs nested within it. The “files” tab shows all the files and folders in your current directory, which the program points to right next to the home icon below the header for the pane. The “plots” tab shows and allows for the saving of any plot output. The “packages” tab shows all the packages that are currently installed. As you start using R-Studio, you will find the need to install many packages and R-Studio makes it easy to do so. 14.2.1 Description of three panels in user interface R Console window The R Environment contains the software’s libraries with all the available datasets, expansion packages and macros. As compared to SAS, the Log and Editor windows are consolidated into a single interface, the “R Console.” Figure 14.3: The R Console. Note: The &gt; is called the prompt. In what follows below it is not typed, but is used to indicate where you are to type if you follow the examples. The Console can be used like a calculator. Below are some examples: 2 + 2 ## [1] 4 (2 - 3) / 6 ## [1] -0.1666667 2 ^ 2 ## [1] 4 sin(pi / 2) ## [1] 1 log(1) ## [1] 0 Results from these calculations can be stored in an object. The &lt;- is used to make the assignment and is read as “gets.” save &lt;- 2 + 2 save The objects are stored in R’s database. When you close R, you will be asked if you would like to save or delete them. This is kind of like the SAS WORK library, but R gives you a choice to save them. To see a listing of the objects, you can do either of the following: ls() objects() To delete an object, use rm() and insert the object name in the parentheses. rm(x, y, z, ink, junk, temp, foo, bar) rm(list=ls()) cleans out all objects from your work space. All objects created during an R session can be stored permanently in a file for use in future R sessions. At the end of each R session, you are given the opportunity to save all the currently available objects. If you indicate that you want to do this, the objects are written to a file called .RData in the current directory (“Save Workspace”), and the command lines used in the session are saved to a file called .Rhistory (“Save History”). save(x, y, z, file = &quot;objects.rdata&quot;) saves objects x, y, z to the file “objects.rdata” in your working directory. load(&quot;objects.rdata&quot;) loads the objects in file “objects.rdata.” R Editor window – type your long R program here Often, you will have a long list of commands that you would like to execute all at once, i.e., a program. Instead of typing all of the code line by line at the R Console, you could type it in the R Script Window. Select File -&gt; New File -&gt; R script to create a new program. Below is what the editor looks like. To run the current line of the code (where the cursor is positioned) or some code highlighted, click “Run.” To save your code as a program outside of R, select File -&gt; Save and make sure to use an .R extension on the file name. Error messages R will provide intuitive error messages regarding the submitted syntax. Unlike in SAS these comments are printed right in the console. 2 + 2 2 + 2 + (3xz 2 + (3 * z) R will provide intuitive error messages regarding the submitted syntax. Unlike in SAS these comments are printed right in the console. 14.2.2 R help To see a listing of all R functions which are “built in,” open the Help by selecting Help -&gt; R Help from the main menu bar. Under Reference, select the link called Packages. All built-in R functions are stored in a package. We have been using functions from the base and stats package. By selecting stats, you can scroll down to find help on the pnorm() function. Note the full syntax for pnorm() is pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) The q value corresponds to the 1.96 can be found by pnorm(1.96) ## [1] 0.9750021 pnorm(q = 1.96) ## [1] 0.9750021 pnorm(q = 1.96, mean = 0, sd = 1) ## [1] 0.9750021 These produce the same results. The other entries in the function have default values set. For example, R assumes you want to work with the standard normal distribution by assigning mean=0 and sd=1 (standard deviation). If you know the exact name of the function, simply type help(function name) at the R Console command prompt to bring up its help in a window inside of R. For example, help(pnorm) brings up Figure 14.4: Figure 14.4: The R help for the R function pnorm(). An alternative is ?pnorm For a feature specified by special characters, the argument must be enclosed in double or single quotes, making it a “character string”: This is also necessary for a few words with syntactic meaning including if and for functions. help(&quot;pnorm&quot;) If you need to use a function but don’t know its exact name or are not sure of its existence. There is a very useful function called apropos(‘argument‘) that lists all functions that contain your argument as part of their names. Note that your argument must be put within either single or double quotation marks. For example, here is what I got when I looked for similar functions containing the string table: apropos(&#39;table&#39;) ## [1] &quot;.S3_methods_table&quot; &quot;[.table&quot; ## [3] &quot;add_table&quot; &quot;aperm.table&quot; ## [5] &quot;as.data.frame.table&quot; &quot;as.relistable&quot; ## [7] &quot;as.table&quot; &quot;as.table.default&quot; ## [9] &quot;asTable&quot; &quot;covariate_table&quot; ## [11] &quot;db_create_table&quot; &quot;db_drop_table&quot; ## [13] &quot;db_has_table&quot; &quot;db_list_tables&quot; ## [15] &quot;db_write_table&quot; &quot;ftable&quot; ## [17] &quot;ggplot_gtable&quot; &quot;html_dependency_bsTable&quot; ## [19] &quot;html_dependency_lightable&quot; &quot;is.relistable&quot; ## [21] &quot;is.table&quot; &quot;margin.table&quot; ## [23] &quot;melt_table&quot; &quot;melt_table2&quot; ## [25] &quot;model.tables&quot; &quot;pairwise.table&quot; ## [27] &quot;print.summary.table&quot; &quot;print.table&quot; ## [29] &quot;prop.table&quot; &quot;r2dtable&quot; ## [31] &quot;read_table&quot; &quot;read_table2&quot; ## [33] &quot;read.ftable&quot; &quot;read.table&quot; ## [35] &quot;spec_table&quot; &quot;spec_table2&quot; ## [37] &quot;summary.table&quot; &quot;table&quot; ## [39] &quot;table1&quot; &quot;table2&quot; ## [41] &quot;table3&quot; &quot;table4a&quot; ## [43] &quot;table4b&quot; &quot;table5&quot; ## [45] &quot;write.ftable&quot; &quot;write.table&quot; ## [47] &quot;xtable2kable&quot; &quot;xyTable&quot; Note that the argument is a string, so it does not need to be an actual word or name of a function. For example, apropos('tabl') will return the same results. Try it! There may be other times when you want to learn about all functions involving a certain term, but searching for R-related pages on that term returns too many irrelevant results. This term may not even be an R function or command, making the Google search all the more difficult, even with good searching techniques. In these situations, use the help.search(‘argument‘) function. (Again, you need to put your arguments around single or double quotation marks.) This will return all functions with your argument in the help page title or as an alias. For example, I wanted to know about using PDF files in R. I ran help.search(‘pdf’) in R and got the following results. help.search(&quot;pdf&quot;) 14.2.3 Some mathematical expressions Try the following expressions: -2^.5 -2**.5 -(2^.5) (-2)^.5 The first three will give same result. The last produces NaN, not a number. In EXCEL the first produces an error (also in C). It is interpreted just as the 4th expression above. When in doubt, use ( ) to enforce proper order of evaluation. Next try the following factorial(5) produces \\(5! = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 = 120\\); choose(8,4) produces \\(\\binom{8}{4}=70\\), and sqrt(2) gives \\(\\sqrt{2}=1.414214\\). We will learn more mathematical and statistical expressions in the next few chapters. 14.2.4 R Packages This is a very important topic in R. In SAS and SPSS installations, you usually have everything you have paid for installed at once. R is much more modular. The main installation will install R and a popular set of add‐ons called libraries. Hundreds of other libraries are available to install separately from the Comprehensive R Archive Network, (CRAN). Right under the “viewer” tab, the icon for “setting” allows for changing the working directory or copying and moving files. The tab for “packages,” shows all the packages that are installed and available. Clicking on the checkbox next to the name of the package loads the package for use using the following command, which will appear in the console pane of the interface. Library(Package name) Clicking on the name of the package (under the package tab on the lower right pane) itself brings up the description of what the package does. This is one of the benefits of using R-Studio as opposed to R, which makes it easy to look up all the packages available and what each does. Although the description and examples for packages are sometimes not explicit enough, it is nevertheless a useful starting point for many tasks. The viewer pane tab also makes it easy to install and update packages right from the lower right panel of the user interface. If you want to use functions in other packages, you may need to install and then load the package into R. Packages if they have already been downloaded from a CRAN mirror site can be loaded using this procedure. If the package has not been downloaded, it can be installed using the install.packages(package name) option. Also, an installed package can be loaded by specifying library(name of package). For example, we will be using the ggplot2 package later for data visualization. While in the R console, select Tools -&gt; Install Packages from the main menu. A number of locations around the world will come up. Insert the ggplot2 package and select Install. The package will now be installed onto your computer. This only needs to be done once on your computer. To load the package into your current R session, type library(ggplot2) at the R Console prompt. This needs to be done only once in an R session. If you close R and reopen, you will need to use the library() function again. If the package contains example data sets, you can load them with the data command. Enter data() to see what is available and then data(mydata) to load one named, for example, mydata. Clicking on the link to the package name from within the “packages” tab in the viewer pane provides an overview of what the package does. Here it is useful to spend some time understanding how to use a package. Clicking on the link provides details on its documentation. Next to the package is the package title, which states, “Create Elegant Data Visualisations Using the Grammar of Graphics.” Thus, ggplot2 is a package that makes elegant data visualizations. Clicking on the package link provides an alphabetized list of all that the package does, such as a function called aes that helps construct aesthetic mappings. Another function called borders helps to create a layer of map borders. Clicking on the link aes provides an explanation for what the function does and the way (syntax) it is used. Understanding the structure of this description helps us understand how to use packages. The description of the function within the package has several parts to it as follows: Description: Aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms. Aesthetic mappings can be set in ggplot() and in individual layers. Usage: The second aspect of the description of the function refers to its usages. Arguments: The third part of the structure of a function is referred to as arguments. This describes the objects or variables that this function will operate on. Example: Typically, any description of what a function does is accompanied by an example of how to use it. Thus, understanding how to install a package, the functions it is capable of, along with the examples its description provides makes R very versatile for the user. In the next chapter, we will learn how to use R and its basic functions. 14.2.5 Creating a project and setting working directory Before launching into creating a dataset, it is important to understand how R handles data from a filing and directory perspective. Before creating a dataset, R starts with the creation of a ‘new project.’ A project name is a name given to a folder that will hold everything associated with a specific project such as data, history of commands used, objects (In R, a variable and/or data are stored as “objects”), or variables are created. Along with creating a new project name, it is important to understand the concept of the working directory as R will look for variables and objects or any other files that are being called in the working directory. A simple way to check on the current working directory is to type the command getwd() into the command console. If it is not the intended directory you want to use, the simplest way to change it is by using the “viewer” pane and clicking on the tab that says ‘more.’ Ensuring that your working directory is where you want your files and objects created to be stored is important, especially to a beginner. You may now create a new project by opening R-Studio, clicking on the file, and then ‘new project.’ As shown in Figure 14.5 that opens asks if you would like to open the project in an existing directory, a new one, or simply version control. The existing directory is the directory that is currently the working directory. You may either choose an existing directory or a new one. However, if you choose a new one, you need to make sure that it is selected as the working directory as shown earlier. You may name your project as ‘Learning R.’ Figure 14.5: Interface for creating a project in R. Once the project is created, you will see a .proj file under the files section in the viewer pane, as shown in Figure 14.6. As the figure shows, a new project called ‘Learning R.rproj’ has been created in the directory ~/STAT 480/Learning R. Any work done will now be stored in this directory (by setting it as the working directory) and in this project as long as it is saved when you exit. Figure 14.6: Directory with new project. 14.3 Export/Import Data In this section, you’ll learn how to read plain-text rectangular files into R, and how to export data from R to txt, csv, and R data file formats. First, we need is an idea of where the files are stored with R and how to manipulate those files. Every R session has a default location on your operating system’s file structure called the working directory. You need to keep track and deliberately set your working directory in each R session. If you read or write files to disk, this takes place in the working directory. If you don’t set the working directory to your desired location, you could easily write files to an undesirable file location. Working Directory The getwd() function tells you what the current working directory is: getwd() To change the working directory, use the setwd() function. Be sure to enter the working directory as a character string. This example shows how to change your working directory to a folder called “F:/STAT480/R”: setwd(&quot;F:/STAT480/R&quot;) getwd() Note that the separator between folders is “/,” as it is on Linux and Mac systems. When working in Windows, you need to either use “/” or “” 14.3.1 Data Export Using function cat() The function cat is useful for producing output in user-defined functions. cat(&quot;Good morning!&quot;,&quot;\\n&quot;) #\\n: newline ## Good morning! cat(file = &quot;test.txt&quot;, &quot;123456&quot;, &quot;987654&quot;, sep = &quot;\\n&quot;) Using function print() The function print prints its argument. It is a generic function which means that new printing methods can be easily added for new classes. print(&quot;Good morning!&quot;) ## [1] &quot;Good morning!&quot; Write a matrix or data frame to file The commonest task is to write a matrix or data frame to file as a rectangular grid of numbers, possibly with row and column labels. This can be done by the functions write.table and write. Command to copy and paste from R into Excel or other programs. It writes the data of an R data frame object into the clipboard from where it can be pasted into other applications. age &lt;- 18:29 height &lt;- c(76.1, 77, 78.1, 78.2, 78.8, 79.7, 79.9, 81.1, 81.2, 81.8, 82.8, 83.5) village &lt;- data.frame(age = age,height = height) # Write village into clipboard write.table(village, &quot;clipboard&quot;, sep = &quot;\\t&quot;, col.names = NA, quote = F) Remark: The argument quote is a logical value (TRUE or FALSE) or a numeric vector. If TRUE, any character or factor columns will be surrounded by double quotes. If FALSE, nothing is quoted. The argument col.names= NA makes sure that the titles align with columns when row/index names are exported (default). Write data frame to a tab-delimited text file. write.delim(village, file = &quot;village.txt&quot;) # provides same results as read.delim write.table(village, file = &quot;village.txt&quot;, sep = &quot;\\t&quot;) Write data to csv files: write.csv(village, file = &quot;village.csv&quot;) Write matrix data to a file. x &lt;- matrix(1, 20, 20) write(x, file = &quot;file path&quot;, ncolumns = 20) Remark: write.table() is the multipurpose work-horse function in base R for exporting data. The functions write.csv() and write.delim() are special cases of write.table() in which the defaults have been adjusted for efficiency. 5 .RData files The best way to store objects from R is with .RData files. .RData files are specific to R and can store as many objects as you’d like within a single file. ** The save() function age &lt;- 18:29 height &lt;- c(76.1, 77, 78.1, 78.2, 78.8, 79.7, 79.9, 81.1, 81.2, 81.8, 82.8, 83.5) village &lt;- data.frame(age = age, height = height) # Save the object as a new .RData file save(village, file = &quot;data/village.rda&quot;) To save selected objects into one .RData file, use the save() function. When you run the save() function with specific objects as arguments, (like save(a, b, c, file = \"myobjects.RData\") all of those objects will be saved in a single file called myobjects.RData. For example, let’s create a few objects corresponding to a study. # Create two objects student.df &lt;- data.frame(id = 1:5, sex = c(&quot;m&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;, &quot;m&quot;), score = c(90, 80, 97, 62, 82)) score.by.sex &lt;- aggregate(score ~ sex, FUN = mean, data = student.df) # Save two objects as a new .RData file save(student.df, score.by.sex, file = &quot;data/student.rda&quot;) 14.3.2 Data Import The load() function To load an .RData file, that is, to import all of the objects contained in the .RData file into your current workspace, use the load() function. For example, to load the three specific objects that I saved earlier (study1.df, score.by.sex) in study1.rda, I’d run the following: # Load objects in village.rda into my workspace load(file = &quot;data/village.rda&quot;) To load all of the objects in the workspace that we have saved to the data folder in a working directory named projectnew.rda, we can run the following: # Load objects in projectnew.rda into my workspace load(file = &quot;data/projectnew.rda&quot;) The read.table() function Large data objects will usually be read as values from external files rather than entered during an R session at the keyboard. R input facilities are simple, and their requirements are fairly strict and even rather inflexible. If variables are to be held mainly in data frames, as we strongly suggest they should be, an entire data frame can be read directly with the read.table() function. There is also a more primitive input function, scan(), that can be called directly. For more details on importing data into R and also exporting data, see the R Data Import/Export manual. To read an entire data frame directly, the external file will typically have a special form. The first line of the file should have a name for each variable in the data frame. Each additional line of the file has its first item, a row label, and the values for each variable. By default, numeric items (except row labels) are read as numeric variables and non-numeric variables, such as name and gender in the above example, as factors. The function read.table() can then be used to read the data frame directly. For the file, scores_names.txt, you might want to omit, including the row labels, directly and use the default labels. In this case, the file may omit the row label column as in the following. scores &lt;- read.table(&quot;scores_names.txt&quot;, header = TRUE) scores[[&#39;gender&#39;]] scores[[&#39;aptitude&#39;]] where the header=TRUE option specifies that the first line is a line of headings, and hence, by implication from the form of the file, that no explicit row labels are given. This can be changed if necessary. scores &lt;- read.table(&quot;scores_names.txt&quot;, colClasses = c(&quot;character&quot;, &quot;character&quot;, &quot;integer&quot;, &quot;integer&quot;), header = TRUE) &gt; scores[[&#39;gender&#39;]] If the values are separated by commas or another “delimiter,” we have to specify the delaminating character(s). For example, look at the file reading.txt on the Blackboard folder. reading &lt;- read.table(&quot;reading.txt&quot;, sep = &quot;,&quot;) # names() is to get or set the names of an object. names(reading) = c(&quot;Name&quot;, &quot;Week1&quot;, &quot;Week2&quot;, &quot;Week3&quot;, &quot;Week4&quot;, &quot;Week5&quot;) print(reading) If sep = \"\" (the default for read.table) the separator is ‘white space,’ that is one or more spaces, tabs, newlines or carriage returns. The read.csv() function Alternatively, you may have data from a spreadsheet. The simplest way to enter this into R is through a file format. Typically, this is a CSV format (comma-separated values). First, save the data from the spreadsheet as a CSV file say data.csv. Then the R command read.csv will read it in as follows x &lt;- read.csv(file=&quot;data.csv&quot;) CSV file can be comma delimited or tab or any other delimiter specified by parameter sep =. If the parameter header = is TRUE, then the first row will be treated as the row names. read.csv(file, header = FALSE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.csv2(file, header = TRUE, sep = &quot;;&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;,&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) file: file name header: 1st line as header or not, logical sep: field separator quote: quoting characters … The difference between read.csv and read.csv2 is the default field seperator, as “,” and “;” respectively. Following is a csv file example: t1 t2 t3 t4 t5 t6 t7 t8 r1 1 0 1 0 0 1 0 2 r2 1 2 2 1 2 1 2 1 r3 0 0 0 2 1 1 0 1 r4 0 0 1 1 2 0 0 0 r5 0 2 1 1 1 0 0 0 r6 2 2 0 1 1 1 0 0 r7 2 2 0 1 1 1 0 1 r8 0 2 1 0 1 1 2 0 r9 1 0 1 2 0 1 0 1 r10 1 0 2 1 2 2 1 0 r11 1 0 0 0 1 2 1 2 r12 1 2 0 0 0 1 2 1 r13 2 0 0 1 0 2 1 0 r14 0 2 0 2 1 2 0 2 r15 0 0 0 2 0 2 2 1 r16 0 0 0 1 2 0 1 0 r17 2 1 0 1 2 0 1 0 r18 1 1 0 0 1 0 1 2 r19 0 1 1 1 1 0 0 1 r20 0 0 2 1 1 0 0 1 x &lt;- read.csv(&quot;readcsv.csv&quot;, header = T, dec = &quot;.&quot;, sep = &quot;\\t&quot;) is.data.frame(x) Import an Excel file into R You can import an excel file using the readxl package. To start, here is a template that you can use to import an Excel file into R: library(&quot;readxl&quot;) read_excel(&quot;&lt;name and extension of your file&gt;&quot;) If you want to import a specific sheet within the Excel file, you may use this template: library(&quot;readxl&quot;) read_excel(&quot;&lt;name and extension of the file&gt;&quot;, sheet = &quot;sheet name&quot;) If you want to set a three column excel sheet to contain the data as dates in the first column, characters in the second, and numeric values in the third, you would need the following lines of code: library(&quot;readxl&quot;) read_excel(&quot;&lt;name and extension of your file&gt;&quot;, col_types = c(&quot;date&quot;, &quot;numeric&quot;, &quot;text&quot;)) Accessing built-in datasets Around 100 datasets are supplied with R (in package datasets), and others are available in packages (including the recommended packages supplied with R). To see the list of datasets currently available use data(). As from R version 2.0.0 all the datasets supplied with R are available directly by name. AirPassengers However, many packages still use the earlier convention in which data was also used to load datasets into R, for example, data(AirPassengers) and this can still be used with the standard packages. In most cases this will load an R object of the same name. However, in a few cases it loads several objects, so see the on-line help for the object to see what to expect. Loading data from other R packages To access data from a particular package, use the package argument, for example, data(package = &quot;rpart&quot;) data(Puromycin, package = &quot;datasets&quot;) Figure 14.7 shows a subset oof supported file formats. Figure 14.7: A non-inclusive list of supported file formats. "],["appendix-b.html", "Chapter 15 Appendix B 15.1 Epidemic Data 15.2 Other Factors 15.3 Data Sets", " Chapter 15 Appendix B Since the first infected case reported in December 2019, the outbreak of Coronavirus disease (COVID-19) has unfolded across the globe. In the US, coronavirus has infected more than five million people and killed over 160,000 people, as of the time of writing. While essential public health, economic and social science research in measuring and modeling COVID-19 and its effects is underway, reliable and accurate datasets are vital for scientists to conduct related research and for governments to make better decisions (Killeen et al. 2020). Unfortunately, errors could occur in the data collection process, especially under such a pandemic. In this work, we focus on the data collection, comparison, data inconsistency detection, and the corresponding curating. Living through unprecedented times, governments must rely on timely, reliable data to make decisions to mitigate harm and support their citizens. Every day, several volunteer groups and organizations work very hard on collecting data on COVID-19 from all the counties and states in the US. There are four primary sources, including (1) the New York Times (NYT), (2) the COVID Tracking Project at the Atlantic (Atlantic), (3) the data repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (JHU), and (4) USAFacts. We collect the epidemic data up to county level in the US along with control measures and other local information, such as socioeconomic status, demographic characteristics, healthcare infrastructure, and other essential factors to analyze the spatiotemporal dynamic pattern of the spread of COVID-19. Our data covers about 3,200 county-equivalent areas from 50 US states and the District of Columbia. A live version of the data analysis will be continually updated on the COVID-19 US Dashboard and the Github Repository. 15.1 Epidemic Data The daily counts of cases and deaths of COVID-19 are crucial for understanding how this pandemic is spreading. Thanks to the contribution of the data science communities across the world, multiple sources are providing the COVID-19 data with different precision and focus. In this book, we consider the reported cases from the following four sources: the NYT, the Atlantic, the JHU, and the USAFacts. To clean the data, we first fetch data from the above four sources and compile them into the same format for further comparison and cross-validation. 15.1.1 State level In the state level epidemic data, we include the following variables. Among those variables, the variable State can be used as the key for data merge. State: name of state. There are 48 mainland US states and the District of Columbia. XYYYY.MM.DD: cumulative infection or death cases related to the date of YYYY.MM.DD. YYYY, MM, and DD represent year, month and day, re- spectively. It starts from X2020.01.22. For example, the variable X2020.01.22 is either infection or death cases in a certain state (State) on 01/22/2020. 15.1.2 County level For county-level data, two more county-specific variables are included. As the key of this table, variable ID can be used for future data merge. ID: county-level Federal Information Processing System (FIPS) code, which uniquely identifies the geographic area. The number has five digits, of which the first two are the FIPS code of the state to which the county belongs. County: name of county matched with ID. There are about 3,200 counties and county-equivalents (e.g. independent cities, parishes, boroughs) in the US. State: name of state matched with ID. There are 50 states and the District of Columbia in the US. XYYYY.MM.DD: cumulative infection or death cases related to the date of YYYY.MM.DD. YYYY, MM, and DD represent year, month and day, respectively. It starts from X2020.01.22. For example, the variable X2020.01.22 is either infection or death cases in a certain (County) on 01/22/2020. 15.2 Other Factors When analyzing the reported cases of COVID-19, many other factors may also contribute to the temporal or spatial patterns; see the discussions in (Wang et al. 2020). For example, local features, like socioeconomic and demographic factors, can dramatically influence the course of the epidemic, and thus, the spread of the disease could vary dramatically across different geographical regions. Therefore, these datasets are also supplemented with the population information at the county level in our repository. We further classify these factors into the following six groups. 15.2.1 Policy Data In a race to stunt the spread of COVID-19, federal, state and local governments have issued various executive orders. Government declarations are used to identify the dates that different jurisdictions implemented various social distancing policies (emergency declarations, school closures, bans on large gatherings, limits on bars, restaurants and other public places, the deployment of severe travel restrictions, and “stay-at-home” or “shelter-in-place” orders). For example, President Trump declared a state of emergency on March 13, 2020, to enhance the federal government response to confront COVID-19. Later in the past spring, at least 316 million people in at least 42 states, the District of Columbia and Puerto Rico were urged to stay home. Since the late April, all 50 states in the US began to reopen successively, due to the immense pressures of the crippled economy and anxious public. A state is categorized as “reopening” once its stay-at-home order lifts, or once reopening is permitted in at least one primary sector (restaurants, retail stores, personal care businesses), or once reopening is permitted in a combination of smaller sectors. We compiled the dates of executive orders by checking national and state governmental websites, news articles, and press releases. 15.2.2 Demographic Characteristics In the demographic characteristics category, we consider the factors describing racial, ethnic, sexual, and age structures. These variables are extracted from the 2010 Census, and the 2010–2018 American Community Survey (ACS) Demographic and Housing Estimates. Specifically, we include the following six variable AA_PCT: the percent of the population who identify as African American; HL_PCT: the percent of the population who identify as Hispanic or Latino; Old_PCT: the percent of aged people (age &gt;= 65 years); Sex_ratio: the ratio of male over female; PD_log: the logrithm of the population density per square mile of land area; Pop_log: the logarithm of local population; Mortality: the five-year (1998-2002) average mortality rate, measured by the total counts of deaths per 100, 000 population in a county. 15.2.3 Healthcare Infrastructure We also incorporate several features related to the healthcare infrastructure at the county level in the datasets, including the percent of persons under 65 years without health insurance, the local government expenditures for health per capita, and total bed counts per 1,000 population. NHIC_PCT: the percent of persons under 65 years without health insurance EHPC: the local government expenditures for health per capita; TBed: the total bed counts per 1,000 population. 15.2.4 Socioeconomic Status We consider diverse socioeconomic factors in the county level datasets. All of these factors collected from 2005–2009 ACS five-year estimates. Affluence: social affluence generated by factor analysis from HighIncome, HighEducation, WCEmployment and MedHU; HIncome_PCT: the percent of families with annual incomes higher than $75,000; HEducation_PCT: the percent of the population aged 25 years or older with a bachelor’s degree or higher; MedHU: the median value of owner-occupied housing units; Disadvantage: concentrated disadvantage obtained by factor analysis from HHD_PAI_PCT, HHD_F_PCT and Unemployment_PCT; HHD_PAI_PCT: the percent of the households with public assistance income; HHD_F_PCT: the percent of households with female householders and no husband present; Unemployment_PCT: civilian labor force unemployment rate; Gini: the Gini coefficient, a measure for income inequality and wealth distribution in economics. 15.2.5 Environmental Factor We also collect environmental factors that might affect the spread of epidemics significantly, such as the urban rate and crime rate. UrbanRate: urban rate; ViolentCrime: the total number of violent crimes per 1,000 population; PropertyCrime: the total number of property crimes per 1,000 population; ResidStability: the percent of the population residence in the same house for one year and over. 15.2.6 Mobility Another category of factors in the literature that affects the spread of infectious diseases significantly is the mobility; for example, movements of people from neighborhoods. We collect the mobility data from the Bureau of Transportation Statistics. Number of trips X – XX: number of trips by residents greater than X miles and shorter than XX miles. There are 10 different trip ranges: “&lt;= 1,” “1 – 3,” “3 – 5,” “5–10,” “10 – 25,” “25 – 50,” “50 – 100,” “100 – 250,” “250 – 500,” and \" &gt; 500\". Population Stay at Home: number of residents staying at home, that is, persons who make no trips with a trip end more than one mile away from home. 15.2.7 Geographic Information The longitude and latitude of the geographic center for each county in the US are available in Gazetteer Files: https://www2.census.gov/geo/docs/maps-data/data/gazetteer/2019_Gazetteer. 15.3 Data Sets 15.3.1 Chapter 2 I.county: a 3104 by 328 dataframe with columns ID, County, State and dates fromX2020.12.11 to X2020.01.22. pop.county: a 3142 by 4 dataframe with columns ID, County, State and population. Each row of the dataframe stands for one county along with its population. state.long: a 15925 by 7 dataframe with columns State, Region, Division, pop, DATE, Infected and Death. Each row of the dataframe stands for one state on a specific date. I.state/I.state.wide: a 49 by 326 dataframe with columns State and dates fromX2020.12.11 to X2020.01.22. Each row of the dataframe stands for the time series of infected cases for one state. I.state.long: a 15925 by 3 dataframe with columns State, DATE and Infect. Each row of the dataframe stands for one state on a specific date. 15.3.2 Chapter 3 Altieri, Nick, Rebecca L Barter, James Duncan, Raaz Dwivedi, Karl Kumbier, Xiao Li, Robert Netzorg, et al. 2020. “Curating a COVID-19 Data Repository and Forecasting County-Level Death Counts in the United States.” arXiv. https://doi.org/10.1162/99608f92.1d4e0dae. Altieri, Nick, Rebecca L Barter, James Duncan, Raaz Dwivedi, Karl Kumbier, Xiao Li, Robert Netzorg, et al. 2021. “Curating a COVID-19 Data Repository and Forecasting County-Level Death Counts in the United States.” Harvard Data Science Review, February. https://doi.org/10.1162/99608f92.1d4e0dae. Arik, Sercan O, Chun-Liang Li, Jinsung Yoon, Rajarishi Sinha, Arkady Epshteyn, Long T Le, Vikas Menon, et al. 2020. “Interpretable Sequence Learning for COVID-19 Forecasting.” arXiv Preprint arXiv:2008.00646. http://arxiv.org/abs/2008.00646. Arora, Parul, Himanshu Kumar, and Bijaya Ketan Panigrahi. 2020. “Prediction and Analysis of COVID-19 Positive Cases Using Deep Learning Models: A Descriptive Case Study of India.” Chaos, Solitons &amp; Fractals 139. Assimakopoulos, Vassilis, and Konstantinos Nikolopoulos. 2000. “The Theta Model: A Decomposition Approach to Forecasting.” International Journal of Forecasting 16 (4): 521–30. Barreto, Mauricio L, Maria Glória Teixeira, and Eduardo Hage Carmo. 2006. “Infectious Diseases Epidemiology.” Journal of Epidemiology &amp; Community Health 60 (3): 192–95. Bates, John M, and Clive WJ Granger. 1969. “The Combination of Forecasts.” Journal of the Operational Research Society 20 (4): 451–68. Brauer, Fred. 2008. “Compartmental Models in Epidemiology.” In Mathematical Epidemiology, 19–79. Springer. Brauer, Fred, Pauline van den Driessche, and Jianhong Wu. 2008. Mathematical Epidemiology. Vol. 1945. Springer. Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statist. Sci. 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726. Brockwell, Peter J., and Richard A. Davis. 2016. Introduction to Time Series and Forecasting. Third. Springer Texts in Statistics. Springer, [Cham]. https://doi.org/10.1007/978-3-319-29854-2. Brown, Robert Goodell. 1959. Statistical Forecasting for Inventory Control. New York: McGraw-Hill. Castro1, Lauren, Geoffrey Fairchild, Isaac Michaud, and Dave Osthus. 2020. “COFFEE: COVID-19 Forecasts Using Fast Evaluations and Estimation.” https://covid-19.bsvgateway.org/static/COFFEE-methodology.pdf. Chen, Li-Pang, Qihuang Zhang, Grace Y Yi, and Wenqing He. 2021. “Model-Based Forecasting for Canadian COVID-19 Data.” PloS One 16 (1): e0244536. Cleveland, Robert B, William S Cleveland, Jean E McRae, and Irma Terpenning. 1990. “STL: A Seasonal-Trend Decomposition.” Journal of Official Statistics 6 (1): 3–73. Daley, Daryl J, and Joe Gani. 2001. Epidemic Modelling: An Introduction. Cambridge Studies in Mathematical Biology. Cambridge University Press. https://doi.org/10.1017/CBO9780511608834. De Livera, Alysha M, Rob J Hyndman, and Ralph D Snyder. 2011. “Forecasting Time Series with Complex Seasonal Patterns Using Exponential Smoothing.” Journal of the American Statistical Association 106 (496): 1513–27. Dicker, R, and NC Gathany. 1992. “Principles of Epidemiology, 2nd Edn., US Department of Health and Human Services.” Public Health Service, Centre for Disease Control and Prevention (CDC), Atlanta. Dietz, Klaus, and JAP Heesterbeek. 2002. “Daniel Bernoulli’s Epidemiological Model Revisited.” Mathematical Biosciences 180 (1-2): 1–21. Durojaye, MO, and IJ Ajie. 2017. “Mathematical Model of the Spread and Control of Ebola Virus Disease.” Appl. Math 7: 23–31. En’Ko, PD. 1989. “On the Course of Epidemics of Some Infectious Diseases.” International Journal of Epidemiology 18 (4): 749–55. Gardner, Everette S., and Ed. Mckenzie. 1985. “Forecasting Trends in Time Series.” Management Science 31 (10): 1237–46. https://doi.org/10.1287/mnsc.31.10.1237. Held, Leonhard, Niel Hens, Philip D O’Neill, and Jacco Wallinga. 2020. Handbook of Infectious Disease Data Analysis. New York: Chapman; Hall/CRC. https://doi.org/s10.1201/9781315222912. Hoertel, Nicolas, Martin Blachier, Carlos Blanco, Mark Olfson, Marc Massetti, Marina Sánchez Rico, Frédéric Limosin, and Henri Leleu. 2020. “A Stochastic Agent-Based Model of the SARS-CoV-2 Epidemic in France.” Nature Medicine 26 (9): 1417–21. Holt, Charles C. 1957. “Forecasting Seasonals and Trends by Exponentially Weighted Moving Averages.” Pittsburgh, Penn: Carnegie Institute of Technology, Graduate School of Industrial Administration. Hyndman, Rob J, and George Athanasopoulos. 2018b. Forecasting: Principles and Practice. OTexts. ———. 2018a. Forecasting: Principles and Practice. OTexts. Jewell, Nicholas P. 2003. Statistics for Epidemiology. CRC Press. https://doi.org/10.1093/ije/dyh293. Keeling, Matt J., and Pejman Rohani. 2008. Modeling Infectious Diseases in Humans and Animals. Princeton University Press. https://doi.org/10.2307/j.ctvcm4gk0. Kermack, William Ogilvy, and Anderson G McKendrick. 1927. “A Contribution to the Mathematical Theory of Epidemics.” Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character 115 (772): 700–721. https://doi.org/10.1098/rspa.1933.0106. Killeen, Benjamin D, Jie Ying Wu, Kinjal Shah, Anna Zapaishchykova, Philipp Nikutta, Aniruddha Tamhane, Shreya Chakraborty, et al. 2020. “A County-Level Dataset for Informing the United States’ Response to COVID-19.” Available at https://arxiv.org/abs/2004.00756v2. KRR, Gandhi, Murthy KVR, Prasada Rao SSP, and Francesco Casella. 2020. “Non-Pharmaceutical Interventions (NPIs) to Reduce COVID-19 Mortality.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3560688. Lawson, Andrew B, Sudipto Banerjee, Robert P Haining, and Marı́a Dolores Ugarte. 2016. Handbook of Spatial Epidemiology. CRC Press. https://doi.org/10.1201/b19470. Lessler, Justin, and Derek AT Cummings. 2016. “Mechanistic Models of Infectious Disease and Their Impact on Public Health.” American Journal of Epidemiology 183 (5): 415–22. https://doi.org/10.1093/aje/kww021. Miranda, Gisele HB, Jan M Baetens, Nathalie Bossuyt, Odemir M Bruno, and Bernard De Baets. 2019. “Real-Time Prediction of Influenza Outbreaks in Belgium.” Epidemics 28: 100341. Mojeeb, AL, Isaac Kwasi Adu, and Cuihong Yang. 2017. “A Simple Seir Mathematical Model of Malaria Transmission.” Asian Research Journal of Mathematics, 1–22. Pfeiffer, Dirk, Timothy P Robinson, Mark Stevenson, Kim B Stevens, David J Rogers, Archie CA Clements, and others. 2008. Spatial Analysis in Epidemiology. Vol. 142. 10.1093. Oxford University Press Oxford. Porta, Miquel. 2014. A Dictionary of Epidemiology. Oxford university press. Rahmandad, Hazhir, and John Sterman. 2008. “Heterogeneity and Network Structure in the Dynamics of Diffusion: Comparing Agent-Based and Differential Equation Models.” Management Science 54 (5): 998–1014. Roda, Weston C, Marie B Varughese, Donglin Han, and Michael Y Li. 2020. “Why Is It Difficult to Accurately Predict the COVID-19 Epidemic?” Infectious Disease Modelling 5: 271–81. Sievert, Carson. 2020. Interactive Web-Based Data Visualization with r, Plotly, and Shiny. CRC Press. Sujath, R, Jyotir Moy Chatterjee, and Aboul Ella Hassanien. 2020. “A Machine Learning Forecasting Model for COVID-19 Pandemic in India.” Stochastic Environmental Research and Risk Assessment 34: 959–72. Tashman, Leonard J. 2000. “Out-of-Sample Tests of Forecasting Accuracy: An Analysis and Review.” International Journal of Forecasting 16 (4): 437–50. Wang, Guannan, Zhiling Gu, Xinyi Li, Shan Yu, Myungjin Kim, Yueying Wang, Lei Gao, and Li Wang. 2020. “Comparing and Integrating US COVID-19 Daily Data from Multiple Sources: A County-Level Dataset with Local Characteristics.” Available at https://arxiv.org/abs/2006.01333v3. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\". Winters, Peter R. 1960. “Forecasting Sales by Exponentially Weighted Moving Averages.” Management Science 6 (3): 324–42. https://doi.org/10.1287/mnsc.6.3.324. Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown. Zou, Difan, Lingxiao Wang, Pan Xu, Jinghui Chen, Weitong Zhang, and Quanquan Gu. 2020. “Epidemic Model Guided Machine Learning for COVID-19 Forecasts in the United States.” medRxiv. https://doi.org/10.1101/2020.05.24.20111989. "]]
